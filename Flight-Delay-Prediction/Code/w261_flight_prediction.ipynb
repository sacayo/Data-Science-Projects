{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9665640-e78d-4d34-9ba1-c127aa04f4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install folium --quiet\n",
    "%pip install tabulate --quiet\n",
    "%pip install catboost --quiet\n",
    "%pip install optuna --quiet\n",
    "%pip install optuna-integration[tfkeras] --quiet\n",
    "%pip install graphframes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860d1e18-8843-40ec-9b06-baca76a0b902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "\"\"\" SQL functionsl\"\"\"\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, desc, asc, year, when, isnan, avg, min, max, hour, round, substring, to_timestamp, date_format, lit, concat\n",
    "from pyspark.sql.types import DoubleType, FloatType, StringType, IntegerType, DecimalType, TimestampType, DateType, BooleanType\n",
    "from pyspark.sql.functions import to_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "\"\"\" Spark ML functions\"\"\" \n",
    "from pyspark.ml.functions import vector_to_array\n",
    "# from petastorm.tf_utils import make_petastorm_dataset\n",
    "# from petastorm import make_reader\n",
    "# from petastorm import make_batch_reader\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.stat import Correlation\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE, RFECV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from xgboost.spark import SparkXGBClassifier \n",
    "\n",
    "\"\"\" Deep learning imports\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "\"\"\" Data visualization imports\"\"\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium.plugins import MiniMap\n",
    "import matplotlib.cm as cm\n",
    "from branca.colormap import LinearColormap\n",
    "import calendar\n",
    "import branca.colormap as cm\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "\"\"\" Hyperparameter tuning imports\"\"\"\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\"\"\" warning handling\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"graph fuctions\"\"\"\n",
    "import networkx as nx\n",
    "from graphframes import *\n",
    "from networkx.algorithms import community\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164cb82a-a126-4134-b64f-0699c413b654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Initial analysis to find the airlines we want to tell our story for - final picked Southwest Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206e068b-d6f4-48e7-88cb-e70fc23404e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data    \n",
    "df_flights = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/\")\n",
    "display(df_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "099b7493-cf28-4dc8-b8c8-60e7e5b54eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Total number of rows in the dataset\n",
    "total_rows = df_flights.count()\n",
    "print(f\"Total rows in dataset: {total_rows}\")\n",
    "\n",
    "# Step 2: Count number of nulls in DEP_DEL15 column\n",
    "null_count = df_flights.filter(col(\"DEP_DEL15\").isNull()).count()\n",
    "print(f\"Total null values in DEP_DEL15: {null_count}\")\n",
    "\n",
    "# Step 3: Drop null values in DEP_DEL15 column\n",
    "df_flights_filtered = df_flights.filter(col(\"DEP_DEL15\").isNotNull())\n",
    "\n",
    "# Step 4: Compute total flights and total delays per airline\n",
    "df_delays = (\n",
    "    df_flights_filtered\n",
    "    .groupBy(\"OP_UNIQUE_CARRIER\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Flights\"),  # Count total flights per airline\n",
    "        F.sum(col(\"DEP_DEL15\")).alias(\"Total_Delayed_Flights\")  # Count delays\n",
    "    )\n",
    "    .withColumn(\"Delay_Percentage\", round((col(\"Total_Delayed_Flights\") / col(\"Total_Flights\")) * 100, 2))  # Calculate delay percentage\n",
    "    .orderBy(desc(\"Delay_Percentage\"))  # Sort by worst performer\n",
    ")\n",
    "\n",
    "# Show the top 10 airlines with the highest delay percentage\n",
    "display(df_delays.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41242f6-be78-4d22-afb2-4781953ff6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airline Data    \n",
    "df_flights_complete = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6727e3c8-d5e4-441d-a4ac-7e66953aacd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Total number of rows in the dataset\n",
    "total_rows = df_flights_complete.count()\n",
    "print(f\"Total rows in dataset: {total_rows}\")\n",
    "\n",
    "# Step 2: Count number of nulls in DEP_DEL15 column\n",
    "null_count = df_flights_complete.filter(col(\"DEP_DEL15\").isNull()).count()\n",
    "print(f\"Total null values in DEP_DEL15: {null_count}\")\n",
    "\n",
    "# Step 3: Drop null values in DEP_DEL15 column\n",
    "df_flights_filtered_complete = df_flights_complete.filter(col(\"DEP_DEL15\").isNotNull())\n",
    "\n",
    "# Step 4: Compute total flights and total delays per airline\n",
    "df_delays_complete = (\n",
    "    df_flights_filtered_complete\n",
    "    .groupBy(\"OP_UNIQUE_CARRIER\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Flights\"),  # Count total flights per airline\n",
    "        F.sum(col(\"DEP_DEL15\")).alias(\"Total_Delayed_Flights\")  # Count delays\n",
    "    )\n",
    "    .withColumn(\"Delay_Percentage\", round((col(\"Total_Delayed_Flights\") / col(\"Total_Flights\")) * 100, 2))  # Calculate delay percentage\n",
    "    .orderBy(desc(\"Delay_Percentage\"))  # Sort by worst performer\n",
    ")\n",
    "\n",
    "# Show the top 10 airlines with the highest delay percentage\n",
    "display(df_delays_complete.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cc4ab6-c73c-4b5c-8fc0-5e0313277b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Compute total flights and total delays per airline\n",
    "df_delays_complete = (\n",
    "    df_flights_filtered_complete\n",
    "    .groupBy(\"OP_UNIQUE_CARRIER\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Flights\"),  # Count total flights per airline\n",
    "        F.sum(col(\"DEP_DEL15\")).alias(\"Total_Delayed_Flights\")  # Count delays\n",
    "    )\n",
    "    .withColumn(\"Delay_Percentage\", round((col(\"Total_Delayed_Flights\") / col(\"Total_Flights\")) * 100, 2))  # Calculate delay percentage\n",
    "    .orderBy(asc(\"Delay_Percentage\"))  # Sort by best performer\n",
    ")\n",
    "\n",
    "# Show the top 10 airlines with the highest delay percentage\n",
    "display(df_delays_complete.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55b5d8e-ee29-45b8-bf95-b5028f2c5543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Full Airlines Dataset (2015-2021)\n",
    "df_flights_WN = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_joined_2015_2024.parquet/\")\n",
    "\n",
    "# Filter for Southwest Airlines (WN) only\n",
    "df_flights_WN = df_flights_WN.filter(col(\"OP_UNIQUE_CARRIER\") == \"WN\")\n",
    "\n",
    "# Extract year from flight date\n",
    "df_flights_WN = df_flights_WN.withColumn(\"Year\", year(col(\"FL_DATE\")))\n",
    "\n",
    "# Remove rows where DEP_DEL15 is null\n",
    "df_flights_WN_filtered = df_flights_WN.filter(col(\"DEP_DEL15\").isNotNull())\n",
    "\n",
    "# Aggregate total flights and total delays per year\n",
    "df_WN_yearly_delays = (\n",
    "    df_flights_WN_filtered\n",
    "    .groupBy(\"Year\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Flights_WN\"),\n",
    "        F.sum(col(\"DEP_DEL15\")).alias(\"Total_Delayed_Flights_WN\")\n",
    "    )\n",
    "    .withColumn(\"Delay_Percentage_WN\", (col(\"Total_Delayed_Flights_WN\") / col(\"Total_Flights_WN\")) * 100)  # Compute percentage\n",
    "    .orderBy(\"Year\")  # Order by year\n",
    ")\n",
    "\n",
    "# Show year-over-year delay statistics\n",
    "df_WN_yearly_delays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14647e82-13ee-4acd-af1b-c27360307b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for years 2015-2024\n",
    "df_WN_yearly_delays_filtered = df_WN_yearly_delays.filter((col(\"Year\") >= 2015) & (col(\"Year\") <= 2024))\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for visualization\n",
    "df_WN_pd = df_WN_yearly_delays_filtered.toPandas()\n",
    "\n",
    "# Handle NaN values by filling them with 0\n",
    "df_WN_pd[\"Delay_Percentage_WN\"].fillna(0, inplace=True)\n",
    "\n",
    "# Compute mean delay percentage across all years\n",
    "mean_delay_percentage = df_WN_pd[\"Delay_Percentage_WN\"].mean()\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot bar chart\n",
    "bars = plt.bar(df_WN_pd[\"Year\"].astype(str), df_WN_pd[\"Delay_Percentage_WN\"], color=\"skyblue\", label=\"Yearly Delay Percentage\")\n",
    "\n",
    "# Add a horizontal mean line\n",
    "plt.axhline(y=mean_delay_percentage, color='red', linestyle='--', linewidth=1, label=f\"Mean Delay Percentage ({mean_delay_percentage:.2f}%)\")\n",
    "\n",
    "# Add text labels slightly lower for better visibility\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()  # Get height of bar (delay percentage)\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval - 1, f\"{yval:.2f}%\", ha='center', fontsize=10)\n",
    "\n",
    "# Customize chart\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Delay Percentage (%)\")\n",
    "plt.title(\"Southwest Airlines Year-over-Year Flight Delay Trend (2015-2024)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "plt.legend()\n",
    "\n",
    "# Remove the grey dotted lines\n",
    "plt.grid(False)  # Disables the grid completely\n",
    "\n",
    "# Save the plot as an image\n",
    "chart_path = \"/dbfs/FileStore/WN_flight_delays_2015_2024.png\"  # Path in DBFS\n",
    "plt.savefig(chart_path, dpi=300, bbox_inches='tight')  # High-quality PNG file\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Print path for reference\n",
    "print(f\"Chart saved at: {chart_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7772a163-68c9-47f9-8818-6f218fe4f8c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query for Custom Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "55ab609a-9439-4592-b6d9-b74770877a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Extract zipped weather data from 2022-2024\"\"\"\n",
    "\n",
    "for year in range(2022, 2025):\n",
    "    local_tar_path = f\"/tmp/weather_{year}.tar.gz\"\n",
    "    local_extract_path = f\"/tmp/extracted/weather_{year}\"\n",
    "\n",
    "    # Copy tar.gz from DBFS to local driver disk\n",
    "    dbutils.fs.cp(f\"dbfs:/FileStore/tables/weather_{year}.tar.gz\", f\"file:{local_tar_path}\")\n",
    "\n",
    "    # Extract tar.gz locally\n",
    "    with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=local_extract_path)\n",
    "\n",
    "    # Copy extracted files back to DBFS\n",
    "    for root, dirs, files in os.walk(local_extract_path):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            # Construct DBFS destination path, e.g., keep folder structure if needed\n",
    "            relative_path = os.path.relpath(local_file, local_extract_path)\n",
    "            dbfs_path = f\"dbfs:/FileStore/tables/extracted_weather/{relative_path}\"\n",
    "            dbutils.fs.cp(f\"file:{local_file}\", dbfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6580543b-612c-408c-922c-47f33bfc22e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Read extracted weather data from 2022-2024 and checkpoint as parquet\"\"\"\n",
    "\n",
    "df_weather_new = spark.read.csv(\n",
    "    \"dbfs:/FileStore/tables/extracted_weather/*.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "rename_dict = {\n",
    "    'MonthlyAverageWindSpeed': 'AWND',\n",
    "    'MonthlyHeatingDegreeDays': 'HTDD',\n",
    "    'HeatingDegreeDaysSeasonToDate': 'HDSD',\n",
    "    'MonthlyNumberDaysWithSnowfall': 'DSNW',\n",
    "    'CoolingDegreeDaysSeasonToDate': 'CDSD',\n",
    "    'MonthlyCoolingDegreeDays': 'CLDD'\n",
    "}\n",
    "\n",
    "for old, new in rename_dict.items():\n",
    "    df_weather_new = df_weather_new.withColumnRenamed(old, new)\n",
    "\n",
    "df_weather_new = df_weather_new.drop(\"MonthlyNumberDaysWithThunderstorms\", \"MonthlyNumberDaysWithHeavyFog\")\n",
    "df_weather_new = df_weather_new.withColumn(\"YEAR\", substring(col(\"DATE\"), 1, 4).cast(\"int\"))\n",
    "df_weather_new.write.mode(\"overwrite\").parquet(f\"dbfs:/student-groups/Group_04_04/df_weather_2022_2024.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "468065a2-c4fc-48ef-a61f-eb30ec346d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Function to join flights and weather\"\"\"\n",
    "\n",
    "def join_flights_weather(df_flights, df_weather, df_airport_codes):\n",
    "    df_flights.createOrReplaceTempView(\"flights\")\n",
    "    df_weather.createOrReplaceTempView(\"weather\")\n",
    "    df_airport_codes.createOrReplaceTempView(\"airport_codes\")\n",
    "\n",
    "    df_weather2 = spark.sql(\"\"\"\n",
    "        WITH weather_deduped AS (\n",
    "            SELECT\n",
    "                DISTINCT *\n",
    "            FROM\n",
    "                weather\n",
    "        )\n",
    "        SELECT\n",
    "            * EXCEPT (DATE, YEAR),\n",
    "            TIMESTAMP(\n",
    "                make_timestamp(\n",
    "                    CAST(SUBSTR(DATE, 1, 4) AS INT),\n",
    "                    CAST(SUBSTR(DATE, 6, 2) AS INT),\n",
    "                    CAST(SUBSTR(DATE, 9, 2) AS INT),\n",
    "                    CAST(SUBSTR(DATE, 12, 2) AS INT),\n",
    "                    CAST(SUBSTR(DATE, 15, 2) AS INT),\n",
    "                    CAST(SUBSTR(DATE, 18, 2) AS INT)\n",
    "                )\n",
    "            ) AS DATE\n",
    "        FROM\n",
    "            weather_deduped\n",
    "    \"\"\")\n",
    "\n",
    "    df_flights2 = spark.sql(\"\"\"\n",
    "        WITH ac AS (\n",
    "            SELECT\n",
    "                iata_code,\n",
    "                name,\n",
    "                type,\n",
    "                CAST(SPLIT_PART(coordinates, ',', 1) AS FLOAT) AS longitude,\n",
    "                CAST(SPLIT_PART(coordinates, ',', 2) AS FLOAT) AS latitude\n",
    "            FROM\n",
    "                airport_codes\n",
    "            WHERE\n",
    "                iata_code IS NOT NULL\n",
    "        ),\n",
    "        s AS (\n",
    "            SELECT\n",
    "                DISTINCT STATION as neighbor_id,\n",
    "                NAME as neighbor_name,\n",
    "                CAST(LATITUDE AS FLOAT) AS neighbor_lat,\n",
    "                CAST(LONGITUDE AS FLOAT) AS neighbor_lon\n",
    "            FROM\n",
    "                weather\n",
    "        ),\n",
    "        distances AS (\n",
    "            SELECT\n",
    "                ac.iata_code,\n",
    "                ac.name,\n",
    "                ac.type,\n",
    "                ac.latitude,\n",
    "                ac.longitude,\n",
    "                s.neighbor_id,\n",
    "                s.neighbor_name,\n",
    "                s.neighbor_lat,\n",
    "                s.neighbor_lon,\n",
    "                6391 * 2 * ASIN(SQRT(POWER(SIN(RADIANS(s.neighbor_lat - ac.latitude) / 2), 2) + COS(RADIANS(ac.latitude)) * COS(RADIANS(s.neighbor_lat)) * POWER(SIN(RADIANS(s.neighbor_lon - ac.longitude) / 2), 2))) AS distance_km\n",
    "            FROM\n",
    "                ac CROSS JOIN s\n",
    "        ),\n",
    "        distance_rankings AS (\n",
    "            SELECT\n",
    "                *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY iata_code ORDER BY distance_km ASC) AS distance_rank\n",
    "            FROM\n",
    "                distances\n",
    "        ),\n",
    "        nearest_neighbor AS (\n",
    "            SELECT\n",
    "                iata_code,\n",
    "                name AS airport_name,\n",
    "                latitude AS airport_lat,\n",
    "                longitude AS airport_lon,\n",
    "                type,\n",
    "                neighbor_id,\n",
    "                neighbor_name,\n",
    "                distance_km\n",
    "            FROM\n",
    "                distance_rankings\n",
    "            WHERE\n",
    "                distance_rank = 1\n",
    "            ORDER BY\n",
    "                distance_km ASC\n",
    "        ),\n",
    "        state_time_zones AS (\n",
    "            SELECT \n",
    "                state, \n",
    "                time_zone_offset\n",
    "            FROM \n",
    "                (\n",
    "                    VALUES\n",
    "                        ('AL', 'America/Chicago'), \n",
    "                        ('AK', 'America/Anchorage'), \n",
    "                        ('AZ', 'America/Phoenix'), \n",
    "                        ('AR', 'America/Chicago'), \n",
    "                        ('CA', 'America/Los_Angeles'), \n",
    "                        ('CO', 'America/Denver'), \n",
    "                        ('CT', 'America/New_York'), \n",
    "                        ('DE', 'America/New_York'), \n",
    "                        ('FL', 'America/New_York'), \n",
    "                        ('GA', 'America/New_York'), \n",
    "                        ('HI', 'Pacific/Honolulu'), \n",
    "                        ('ID', 'America/Boise'), \n",
    "                        ('IL', 'America/Chicago'), \n",
    "                        ('IN', 'America/New_York'), \n",
    "                        ('IA', 'America/Chicago'), \n",
    "                        ('KS', 'America/Chicago'), \n",
    "                        ('KY', 'America/New_York'), \n",
    "                        ('LA', 'America/Chicago'), \n",
    "                        ('ME', 'America/New_York'), \n",
    "                        ('MD', 'America/New_York'), \n",
    "                        ('MA', 'America/New_York'), \n",
    "                        ('MI', 'America/New_York'), \n",
    "                        ('MN', 'America/Chicago'), \n",
    "                        ('MS', 'America/Chicago'), \n",
    "                        ('MO', 'America/Chicago'), \n",
    "                        ('MT', 'America/Denver'), \n",
    "                        ('NE', 'America/Chicago'), \n",
    "                        ('NV', 'America/Los_Angeles'), \n",
    "                        ('NH', 'America/New_York'), \n",
    "                        ('NJ', 'America/New_York'), \n",
    "                        ('NM', 'America/Denver'), \n",
    "                        ('NY', 'America/New_York'), \n",
    "                        ('NC', 'America/New_York'), \n",
    "                        ('ND', 'America/Chicago'), \n",
    "                        ('OH', 'America/New_York'), \n",
    "                        ('OK', 'America/Chicago'), \n",
    "                        ('OR', 'America/Los_Angeles'), \n",
    "                        ('PA', 'America/New_York'), \n",
    "                        ('PR', 'America/Puerto_Rico'),\n",
    "                        ('RI', 'America/New_York'), \n",
    "                        ('SC', 'America/New_York'), \n",
    "                        ('SD', 'America/Chicago'), \n",
    "                        ('TN', 'America/Chicago'), \n",
    "                        ('TX', 'America/Chicago'), \n",
    "                        ('UT', 'America/Denver'), \n",
    "                        ('VT', 'America/New_York'), \n",
    "                        ('VA', 'America/New_York'), \n",
    "                        ('WA', 'America/Los_Angeles'), \n",
    "                        ('WV', 'America/New_York'), \n",
    "                        ('WI', 'America/Chicago'), \n",
    "                        ('WY', 'America/Denver')\n",
    "                ) AS t(state, time_zone_offset)\n",
    "        ),\n",
    "        flights_deduped AS (\n",
    "            SELECT\n",
    "                DISTINCT * EXCEPT (FL_DATE),\n",
    "                SUBSTRING(FL_DATE, 1, 10) AS FL_DATE\n",
    "            FROM\n",
    "                flights\n",
    "        ),\n",
    "        flights_with_stations AS (\n",
    "            SELECT\n",
    "                f.*,\n",
    "                TO_TIMESTAMP(\n",
    "                    CONCAT(\n",
    "                        FL_DATE,\n",
    "                        ' ', \n",
    "                        CASE \n",
    "                            WHEN LENGTH(CRS_DEP_TIME) = 3 THEN CONCAT('0', SUBSTR(CRS_DEP_TIME, 1, 1), ':', SUBSTR(CRS_DEP_TIME, 2, 2))\n",
    "                            ELSE CONCAT(SUBSTR(CRS_DEP_TIME, 1, 2), ':', SUBSTR(CRS_DEP_TIME, 3, 2))\n",
    "                        END,\n",
    "                        ':00'\n",
    "                    ),\n",
    "                    'yyyy-MM-dd HH:mm:ss'\n",
    "                ) AS sched_depart_date_time,\n",
    "                to_utc_timestamp(sched_depart_date_time, st.time_zone_offset) AS sched_depart_date_time_UTC,\n",
    "                TO_TIMESTAMP(\n",
    "                    CONCAT(\n",
    "                        FL_DATE,\n",
    "                        ' ', \n",
    "                        CASE \n",
    "                            WHEN LENGTH(DEP_TIME) = 3 THEN CONCAT('0', SUBSTR(DEP_TIME, 1, 1), ':', SUBSTR(DEP_TIME, 2, 2))\n",
    "                            ELSE CONCAT(SUBSTR(DEP_TIME, 1, 2), ':', SUBSTR(DEP_TIME, 3, 2))\n",
    "                        END,\n",
    "                        ':00'\n",
    "                    ),\n",
    "                    'yyyy-MM-dd HH:mm:ss'\n",
    "                ) AS depart_date_time,\n",
    "                to_utc_timestamp(depart_date_time, st.time_zone_offset) AS depart_date_time_UTC,\n",
    "                TIMESTAMPADD(MINUTE, ACTUAL_ELAPSED_TIME, depart_date_time_UTC) AS arrival_date_time_UTC,\n",
    "                sched_depart_date_time_UTC - INTERVAL 4 HOUR AS four_hours_prior_depart_UTC,\n",
    "                sched_depart_date_time_UTC - INTERVAL 2 HOUR AS two_hours_prior_depart_UTC,\n",
    "                nn.airport_name AS origin_airport_name,\n",
    "                nn.type AS origin_type,\n",
    "                nn.airport_lat AS origin_airport_lat,\n",
    "                nn.airport_lon AS origin_airport_lon,\n",
    "                nn.neighbor_id AS origin_station_id,\n",
    "                nn.neighbor_name AS origin_station_name,\n",
    "                nn.distance_km AS origin_dis,\n",
    "                nn2.airport_name AS dest_airport_name,\n",
    "                nn2.type AS dest_type,\n",
    "                nn2.airport_lat AS dest_airport_lat,\n",
    "                nn2.airport_lon AS dest_airport_lon,\n",
    "                nn2.neighbor_id AS dest_station_id,\n",
    "                nn2.neighbor_name AS dest_station_name,\n",
    "                nn2.distance_km AS dest_dis\n",
    "            FROM\n",
    "                flights_deduped f\n",
    "                LEFT JOIN state_time_zones st\n",
    "                    ON f.ORIGIN_STATE_ABR = st.state\n",
    "                LEFT JOIN nearest_neighbor nn\n",
    "                    ON f.ORIGIN = nn.iata_code\n",
    "                LEFT JOIN nearest_neighbor nn2\n",
    "                    ON f.DEST = nn2.iata_code\n",
    "            ORDER BY\n",
    "                TAIL_NUM DESC,\n",
    "                sched_depart_date_time_UTC\n",
    "        )\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN LAG(CANCELLED) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC)\n",
    "                ELSE NULL\n",
    "            END AS PREV_CANCELLED,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN LAG(ORIGIN) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC)\n",
    "                ELSE NULL\n",
    "            END AS PREV_ORIGIN,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN LAG(arrival_date_time_UTC) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC)\n",
    "                ELSE NULL\n",
    "            END AS PREV_arrival_date_time_UTC,\n",
    "            TIMESTAMPDIFF(MINUTE, PREV_arrival_date_time_UTC, sched_depart_date_time_UTC) AS MINUTES_BETWEEN_FLIGHTS,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN LAG(ARR_DELAY) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC)\n",
    "                ELSE NULL\n",
    "            END AS PREV_ARR_DELAY,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN LAG(ARR_DELAY_NEW) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC)\n",
    "                ELSE NULL\n",
    "            END AS PREV_ARR_DELAY_NEW,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN LAG(ARR_DEL15) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC)\n",
    "                ELSE NULL\n",
    "            END AS PREV_ARR_DEL15,\n",
    "            CASE\n",
    "                WHEN TAIL_NUM IS NOT NULL THEN CONCAT_WS('->', LAG(ORIGIN) OVER (PARTITION BY TAIL_NUM ORDER BY sched_depart_date_time_UTC), ORIGIN, DEST)\n",
    "                ELSE NULL\n",
    "            END AS TRIPLET\n",
    "        FROM\n",
    "            flights_with_stations\n",
    "    \"\"\")\n",
    "\n",
    "    df_flights2 = df_flights2.withColumn(\"join_date\", F.date_trunc(\"hour\", F.col(\"four_hours_prior_depart_UTC\")))\n",
    "    df_weather2 = df_weather2.withColumn(\"join_date\", F.date_trunc(\"hour\", F.col(\"DATE\")))\n",
    "\n",
    "    df_joined = (\n",
    "        df_flights2\n",
    "        .join(\n",
    "            df_weather2,\n",
    "            (\n",
    "                (df_flights2.origin_station_id == df_weather2.STATION) &\n",
    "                (df_flights2.join_date == df_weather2.join_date) &\n",
    "                (df_weather2.DATE.between(\n",
    "                    df_flights2.four_hours_prior_depart_UTC,\n",
    "                    df_flights2.two_hours_prior_depart_UTC\n",
    "                ))\n",
    "            ),\n",
    "            how='left'\n",
    "        )\n",
    "        .drop(df_weather2.join_date)\n",
    "    )\n",
    "\n",
    "    return df_joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "56b0fecc-70ef-470c-8e34-a94f92e78d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Read flights data from 2022-2024 and checkpoint as parquet\"\"\"\n",
    "\n",
    "file_paths = [\n",
    "    f\"dbfs:/FileStore/tables/flights_{year}_{month:02d}.csv\"\n",
    "    for year in range(2022, 2025)\n",
    "    for month in range(1, 13)\n",
    "]\n",
    "\n",
    "df_flights_new = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(file_paths)\n",
    "\n",
    "# List of columns to cast to StringType\n",
    "columns_to_cast = [\n",
    "    'DIV3_TOTAL_GTIME', 'DIV3_LONGEST_GTIME', 'DIV2_LONGEST_GTIME',\n",
    "    'DIV3_AIRPORT_ID', 'DIV3_AIRPORT_SEQ_ID', 'DIV2_AIRPORT_ID',\n",
    "    'DIV2_TOTAL_GTIME', 'DIV3_WHEELS_OFF', 'DIV3_WHEELS_ON',\n",
    "    'DIV2_WHEELS_OFF', 'DIV2_WHEELS_ON', 'DIV2_AIRPORT_SEQ_ID'\n",
    "]\n",
    "\n",
    "# Cast all specified columns to StringType\n",
    "for column in columns_to_cast:\n",
    "    df_flights_new = df_flights_new.withColumn(column, col(column).cast(\"string\"))\n",
    "\n",
    "df_flights_new = df_flights_new.withColumn(\n",
    "    \"FL_DATE\",\n",
    "    date_format(\n",
    "        to_timestamp(col(\"FL_DATE\"), \"M/d/yyyy h:mm:ss a\"),\n",
    "        \"yyyy-MM-dd\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_flights_new.write.mode(\"overwrite\").parquet(f\"dbfs:/student-groups/Group_04_04/df_flights_2022_2024.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6621d6a5-abee-465e-ac08-07666e837932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Join and union old and new flights and weather data\"\"\"\n",
    "\n",
    "df_flights_old = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data\")\n",
    "df_weather_old = spark.read.parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data\")\n",
    "df_flights_new = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_flights_2022_2024.parquet\")\n",
    "df_weather_new = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_weather_2022_2024.parquet\")\n",
    "df_airport_codes = spark.read.format(\"csv\").option(\"header\",\"true\").load(f\"dbfs:/mnt/mids-w261/airport-codes_csv.csv\")\n",
    "\n",
    "df_joined_old = join_flights_weather(df_flights_old, df_weather_old, df_airport_codes)\n",
    "df_joined_new = join_flights_weather(df_flights_new, df_weather_new, df_airport_codes)\n",
    "df_joined = df_joined_old.unionByName(df_joined_new)\n",
    "\n",
    "df_joined.write.mode(\"overwrite\").parquet(f\"dbfs:/student-groups/Group_04_04/df_joined_2015_2024.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb41780a-3bff-495f-89ca-333e42e78796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Exploratory Data Analysis - Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f80d3e-8340-4047-814a-76d52af139b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a temporary parquet file to check its size\n",
    "temp_parquet_path = \"dbfs:/student-groups/Group_04_04/df_joined_2015_2024.parquet\"\n",
    "# df_otpw_combined_60M.write.mode(\"overwrite\").parquet(temp_parquet_path)\n",
    "\n",
    "# Get the size of the parquet file\n",
    "file_info = dbutils.fs.ls(temp_parquet_path)\n",
    "total_size = sum(file.size for file in file_info)\n",
    "print(f\"Total size of the combined DataFrame: {total_size / (1024 * 1024 * 1024):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f2c870b-2be4-4783-9928-758d30a3959e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save df_southwest_cleaned as a parquet file\n",
    "df_otpw_combined_60M.write.parquet(f\"{folder_path}/df_otpw_60M_complete.parquet\") # use to save a new version\n",
    "# df_southwest_cleaned.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_12M_southwest_cleaned_updated_Apr_02.parquet\") # use if you want to overwrite exisiting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c070137-07d9-448a-b738-083c185d275a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/student-groups/Group_04_04/\"\n",
    "# display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e4c162-e16b-4957-9317-cab470faada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading and Processing Data from the Custom Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6b9185-3454-4fb8-8280-41c682949ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_otpw_60M_complete_parquet = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_joined_full_with_lag.parquet/\")\n",
    "df_otpw_60M_complete_parquet = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_joined_2015_2024.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c1344f3-a8c3-4a06-91fc-4f083a27c712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_otpw_60M_complete_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903b77ea-38a5-4fe9-8a43-241a51da17b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute null percentage per column\n",
    "def calculate_null_percentage(df):\n",
    "    # Count total rows for reference\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    null_percentage_df = df.select([\n",
    "        (count(when(col(c).isNull() | (col(c) == \"\"), c)) / total_rows * 100).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    return null_percentage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6049da33-1105-44f7-8887-f491efc138e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# ✅ Check Dataset Shape (Rows & Columns)\n",
    "# ----------------------------- #\n",
    "num_rows = df_otpw_60M_complete_parquet.count()\n",
    "num_cols = len(df_otpw_60M_complete_parquet.columns)\n",
    "\n",
    "print(f\"✅ Dataset contains {num_rows:,} rows and {num_cols} columns.\")\n",
    "\n",
    "null_percentage_df = calculate_null_percentage(df_otpw_60M_complete_parquet)\n",
    "display(null_percentage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6045dd-a274-4c0c-9af2-b9e57b432038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "distinct_years = df_otpw_60M_complete_parquet.select(\"YEAR\").distinct()\n",
    "display(distinct_years.orderBy(\"YEAR\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82358af4-00cf-4c14-9539-6d0d12fc6d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Drop unwanted cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf04cf8-a8d6-4fd8-b678-e17800ac1d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delay_columns_to_remove = [\n",
    "    \"DEP_DELAY\",           # Actual departure delay in minutes\n",
    "    \"DEP_DELAY_NEW\",       # Departure delay, with negative values set to 0\n",
    "    \"DEP_DELAY_GROUP\",     # Departure delay groups\n",
    "    \"ARR_DELAY\",           # Arrival delay in minutes\n",
    "    \"ARR_DELAY_NEW\",       # Arrival delay, with negative values set to 0\n",
    "    \"ARR_DEL15\",           # Arrival delay indicator (15+ minutes)\n",
    "    \"ARR_DELAY_GROUP\",     # Arrival delay groups\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f940507c-4c54-4591-8c2d-035db9d6f2ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "actual_time_columns_to_remove = [\n",
    "    \"DEP_TIME\",            # Actual departure time\n",
    "    \"TAXI_OUT\",            # Taxi out time\n",
    "    \"WHEELS_OFF\",          # Wheels off time\n",
    "    \"WHEELS_ON\",           # Wheels on time\n",
    "    \"TAXI_IN\",             # Taxi in time\n",
    "    \"ARR_TIME\",            # Actual arrival time\n",
    "    \"ACTUAL_ELAPSED_TIME\", # Actual elapsed time\n",
    "    \"AIR_TIME\"             # Air time\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e650e54-a795-4ba7-8000-48763bb607f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a list to store column names with null percentage <= 50%\n",
    "columns_with_low_nulls = []\n",
    "\n",
    "# Get the threshold\n",
    "threshold = 50.0\n",
    "\n",
    "# Collect the single row of percentages\n",
    "null_percentages = null_percentage_df.collect()[0]\n",
    "\n",
    "# Get the column names that meet the threshold\n",
    "columns_with_low_nulls = [col_name for col_name in null_percentage_df.columns \n",
    "                         if null_percentages[col_name] <= threshold]\n",
    "\n",
    "# Additional columns to remove\n",
    "columns_to_remove = delay_columns_to_remove + actual_time_columns_to_remove\n",
    "\n",
    "# Filter out these columns from your list of columns with low nulls\n",
    "filtered_columns = [col for col in columns_with_low_nulls if col not in columns_to_remove]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Found {len(columns_with_low_nulls)} columns with null percentage <= 50%\")\n",
    "print(f\"Found {len(columns_to_remove)} columns that could cause data leakage\")\n",
    "print(f\"Found {len(filtered_columns)} columns to be used finally\")\n",
    "\n",
    "# Create a new DataFrame with only these columns if needed\n",
    "df_otpw_60M_complete_parquet_col_filtered = df_otpw_60M_complete_parquet.select(filtered_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fced751-e7f5-4fd6-a4d7-254ee64ce5d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_with_low_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d32cf8b-1c36-422d-818f-f3594505127c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_otpw_60M_complete_parquet_col_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d54e7592-c318-42b0-9405-44b54e202d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic cleaning of selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23ac5e1-8b7d-4b41-b20b-f7ca7361823f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_column_types(df):\n",
    "    \"\"\"\n",
    "    Function to convert column data types appropriately for the flight delay dataset,\n",
    "    after removing columns that could cause data leakage.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame to convert\n",
    "        \n",
    "    Returns:\n",
    "        PySpark DataFrame with converted column types\n",
    "    \"\"\"\n",
    "    # Define column type mappings\n",
    "    \n",
    "    # Date columns - date fields\n",
    "    date_columns = [\"FL_DATE\", \"DATE\"]\n",
    "    \n",
    "    # Timestamp columns - date-time fields with time zone information\n",
    "    timestamp_columns = [\n",
    "        \"sched_depart_date_time_UTC\", \"four_hours_prior_depart_UTC\", \n",
    "        \"two_hours_prior_depart_UTC\"\n",
    "    ]\n",
    "    \n",
    "    # Integer columns - IDs, counts, whole numbers\n",
    "    integer_columns = [\n",
    "        \"YEAR\", \"QUARTER\", \"MONTH\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "        \"OP_CARRIER_AIRLINE_ID\", \"OP_CARRIER_FL_NUM\", \n",
    "        \"ORIGIN_AIRPORT_ID\", \"ORIGIN_AIRPORT_SEQ_ID\", \"ORIGIN_CITY_MARKET_ID\",\n",
    "        \"ORIGIN_STATE_FIPS\", \"ORIGIN_WAC\",\n",
    "        \"DEST_AIRPORT_ID\", \"DEST_AIRPORT_SEQ_ID\", \"DEST_CITY_MARKET_ID\",\n",
    "        \"DEST_STATE_FIPS\", \"DEST_WAC\",\n",
    "        \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"FLIGHTS\", \"DISTANCE_GROUP\"\n",
    "    ]\n",
    "    \n",
    "    # Double columns - measurements, calculations with decimal points\n",
    "    double_columns = [\n",
    "        \"DEP_DEL15\",       # Keep target variable\n",
    "        \"CRS_ELAPSED_TIME\", \"DISTANCE\",\n",
    "        \"CANCELLED\", \"DIVERTED\",\n",
    "        \"origin_station_lat\", \"origin_station_lon\", \"origin_airport_lat\", \n",
    "        \"origin_airport_lon\", \"origin_station_dis\",\n",
    "        \"dest_station_lat\", \"dest_station_lon\", \"dest_airport_lat\", \n",
    "        \"dest_airport_lon\", \"dest_station_dis\",\n",
    "        \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\"\n",
    "    ]\n",
    "    \n",
    "    # Decimal columns - weather measurements that need higher precision\n",
    "    decimal_columns = [\n",
    "        \"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyDryBulbTemperature\", \n",
    "        \"HourlyPrecipitation\", \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\", \n",
    "        \"HourlyStationPressure\", \"HourlyVisibility\", \"HourlyWetBulbTemperature\", \n",
    "        \"HourlyWindDirection\", \"HourlyWindSpeed\"\n",
    "    ]\n",
    "    \n",
    "    # String columns - text identifiers, codes, names, etc.\n",
    "    string_columns = [\n",
    "        \"OP_UNIQUE_CARRIER\", \"OP_CARRIER\", \"TAIL_NUM\", \n",
    "        \"ORIGIN\", \"DEST\", \"ORIGIN_CITY_NAME\", \"DEST_CITY_NAME\", \n",
    "        \"ORIGIN_STATE_ABR\", \"DEST_STATE_ABR\", \"ORIGIN_STATE_NM\", \"DEST_STATE_NM\", \n",
    "        \"DEP_TIME_BLK\", \"ARR_TIME_BLK\", \"HourlySkyConditions\", \"REM\",\n",
    "        \"origin_airport_name\", \"origin_station_name\", \"origin_station_id\", \n",
    "        \"origin_iata_code\", \"origin_icao\", \"origin_type\", \"origin_region\",\n",
    "        \"dest_airport_name\", \"dest_station_name\", \"dest_station_id\", \n",
    "        \"dest_iata_code\", \"dest_icao\", \"dest_type\", \"dest_region\",\n",
    "        \"STATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"WindEquipmentChangeDate\", \"TRIPLET\"\n",
    "    ]\n",
    "    \n",
    "    # Convert date columns\n",
    "    for column in date_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, to_date(col(column)))\n",
    "    \n",
    "    # Convert timestamp columns\n",
    "    for column in timestamp_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, to_timestamp(col(column)))\n",
    "    \n",
    "    # Convert integer columns\n",
    "    for column in integer_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, col(column).cast(IntegerType()))\n",
    "    \n",
    "    # Convert double columns\n",
    "    for column in double_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, col(column).cast(DoubleType()))\n",
    "    \n",
    "    # Convert decimal columns (with higher precision for weather data)\n",
    "    for column in decimal_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, col(column).cast(DecimalType(10, 4)))\n",
    "    \n",
    "    # Convert string columns\n",
    "    for column in string_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.withColumn(column, col(column).cast(StringType()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c3671f-9a1e-4d42-af5c-8d851ad4e301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_flight_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning function for airline dataset addressing all columns with null values.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with flight and weather data\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned PySpark DataFrame with no null values\n",
    "    \"\"\"\n",
    "    # 1. First sort the data to ensure proper time-based operations\n",
    "    df = df.orderBy(\"ORIGIN\", \"FL_DATE\", \"CRS_DEP_TIME\")\n",
    "    \n",
    "    # 2. Handle DEP_DEL15 for cancelled flights - assign value 2 to represent \"cancelled\"\n",
    "    if \"DEP_DEL15\" in df.columns and \"CANCELLED\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"DEP_DEL15\", \n",
    "            F.when(F.col(\"DEP_DEL15\").isNull() & (F.col(\"CANCELLED\") == 1), 2).otherwise(F.col(\"DEP_DEL15\"))\n",
    "        )\n",
    "    \n",
    "    # 3. Drop rows with missing values in critical columns INCLUDING DEP_DEL15 after imputation\n",
    "    critical_cols = [\"FL_DATE\", \"OP_CARRIER\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\", \"TAIL_NUM\", \"DEP_DEL15\"]\n",
    "    for col in critical_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.filter(F.col(col).isNotNull())\n",
    "    \n",
    "    # 4. Calculate CRS_ELAPSED_TIME from CRS_DEP_TIME and CRS_ARR_TIME if missing\n",
    "    if \"CRS_ELAPSED_TIME\" in df.columns and \"CRS_DEP_TIME\" in df.columns and \"CRS_ARR_TIME\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"CRS_ELAPSED_TIME_CALC\",\n",
    "            F.when(\n",
    "                F.col(\"CRS_ARR_TIME\") < F.col(\"CRS_DEP_TIME\"),  # Handle overnight flights\n",
    "                ((F.col(\"CRS_ARR_TIME\") + 2400) - F.col(\"CRS_DEP_TIME\"))\n",
    "            ).otherwise(\n",
    "                F.col(\"CRS_ARR_TIME\") - F.col(\"CRS_DEP_TIME\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Convert from HHMM format to minutes\n",
    "        df = df.withColumn(\n",
    "            \"CRS_ELAPSED_TIME_CALC\",\n",
    "            (F.floor(F.col(\"CRS_ELAPSED_TIME_CALC\")/F.lit(100)) * F.lit(60)) + \n",
    "            (F.col(\"CRS_ELAPSED_TIME_CALC\") % F.lit(100))\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn(\"CRS_ELAPSED_TIME\", \n",
    "                          F.coalesce(F.col(\"CRS_ELAPSED_TIME\"), F.col(\"CRS_ELAPSED_TIME_CALC\")))\n",
    "        df = df.drop(\"CRS_ELAPSED_TIME_CALC\")\n",
    "    \n",
    "    # 5. Extract temporal features from FL_DATE if missing\n",
    "    date_derived_cols = {\n",
    "        \"YEAR\": F.year(\"FL_DATE\"),\n",
    "        \"QUARTER\": F.quarter(\"FL_DATE\"),\n",
    "        \"MONTH\": F.month(\"FL_DATE\"),\n",
    "        \"DAY_OF_WEEK\": F.dayofweek(\"FL_DATE\"),\n",
    "        \"DAY_OF_MONTH\": F.dayofmonth(\"FL_DATE\")\n",
    "    }\n",
    "    \n",
    "    for col_name, expr in date_derived_cols.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, F.coalesce(F.col(col_name), expr))\n",
    "    \n",
    "    # 6. Handle weather data with null values - using BACKWARD-LOOKING windows only to prevent data leakage\n",
    "    # Include ALL weather columns that have nulls\n",
    "    weather_cols = [\n",
    "        \"HourlyWindDirection\", \"HourlyAltimeterSetting\", \"HourlySkyConditions\",\n",
    "        \"HourlyVisibility\", \"HourlyDewPointTemperature\", \"HourlyWindSpeed\",\n",
    "        \"HourlyDryBulbTemperature\", \"HourlyPrecipitation\", \"HourlyRelativeHumidity\",\n",
    "        \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyWetBulbTemperature\"\n",
    "    ]\n",
    "    \n",
    "    # Use a backward-looking window to avoid data leakage\n",
    "    if any(col in df.columns for col in weather_cols):\n",
    "        # Create window specs once - only looking at past values (negative range)\n",
    "        # Increase window size to capture more historical data\n",
    "        origin_date_window = Window.partitionBy(\"ORIGIN\", F.to_date(\"FL_DATE\")).orderBy(\"CRS_DEP_TIME\").rowsBetween(-10, 0)\n",
    "        origin_past_window = Window.partitionBy(\"ORIGIN\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-20, 0)\n",
    "        origin_month_window = Window.partitionBy(\"ORIGIN\", \"MONTH\")\n",
    "        \n",
    "        # Process numeric weather columns with backward-looking average\n",
    "        numeric_weather_cols = [\n",
    "            \"HourlyWindDirection\", \"HourlyAltimeterSetting\", \"HourlyVisibility\", \n",
    "            \"HourlyDewPointTemperature\", \"HourlyWindSpeed\", \"HourlyDryBulbTemperature\",\n",
    "            \"HourlyPrecipitation\", \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\", \n",
    "            \"HourlyStationPressure\", \"HourlyWetBulbTemperature\"\n",
    "        ]\n",
    "        \n",
    "        for col in numeric_weather_cols:\n",
    "            if col in df.columns:\n",
    "                # First try rolling average within same day/origin (past values only)\n",
    "                df = df.withColumn(f\"{col}_rolling\", F.avg(F.col(col)).over(origin_date_window))\n",
    "                \n",
    "                # Then try past days at same origin if still null\n",
    "                df = df.withColumn(f\"{col}_past\", F.avg(F.col(col)).over(origin_past_window))\n",
    "                \n",
    "                # Then try origin/month median if still null\n",
    "                df = df.withColumn(f\"{col}_median\", F.expr(f\"percentile_approx({col}, 0.5)\").over(origin_month_window))\n",
    "                \n",
    "                # Apply all imputations in one operation\n",
    "                df = df.withColumn(col, F.coalesce(\n",
    "                    F.col(col),\n",
    "                    F.col(f\"{col}_rolling\"),\n",
    "                    F.col(f\"{col}_past\"),\n",
    "                    F.col(f\"{col}_median\")\n",
    "                ))\n",
    "                \n",
    "                # Drop temporary columns\n",
    "                df = df.drop(f\"{col}_rolling\", f\"{col}_past\", f\"{col}_median\")\n",
    "                \n",
    "                # Final global median fallback for any remaining nulls\n",
    "                median_val = df.filter(F.col(col).isNotNull()).select(\n",
    "                    F.expr(f\"percentile_approx({col}, 0.5)\").alias(\"median\")\n",
    "                ).first()[\"median\"]\n",
    "                \n",
    "                if median_val is not None:\n",
    "                    df = df.withColumn(col, F.coalesce(F.col(col), F.lit(median_val)))\n",
    "                else:\n",
    "                    # If even the global median is null, use a reasonable default based on column type\n",
    "                    if \"Temperature\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(0.0)))  # Default temperature\n",
    "                    elif \"Direction\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(0.0)))  # Default direction\n",
    "                    elif \"Speed\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(0.0)))  # Default speed\n",
    "                    elif \"Precipitation\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(0.0)))  # Default precipitation\n",
    "                    elif \"Humidity\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(50.0)))  # Default humidity\n",
    "                    elif \"Pressure\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(1013.25)))  # Default pressure (standard atmosphere)\n",
    "                    elif \"Visibility\" in col:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(10.0)))  # Default visibility\n",
    "                    else:\n",
    "                        df = df.withColumn(col, F.coalesce(F.col(col), F.lit(0.0)))  # Generic default\n",
    "        \n",
    "        # Handle HourlySkyConditions separately (categorical)\n",
    "        if \"HourlySkyConditions\" in df.columns:\n",
    "            # First try mode within same day/origin\n",
    "            df = df.withColumn(\"sky_mode_day\", \n",
    "                              F.first(F.col(\"HourlySkyConditions\"), ignorenulls=True).over(origin_date_window))\n",
    "            \n",
    "            # Then try mode for origin/month\n",
    "            df = df.withColumn(\"sky_mode_month\", \n",
    "                              F.first(F.col(\"HourlySkyConditions\"), ignorenulls=True).over(origin_month_window))\n",
    "            \n",
    "            # Apply imputations\n",
    "            df = df.withColumn(\"HourlySkyConditions\", F.coalesce(\n",
    "                F.col(\"HourlySkyConditions\"), \n",
    "                F.col(\"sky_mode_day\"),\n",
    "                F.col(\"sky_mode_month\")\n",
    "            ))\n",
    "            \n",
    "            # Drop temporary columns\n",
    "            df = df.drop(\"sky_mode_day\", \"sky_mode_month\")\n",
    "            \n",
    "            # Final global mode fallback\n",
    "            mode_val = df.filter(F.col(\"HourlySkyConditions\").isNotNull()).groupBy(\"HourlySkyConditions\").count() \\\n",
    "                         .orderBy(F.desc(\"count\")).limit(1).select(\"HourlySkyConditions\").first()\n",
    "            \n",
    "            if mode_val is not None:\n",
    "                mode_val = mode_val[\"HourlySkyConditions\"]\n",
    "                df = df.withColumn(\"HourlySkyConditions\", F.coalesce(F.col(\"HourlySkyConditions\"), F.lit(mode_val)))\n",
    "            else:\n",
    "                # Default sky condition if no mode can be determined\n",
    "                df = df.withColumn(\"HourlySkyConditions\", F.coalesce(F.col(\"HourlySkyConditions\"), F.lit(\"CLR\")))\n",
    "    \n",
    "    # 7. Handle DEP_TIME_BLK with nulls\n",
    "    if \"DEP_TIME_BLK\" in df.columns and \"CRS_DEP_TIME\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"DEP_TIME_BLK_DERIVED\", \n",
    "            F.concat(\n",
    "                F.lpad(F.floor(F.col(\"CRS_DEP_TIME\")/F.lit(100)), 2, \"0\"),\n",
    "                F.lit(\"00-\"),\n",
    "                F.lpad(F.floor(F.col(\"CRS_DEP_TIME\")/F.lit(100)) + F.lit(1), 2, \"0\"),\n",
    "                F.lit(\"59\")\n",
    "            )\n",
    "        )\n",
    "        df = df.withColumn(\"DEP_TIME_BLK\", F.coalesce(F.col(\"DEP_TIME_BLK\"), F.col(\"DEP_TIME_BLK_DERIVED\")))\n",
    "        df = df.drop(\"DEP_TIME_BLK_DERIVED\")\n",
    "    \n",
    "    # 8. Handle ARR_TIME_BLK with potential nulls\n",
    "    if \"ARR_TIME_BLK\" in df.columns and \"CRS_ARR_TIME\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"ARR_TIME_BLK_DERIVED\", \n",
    "            F.concat(\n",
    "                F.lpad(F.floor(F.col(\"CRS_ARR_TIME\")/F.lit(100)), 2, \"0\"),\n",
    "                F.lit(\"00-\"),\n",
    "                F.lpad(F.floor(F.col(\"CRS_ARR_TIME\")/F.lit(100)) + F.lit(1), 2, \"0\"),\n",
    "                F.lit(\"59\")\n",
    "            )\n",
    "        )\n",
    "        df = df.withColumn(\"ARR_TIME_BLK\", F.coalesce(F.col(\"ARR_TIME_BLK\"), F.col(\"ARR_TIME_BLK_DERIVED\")))\n",
    "        df = df.drop(\"ARR_TIME_BLK_DERIVED\")\n",
    "    \n",
    "    # 9. Handle timestamp columns\n",
    "    timestamp_cols = [\"sched_depart_date_time_UTC\", \"four_hours_prior_depart_UTC\", \"two_hours_prior_depart_UTC\"]\n",
    "    \n",
    "    if any(col in df.columns for col in timestamp_cols) and \"FL_DATE\" in df.columns and \"CRS_DEP_TIME\" in df.columns:\n",
    "        # Create base timestamp from FL_DATE and CRS_DEP_TIME\n",
    "        df = df.withColumn(\n",
    "            \"base_timestamp\", \n",
    "            F.to_timestamp(\n",
    "                F.concat(\n",
    "                    F.date_format(\"FL_DATE\", \"yyyy-MM-dd\"), \n",
    "                    F.lit(\" \"), \n",
    "                    F.lpad(F.floor(F.col(\"CRS_DEP_TIME\")/F.lit(100)), 2, \"0\"),\n",
    "                    F.lit(\":\"),\n",
    "                    F.lpad(F.col(\"CRS_DEP_TIME\") % F.lit(100), 2, \"0\"),\n",
    "                    F.lit(\":00\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Apply to each timestamp column\n",
    "        if \"sched_depart_date_time_UTC\" in df.columns:\n",
    "            df = df.withColumn(\"sched_depart_date_time_UTC\", \n",
    "                              F.coalesce(F.col(\"sched_depart_date_time_UTC\"), F.col(\"base_timestamp\")))\n",
    "        \n",
    "        if \"four_hours_prior_depart_UTC\" in df.columns:\n",
    "            # Use unix_timestamp for arithmetic on timestamps (4 hours = 14400 seconds)\n",
    "            df = df.withColumn(\"four_hours_prior_depart_UTC\", \n",
    "                              F.coalesce(\n",
    "                                  F.col(\"four_hours_prior_depart_UTC\"), \n",
    "                                  F.from_unixtime(F.unix_timestamp(F.col(\"base_timestamp\")) - F.lit(14400))\n",
    "                              ))\n",
    "        \n",
    "        if \"two_hours_prior_depart_UTC\" in df.columns:\n",
    "            # Use unix_timestamp for arithmetic on timestamps (2 hours = 7200 seconds)\n",
    "            df = df.withColumn(\"two_hours_prior_depart_UTC\", \n",
    "                              F.coalesce(\n",
    "                                  F.col(\"two_hours_prior_depart_UTC\"), \n",
    "                                  F.from_unixtime(F.unix_timestamp(F.col(\"base_timestamp\")) - F.lit(7200))\n",
    "                              ))\n",
    "        \n",
    "        df = df.drop(\"base_timestamp\")\n",
    "    \n",
    "    # 10. Handle REM column (0.02% nulls)\n",
    "    if \"REM\" in df.columns:\n",
    "        df = df.withColumn(\"REM\", F.coalesce(F.col(\"REM\"), F.lit(\"UNKNOWN\")))\n",
    "    \n",
    "    # 11. Handle WindEquipmentChangeDate (28.5% nulls)\n",
    "    if \"WindEquipmentChangeDate\" in df.columns:\n",
    "        # First try to use the most common value per station\n",
    "        if \"STATION\" in df.columns:\n",
    "            station_window = Window.partitionBy(\"STATION\")\n",
    "            df = df.withColumn(\"wind_equip_mode\", \n",
    "                              F.first(F.col(\"WindEquipmentChangeDate\"), ignorenulls=True).over(station_window))\n",
    "            df = df.withColumn(\"WindEquipmentChangeDate\", \n",
    "                              F.coalesce(F.col(\"WindEquipmentChangeDate\"), F.col(\"wind_equip_mode\")))\n",
    "            df = df.drop(\"wind_equip_mode\")\n",
    "        \n",
    "        # Then use a default value for any remaining nulls\n",
    "        df = df.withColumn(\"WindEquipmentChangeDate\", \n",
    "                          F.coalesce(F.col(\"WindEquipmentChangeDate\"), F.lit(\"UNKNOWN\")))\n",
    "    \n",
    "    # 12. Final check for any remaining nulls in any column\n",
    "    for column in df.columns:\n",
    "        # Check if column has any nulls\n",
    "        null_count = df.filter(F.col(column).isNull()).count()\n",
    "        \n",
    "        if null_count > 0:\n",
    "            # For any remaining nulls, use appropriate default values based on column type\n",
    "            col_type = df.schema[column].dataType.simpleString()\n",
    "            \n",
    "            if \"int\" in col_type:\n",
    "                df = df.withColumn(column, F.coalesce(F.col(column), F.lit(0)))\n",
    "            elif \"double\" in col_type or \"decimal\" in col_type:\n",
    "                df = df.withColumn(column, F.coalesce(F.col(column), F.lit(0.0)))\n",
    "            elif \"string\" in col_type:\n",
    "                df = df.withColumn(column, F.coalesce(F.col(column), F.lit(\"UNKNOWN\")))\n",
    "            elif \"timestamp\" in col_type:\n",
    "                # Use FL_DATE as a base for any timestamp\n",
    "                df = df.withColumn(column, \n",
    "                                  F.coalesce(F.col(column), F.to_timestamp(F.col(\"FL_DATE\"))))\n",
    "            elif \"date\" in col_type:\n",
    "                df = df.withColumn(column, F.coalesce(F.col(column), F.col(\"FL_DATE\")))\n",
    "            else:\n",
    "                # For any other types, use null (should not happen after all the above)\n",
    "                pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13735d5-b757-4e39-ab22-94d03d82ae8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60M_complete_parquet_col_filtered_type_conversion = convert_column_types(df_otpw_60M_complete_parquet_col_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0f5a8b-56e5-4d26-9611-b2b9a5e7507c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60M_complete_parquet_col_filtered_cleaned = clean_flight_data(df_otpw_60M_complete_parquet_col_filtered_type_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c38cc58-6a1e-4f8f-8bc4-ec8df99a3ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze missing values in cleaned features\n",
    "null_percentage_df = calculate_null_percentage(df_otpw_60M_complete_parquet_col_filtered_cleaned)\n",
    "display(null_percentage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa9103d-fcc5-41a6-a936-c9979e864a47",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_35f85192\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_7d1ede7e\",\"enabled\":true,\"columnId\":\"OP_CARRIER\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1743615845525}],\"syncTimestamp\":1743615845525}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_otpw_60M_complete_parquet_col_filtered_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a6d548-b645-4a09-b097-e1796487a9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save df_southwest_cleaned as a parquet file\n",
    "df_otpw_60M_complete_parquet_col_filtered_cleaned.write.parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_col_filtered_cleaned.parquet\") # use to save a new version\n",
    "# df_otpw_60M_complete_parquet_col_filtered_cleaned.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_col_filtered_cleaned.parquet\") # use if you want to overwrite exisiting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a330bbd2-df43-43f2-83b9-f7aee7ce7560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/student-groups/Group_04_04/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01639eb-bdfe-4c18-91f7-f6d8c2c663a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read the Cleaned Data and Start Data Engineering and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321084dd-99e4-4471-8a69-6166e6967fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_otpw_60M_complete_parquet_cleaned = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_60M_complete_parquet_col_filtered_cleaned.parquet/\")\n",
    "df_otpw_60M_complete_parquet_cleaned = df_otpw_60M_complete_parquet_col_filtered_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e3785da-4ec4-4046-b8dd-29c722e02c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Comprehensive Feature Engineering for Flight Delay Prediction\n",
    "\n",
    "## Airport Profile Features\n",
    "\n",
    "| Feature | Description | Null Handling | Temporal Integrity |\n",
    "|---------|-------------|---------------|---------------------|\n",
    "| **Origin Airport Daily Operations** | Total number of flights departing from each origin airport on a given day | None needed (always populated) | Current day only |\n",
    "| **Origin Airport 30-Day Rolling Volume** | Sum of flights from the origin airport over the past 30 days | 0 for first 30 days (no history available) | Growing window until 30 days history |\n",
    "| **Origin Airport 1-Year Delay Rate** | Annual delay percentage at origin airport | Global fallback (15%) for first year (2015) rows | Expanding window using all prior data |\n",
    "| **Route** | The origin and destination of the flight | Not needed | Concat the origin and destination in a single window |\n",
    "| **Route Traffic Volume** | Number of flights between specific origin-destination pairs over the past year | 0 for new routes or first year (2015) rows | Expanding window using all prior data |\n",
    "| **Southwest Market Share** | Percentage of flights operated by Southwest at each origin airport over the past year | 0 when no data available for Southwest flights | Rolling 365-day window |\n",
    "| **Southwest Origin 30-Day Delay Rate** | Recent Southwest delay performance at origin airport (past 30 days) | Global fallback (15%) for missing data in the previous 30 days | Growing window until 30 days history |\n",
    "| **Southwest Route Historical Performance** | Southwest's historical delay rate on specific routes over the past year | Global fallback (15%) for missing route data or first year rows | Expanding window using all prior data |\n",
    "| **Southwest Relative Performance Index** | How Southwest compares to other airlines at the same airport (delay rate ratio) | Default value of 1.0 when no data available or division by zero occurs | Ratio with epsilon smoothing to prevent division by zero |\n",
    "\n",
    "\n",
    "## Time-Based Profile Features\n",
    "\n",
    "| Feature | Description | Null Handling | Temporal Integrity |\n",
    "|---------|-------------|---------------|---------------------|\n",
    "| **time_bucket** | 15-minute departure intervals | Derived from CRS_DEP_TIME (always populated) | Current flight only |\n",
    "| **dep_hour** | Hour of day for scheduled departure | None needed | Current flight only |\n",
    "| **time_of_day_category** | Morning/Midday/Evening/Night | Categorical fallback to \"night\" | Current flight only |\n",
    "| **is_weekend** | Weekend flight indicator | None needed | Current flight only |\n",
    "| **holiday_season** | Peak travel period indicator | None needed | Current flight only |\n",
    "| **prior_day_delay_rate** | Previous day's delay rate at origin airport | 3-level fallback: prior day → airport avg → 15% global fallback | Strict date ordering |\n",
    "| **same_day_prior_delay_percentage** | Percentage of flights delayed earlier in the day at the same airport | Additive smoothing (prevents 0/0) and nulls default to 0% delay rate | Same-day ordering |\n",
    "| **time_based_congestion_ratio** | Current vs historical congestion ratio for the same time bucket (hour + 15-min interval) on the same day of the week at the same airport | 3-level fallback: historical average → airport avg → default capacity (10 flights) | 365-day lookback excluding current day |\n",
    "\n",
    "\n",
    "## Weather-Based Profile Features\n",
    "\n",
    "| Feature | Description | Calculation Method | Null Handling |\n",
    "|---------|-------------|--------------------|---------------|\n",
    "| **extreme_precipitation** | Flag for heavy precipitation | 95th percentile of historical precipitation data | 0 if missing |\n",
    "| **extreme_wind** | Flag for high wind conditions | 95th percentile of historical wind speed data | 0 if missing |\n",
    "| **extreme_temperature** | Flag for extreme temperatures | 5th/95th percentiles of historical temperature data | 0 if missing |\n",
    "| **low_visibility** | Flag for poor visibility | 5th percentile of historical visibility data | 0 if missing |\n",
    "| **extreme_weather_score** | Weighted weather risk score | Weighted sum of extreme conditions based on their historical delay impact | Scaled to [-1,1] |\n",
    "| **heat_index** | Perceived temperature | NOAA heat index formula for T ≥ 80°F and RH ≥ 40% | Raw temp otherwise |\n",
    "| **rapid_weather_change** | Significant weather shifts | Z-score > 3 in temp/wind over 24h window | 0 if missing data |\n",
    "| **temp_anomaly_z** | Temperature deviation | Z-score vs. airport-month historical average | 0 if no history |\n",
    "| **precip_anomaly_z** | Precipitation deviation | Z-score vs. airport-month historical average | 0 if no history |\n",
    "\n",
    "\n",
    "## Southwest Airlines Profile Features\n",
    "\n",
    "| Feature | Description | Calculation Method | Null Handling |\n",
    "|---------|-------------|--------------------|---------------|\n",
    "| **sw_time_of_day_delay_rate** | Southwest's delay rate by origin and time bucket | Expanding window average with origin/global fallbacks | Uses origin average → global median |\n",
    "| **sw_day_of_week_delay_rate** | Bayesian-smoothed delay rate by route and weekday | (Delays + 3*global_p30)/(Flights + 3) | Built-in smoothing prevents nulls |\n",
    "| **sw_aircraft_delay_rate** | Aircraft performance metric | Hierarchical: aircraft → route → global median | Always populated |\n",
    "| **sw_origin_hub** | Dynamic hub identification | Top 15th percentile of Southwest flight volume | 0/1 encoding |\n",
    "| **sw_schedule_buffer_ratio** | Schedule padding ratio | Current vs 1-year historical average | Defaults to 1.0 |\n",
    "| **sw_origin_time_perf** | Hybrid airport/time performance | Time bucket → time category → global fallback | Hierarchical coalesce |\n",
    "| **sw_route_importance** | Normalized route significance | (Flight count + distance) normalized | Always 0-2 range |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c9ee47a-8059-4b2f-af03-eaae790bb510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Airport Profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d75bb4-adaf-4f42-8471-da603222bf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_airport_profile_features(df):\n",
    "    \"\"\"\n",
    "    Robust airport profile features with temporal integrity and null handling\n",
    "    \"\"\"\n",
    "    # Time basis for all rolling calculations\n",
    "    # Time basis for all rolling calculations\n",
    "    df = df.withColumn(\"days_since_epoch\", F.datediff(\"FL_DATE\", F.lit(\"1970-01-01\")))\n",
    "    \n",
    "    # 1. Origin Airport Daily Operations\n",
    "    window_daily = Window.partitionBy(\"ORIGIN\", \"FL_DATE\")\n",
    "    df = df.withColumn(\"daily_operations\", F.count(\"*\").over(window_daily))\n",
    "    \n",
    "    # 2. Origin Airport 30-Day Rolling Volume (Fixed window)\n",
    "    window_30d = Window.partitionBy(\"ORIGIN\") \\\n",
    "        .orderBy(\"days_since_epoch\") \\\n",
    "        .rowsBetween(-29, 0)  # 30 rows (days)\n",
    "    \n",
    "    df = df.withColumn(\"rolling_30day_volume\", \n",
    "                      F.coalesce(\n",
    "                          F.sum(\"daily_operations\").over(window_30d),\n",
    "                          F.lit(0)\n",
    "                      ))\n",
    "    \n",
    "    # 3. 1-Year Delay Rate (Fixed window)\n",
    "    window_1yr = Window.partitionBy(\"ORIGIN\") \\\n",
    "        .orderBy(\"days_since_epoch\") \\\n",
    "        .rowsBetween(-365, 0)  # 365 rows (days)\n",
    "    \n",
    "    df = df.withColumn(\"origin_1yr_delay_rate\", \n",
    "                      F.coalesce(\n",
    "                          F.avg(F.col(\"DEP_DEL15\")).over(window_1yr),\n",
    "                          F.lit(0.15)  # Global average fallback\n",
    "                      ))\n",
    "    \n",
    "    # 4. Route Traffic Volume (Expanding window)\n",
    "    window_route = Window.partitionBy(\"ORIGIN\", \"DEST\") \\\n",
    "        .orderBy(\"days_since_epoch\") \\\n",
    "        .rowsBetween(Window.unboundedPreceding, 0)\n",
    "    \n",
    "    df = df.withColumn(\"route_1yr_volume\", \n",
    "                      F.coalesce(\n",
    "                          F.count(\"*\").over(window_route),\n",
    "                          F.lit(0)\n",
    "                      ))\n",
    "    \n",
    "    # 5. Southwest Market Share (Fixed window)\n",
    "    window_1yr_sw = Window.partitionBy(\"ORIGIN\") \\\n",
    "        .orderBy(\"days_since_epoch\") \\\n",
    "        .rowsBetween(-365, 0)  # 365 rows (days)\n",
    "    \n",
    "    df = df.withColumn(\"sw_market_share\",\n",
    "                      F.when(F.count(\"*\").over(window_1yr_sw) > 0,\n",
    "                            F.count(F.when(F.col(\"OP_CARRIER\") == \"WN\", 1)).over(window_1yr_sw)\n",
    "                            / F.count(\"*\").over(window_1yr_sw))\n",
    "                       .otherwise(0))\n",
    "    \n",
    "    # 6. Southwest Origin 30-Day Delay Rate (Same as window_30d)\n",
    "    df = df.withColumn(\"sw_30d_delay\",\n",
    "                      F.coalesce(\n",
    "                          F.avg(F.when(F.col(\"OP_CARRIER\") == \"WN\", F.col(\"DEP_DEL15\")))\n",
    "                            .over(window_30d),\n",
    "                          F.lit(0.15)\n",
    "                      ))\n",
    "    \n",
    "    # 7. Southwest Route Performance (Expanding window)\n",
    "    df = df.withColumn(\"sw_route_delay\",\n",
    "                      F.coalesce(\n",
    "                          F.avg(F.when(F.col(\"OP_CARRIER\") == \"WN\", F.col(\"DEP_DEL15\")))\n",
    "                            .over(window_route),\n",
    "                          F.lit(0.15)\n",
    "                      ))\n",
    "    \n",
    "    # 8. Southwest Relative Performance Index\n",
    "    df = df.withColumn(\"sw_rel_perf\",\n",
    "                      F.when(F.col(\"origin_1yr_delay_rate\") + F.lit(1e-6) > 0,\n",
    "                            (F.col(\"sw_route_delay\") + F.lit(1e-6)) \n",
    "                            / (F.col(\"origin_1yr_delay_rate\") + F.lit(1e-6)))\n",
    "                       .otherwise(1.0))\n",
    "    \n",
    "    # 9. Add route column\n",
    "    df = df.withColumn(\"route\", F.concat_ws(\"-\", \"ORIGIN\", \"DEST\"))\n",
    "    \n",
    "    return df.drop(\"days_since_epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0caf6d87-b6cc-4d01-9776-02612f86e583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Time-Based Profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e231b51c-688f-4ca8-8ded-59676bb6addf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_time_based_features(df):\n",
    "    \"\"\"\n",
    "    Enhanced time-based features with 15-minute buckets and robust null handling\n",
    "    \"\"\"\n",
    "    print(\"Adding time-based features...\")\n",
    "    \n",
    "    # 1. Time Bucket Features\n",
    "    df = (df\n",
    "          .withColumn(\"dep_hour\", (F.col(\"CRS_DEP_TIME\") / 100).cast(\"int\"))\n",
    "          .withColumn(\"dep_minute\", (F.col(\"CRS_DEP_TIME\") % 100))\n",
    "          .withColumn(\"time_bucket_15min\", \n",
    "                     (F.col(\"dep_minute\") / 15).cast(\"int\"))\n",
    "          .withColumn(\"time_bucket\", \n",
    "                     F.concat_ws(\"_\", \"dep_hour\", \"time_bucket_15min\"))\n",
    "         )\n",
    "    \n",
    "    # 2. Time Categories\n",
    "    df = (df\n",
    "          .withColumn(\"time_of_day_category\",\n",
    "                     F.when(F.col(\"dep_hour\").between(5, 9), \"morning\")\n",
    "                      .when(F.col(\"dep_hour\").between(10, 15), \"midday\")\n",
    "                      .when(F.col(\"dep_hour\").between(16, 19), \"evening\")\n",
    "                      .otherwise(\"night\"))\n",
    "          .withColumn(\"is_weekend\", F.when(F.col(\"DAY_OF_WEEK\").isin(6, 7), 1).otherwise(0))\n",
    "          .withColumn(\"holiday_season\", F.when(F.col(\"MONTH\").isin(6, 7, 12), 1).otherwise(0))\n",
    "         )\n",
    "    \n",
    "    # 3. Daily Operations (prerequisite for other features)\n",
    "    window_daily = Window.partitionBy(\"ORIGIN\", \"FL_DATE\")\n",
    "    df = df.withColumn(\"daily_operations\", F.count(\"*\").over(window_daily))\n",
    "    \n",
    "    # 4. Prior Day Delay Rate with Hierarchical Fallbacks\n",
    "    window_prior_day = Window.partitionBy(\"ORIGIN\").orderBy(\"FL_DATE\")\n",
    "    window_origin_avg = Window.partitionBy(\"ORIGIN\")\n",
    "    \n",
    "    df = (df\n",
    "          .withColumn(\"daily_delay_rate\",\n",
    "                     F.avg(F.col(\"DEP_DEL15\")).over(window_daily))\n",
    "          .withColumn(\"prior_day_delay_rate\",\n",
    "                     F.coalesce(\n",
    "                         F.lag(\"daily_delay_rate\").over(window_prior_day),\n",
    "                         F.avg(F.col(\"DEP_DEL15\")).over(window_origin_avg),\n",
    "                         F.lit(0.15)  # Global average fallback\n",
    "                     ))\n",
    "         )\n",
    "    \n",
    "    # 5. Same-Day Prior Delays with Zero-Count Protection\n",
    "    window_same_day = Window.partitionBy(\"ORIGIN\", \"FL_DATE\").orderBy(\"CRS_DEP_TIME\")\n",
    "    df = (df\n",
    "          .withColumn(\"prior_flights_today\",\n",
    "                     F.coalesce(\n",
    "                         F.count(\"*\").over(window_same_day.rowsBetween(Window.unboundedPreceding, -1)),\n",
    "                         F.lit(0)\n",
    "                     ))\n",
    "          .withColumn(\"prior_delays_today\",\n",
    "                     F.coalesce(\n",
    "                         F.sum(F.col(\"DEP_DEL15\")).over(window_same_day.rowsBetween(Window.unboundedPreceding, -1)),\n",
    "                         F.lit(0)\n",
    "                     ))\n",
    "          .withColumn(\"same_day_prior_delay_percentage\",\n",
    "                     (F.col(\"prior_delays_today\") + F.lit(0.5)) / \n",
    "                     (F.col(\"prior_flights_today\") + F.lit(1)))\n",
    "         )\n",
    "    \n",
    "    # 6. Time-Based Congestion Ratio with 3-Level Fallback\n",
    "    window_historical = Window.partitionBy(\"ORIGIN\", \"DAY_OF_WEEK\", \"dep_hour\") \\\n",
    "        .orderBy(\"FL_DATE\") \\\n",
    "        .rowsBetween(-365, -1)  # Exclude current day\n",
    "    \n",
    "    df = (df\n",
    "          .withColumn(\"avg_flights_per_dow_hour\",\n",
    "                     F.coalesce(\n",
    "                         F.avg(\"daily_operations\").over(window_historical),\n",
    "                         F.avg(\"daily_operations\").over(Window.partitionBy(\"ORIGIN\")),\n",
    "                         F.lit(10)  # Default small airport capacity\n",
    "                     ))\n",
    "          .withColumn(\"time_based_congestion_ratio\",\n",
    "                     F.when(F.col(\"avg_flights_per_dow_hour\") > 0,\n",
    "                           F.col(\"daily_operations\") / F.col(\"avg_flights_per_dow_hour\"))\n",
    "                      .otherwise(1.0))\n",
    "         )\n",
    "    \n",
    "    # Cleanup temporary columns\n",
    "    return df.drop(\"dep_minute\", \"time_bucket_15min\", \"daily_delay_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bcb98d5-4e6b-4bbf-990a-a3c09670bf2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Weather-Based Profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa182b3-99d2-4970-826d-235e0fd3803d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_weather_based_features(df):\n",
    "    \"\"\"\n",
    "    Enhanced weather features with data-driven thresholds and statistical weighting\n",
    "    \"\"\"\n",
    "    print(\"Adding weather-based features...\")\n",
    "    \n",
    "    # 1. Extreme Weather Indicators (Dynamic Percentile Thresholds)\n",
    "    weather_metrics = [\"HourlyPrecipitation\", \"HourlyWindSpeed\", \n",
    "                      \"HourlyDryBulbTemperature\", \"HourlyVisibility\"]\n",
    "    \n",
    "    percentiles = {metric: df.approxQuantile(metric, [0.05, 0.95], 0.05)\n",
    "                   for metric in weather_metrics}\n",
    "    \n",
    "    df = (df\n",
    "          .withColumn(\"extreme_precipitation\", F.when(F.col(\"HourlyPrecipitation\") >= percentiles[\"HourlyPrecipitation\"][1], 1).otherwise(0))\n",
    "          .withColumn(\"extreme_wind\", F.when(F.col(\"HourlyWindSpeed\") >= percentiles[\"HourlyWindSpeed\"][1], 1).otherwise(0))\n",
    "          .withColumn(\"extreme_temperature\", F.when(\n",
    "              (F.col(\"HourlyDryBulbTemperature\") <= percentiles[\"HourlyDryBulbTemperature\"][0]) |\n",
    "              (F.col(\"HourlyDryBulbTemperature\") >= percentiles[\"HourlyDryBulbTemperature\"][1]), 1).otherwise(0))\n",
    "          .withColumn(\"low_visibility\", F.when(F.col(\"HourlyVisibility\") <= percentiles[\"HourlyVisibility\"][0], 1).otherwise(0))\n",
    "         )\n",
    "    \n",
    "    # 2. Empirical Risk Score (Explicit type handling)\n",
    "    delay_rates = {}\n",
    "    for condition in [\"extreme_precipitation\", \"extreme_wind\", \n",
    "                     \"extreme_temperature\", \"low_visibility\"]:\n",
    "        # Handle potential None values explicitly\n",
    "        condition_agg = df.filter(F.col(condition) == 1).agg(F.avg(\"DEP_DEL15\").alias(\"rate\"))\n",
    "        condition_rate = condition_agg.first()[\"rate\"] or 0.15\n",
    "        \n",
    "        baseline_agg = df.filter(F.col(condition) == 0).agg(F.avg(\"DEP_DEL15\").alias(\"rate\")) \n",
    "        baseline_rate = baseline_agg.first()[\"rate\"] or 0.15\n",
    "        \n",
    "        # Explicit type conversion and handling\n",
    "        impact = float(condition_rate) - float(baseline_rate)\n",
    "        delay_rates[condition] = impact if impact > 0 else 0.0\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_impact = sum(delay_rates.values())\n",
    "    weights = {k: (v/total_impact if total_impact > 0 else 0.25) \n",
    "              for k, v in delay_rates.items()}\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"extreme_weather_score\",\n",
    "        (F.col(\"extreme_precipitation\") * weights[\"extreme_precipitation\"] +\n",
    "         F.col(\"extreme_wind\") * weights[\"extreme_wind\"] +\n",
    "         F.col(\"extreme_temperature\") * weights[\"extreme_temperature\"] +\n",
    "         F.col(\"low_visibility\") * weights[\"low_visibility\"])\n",
    "    )\n",
    "    \n",
    "    # 3. Heat Index Calculation (No changes needed)\n",
    "    df = df.withColumn(\n",
    "        \"heat_index\",\n",
    "        F.when(\n",
    "            (F.col(\"HourlyDryBulbTemperature\") >= 80) &\n",
    "            (F.col(\"HourlyRelativeHumidity\") >= 40),\n",
    "            -42.379 + 2.04901523*F.col(\"HourlyDryBulbTemperature\") +\n",
    "            10.14333127*F.col(\"HourlyRelativeHumidity\") -\n",
    "            0.22475541*F.col(\"HourlyDryBulbTemperature\")*F.col(\"HourlyRelativeHumidity\") -\n",
    "            6.83783e-3*F.pow(F.col(\"HourlyDryBulbTemperature\"), 2) -\n",
    "            5.481717e-2*F.pow(F.col(\"HourlyRelativeHumidity\"), 2) +\n",
    "            1.22874e-3*F.pow(F.col(\"HourlyDryBulbTemperature\"), 2)*F.col(\"HourlyRelativeHumidity\") +\n",
    "            8.5282e-4*F.col(\"HourlyDryBulbTemperature\")*F.pow(F.col(\"HourlyRelativeHumidity\"), 2) -\n",
    "            1.99e-6*F.pow(F.col(\"HourlyDryBulbTemperature\"), 2)*F.pow(F.col(\"HourlyRelativeHumidity\"), 2)\n",
    "        ).otherwise(F.col(\"HourlyDryBulbTemperature\"))\n",
    "    )\n",
    "    \n",
    "    # 4. Weather Volatility Metrics (Proper PySpark functions)\n",
    "    window_weather = Window.partitionBy(\"ORIGIN\").orderBy(\"FL_DATE\", \"CRS_DEP_TIME\").rowsBetween(-24*7, 0)\n",
    "    \n",
    "    for metric in [\"HourlyDryBulbTemperature\", \"HourlyWindSpeed\"]:\n",
    "        df = (df\n",
    "              .withColumn(f\"{metric}_mean\", F.avg(metric).over(window_weather))\n",
    "              .withColumn(f\"{metric}_std\", F.stddev(metric).over(window_weather))\n",
    "              .withColumn(f\"{metric}_z\", \n",
    "                         (F.col(metric) - F.col(f\"{metric}_mean\")) / F.col(f\"{metric}_std\"))\n",
    "             )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"rapid_weather_change\",\n",
    "        F.when(\n",
    "            (F.abs(F.col(\"HourlyDryBulbTemperature_z\")) > 3) |\n",
    "            (F.abs(F.col(\"HourlyWindSpeed_z\")) > 3), 1\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # 5. Standardized Weather Anomalies\n",
    "    window_seasonal = Window.partitionBy(\"ORIGIN\", \"MONTH\")\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"temp_anomaly_z\",\n",
    "        F.when(\n",
    "            (F.col(\"HourlyDryBulbTemperature\") - F.avg(\"HourlyDryBulbTemperature\").over(window_seasonal)) / \n",
    "            F.stddev(\"HourlyDryBulbTemperature\").over(window_seasonal) > 0,\n",
    "            (F.col(\"HourlyDryBulbTemperature\") - F.avg(\"HourlyDryBulbTemperature\").over(window_seasonal)) / \n",
    "            F.stddev(\"HourlyDryBulbTemperature\").over(window_seasonal)\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    # Cleanup intermediate columns\n",
    "    columns_to_drop = [f\"{metric}_{stat}\" \n",
    "                      for metric in [\"HourlyDryBulbTemperature\", \"HourlyWindSpeed\"]\n",
    "                      for stat in [\"mean\", \"std\", \"z\"]]\n",
    "    \n",
    "    return df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e8b537-7753-4688-b33a-6a0898415248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Southwest Airlines Profile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1785cd16-8103-43c2-8670-73ba96ef3020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_southwest_profile_features(df):\n",
    "    \"\"\"\n",
    "    Fixed Southwest features with proper windowing and global aggregations\n",
    "    \"\"\"\n",
    "    print(\"Adding Southwest Airlines profile features...\")\n",
    "    \n",
    "    # Calculate global percentiles once\n",
    "    global_delay_median = df.approxQuantile(\"DEP_DEL15\", [0.5], 0.05)[0]\n",
    "    global_delay_p30 = df.approxQuantile(\"DEP_DEL15\", [0.3], 0.05)[0]\n",
    "    \n",
    "    # Calculate global averages for route importance\n",
    "    total_flights = df.count()\n",
    "    avg_distance = df.agg(F.avg(\"DISTANCE\")).first()[0]\n",
    "\n",
    "    # 1. Time-of-Day Performance\n",
    "    window_time_expanding = Window.partitionBy(\"ORIGIN\", \"time_of_day_category\") \\\n",
    "        .orderBy(\"FL_DATE\") \\\n",
    "        .rowsBetween(Window.unboundedPreceding, -1)\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"sw_time_of_day_delay_rate\",\n",
    "        F.coalesce(\n",
    "            F.avg(F.when(F.col(\"OP_CARRIER\") == \"WN\", F.col(\"DEP_DEL15\")))\n",
    "              .over(window_time_expanding),\n",
    "            F.avg(F.col(\"DEP_DEL15\")).over(Window.partitionBy(\"ORIGIN\")),\n",
    "            F.lit(global_delay_median)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 2. Day-of-Week Performance\n",
    "    window_dow = Window.partitionBy(\"ORIGIN\", \"DEST\", \"DAY_OF_WEEK\")\n",
    "    df = df.withColumn(\n",
    "        \"sw_day_of_week_delay_rate\",\n",
    "        (F.sum(F.col(\"DEP_DEL15\")).over(window_dow) + 3*global_delay_p30) / \n",
    "        (F.count(\"*\").over(window_dow) + 3)\n",
    "    )\n",
    "    \n",
    "    # 3. Aircraft Performance\n",
    "    window_aircraft = Window.partitionBy(\"TAIL_NUM\")\n",
    "    window_route = Window.partitionBy(\"ORIGIN\", \"DEST\")\n",
    "    df = df.withColumn(\n",
    "        \"sw_aircraft_delay_rate\",\n",
    "        F.coalesce(\n",
    "            F.avg(F.col(\"DEP_DEL15\")).over(window_aircraft),\n",
    "            F.avg(F.col(\"DEP_DEL15\")).over(window_route),\n",
    "            F.lit(global_delay_median)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 4. Dynamic Hub Identification\n",
    "    hub_threshold_df = df.filter(F.col(\"OP_CARRIER\") == \"WN\") \\\n",
    "        .groupBy(\"ORIGIN\") \\\n",
    "        .agg(F.count(\"*\").alias(\"sw_volume\"))\n",
    "    \n",
    "    hub_threshold = hub_threshold_df.approxQuantile(\"sw_volume\", [0.85], 0.05)[0]\n",
    "    hubs = hub_threshold_df.filter(F.col(\"sw_volume\") >= hub_threshold) \\\n",
    "                         .select(\"ORIGIN\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    df = df.withColumn(\"sw_origin_hub\", F.col(\"ORIGIN\").isin(hubs).cast(\"int\"))\n",
    "    \n",
    "    # 5. Schedule Buffer Ratio\n",
    "    window_route_history = Window.partitionBy(\"ORIGIN\", \"DEST\") \\\n",
    "        .orderBy(\"FL_DATE\") \\\n",
    "        .rowsBetween(-365, -1)\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"sw_schedule_buffer_ratio\",\n",
    "        F.coalesce(\n",
    "            F.col(\"CRS_ELAPSED_TIME\") / F.avg(\"CRS_ELAPSED_TIME\").over(window_route_history),\n",
    "            F.lit(1.0)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 6. Origin-Time Performance\n",
    "    window_origin_time = Window.partitionBy(\"ORIGIN\", \"time_bucket\")\n",
    "    df = df.withColumn(\n",
    "        \"sw_origin_time_perf\",\n",
    "        F.coalesce(\n",
    "            F.avg(F.col(\"DEP_DEL15\")).over(window_origin_time),\n",
    "            F.col(\"sw_time_of_day_delay_rate\"),\n",
    "            F.lit(global_delay_median)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 7. Route Importance Score\n",
    "    route_window = Window.partitionBy(\"ORIGIN\", \"DEST\")\n",
    "    df = df.withColumn(\n",
    "        \"sw_route_importance\",\n",
    "        (F.count(\"*\").over(route_window) / F.lit(total_flights)) + \n",
    "        (F.avg(\"DISTANCE\").over(route_window) / F.lit(avg_distance))\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31dcd1bc-2f7a-445a-9475-325129b6666e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_all_features(df):\n",
    "    \"\"\"\n",
    "    Master function to add all engineered features to the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with flight data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all added features\n",
    "    \"\"\"\n",
    "    print(\"Starting feature engineering process...\")\n",
    "    \n",
    "    # Apply each feature set in sequence\n",
    "    print(\"Adding airport profile features...\")\n",
    "    df = add_airport_profile_features(df)\n",
    "    \n",
    "    print(\"Adding time-based features...\")\n",
    "    df = add_time_based_features(df)\n",
    "    \n",
    "    # print(\"Adding weather-based features...\")\n",
    "    # df = add_weather_based_features(df)\n",
    "    \n",
    "    print(\"Adding Southwest Airlines profile features...\")\n",
    "    df = add_southwest_profile_features(df)\n",
    "    \n",
    "    print(\"Feature engineering complete!\")\n",
    "    \n",
    "    # Calculate the number of features added\n",
    "    original_cols = set(df.columns)\n",
    "    engineered_cols = [col for col in df.columns if col not in original_cols]\n",
    "    print(f\"Added {len(engineered_cols)} new features to the DataFrame.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f657140-e813-46e4-a688-ac1ca04df79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_otpw_60M_complete_parquet_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2645b2-d4aa-481f-9347-0c88cace6b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call the master function on your cleaned DataFrame\n",
    "df_otpw_60M_complete_parquet_with_features = add_all_features(df_otpw_60M_complete_parquet_cleaned)\n",
    "\n",
    "# Display the schema of the final DataFrame to verify all features were added\n",
    "print(\"Schema of final DataFrame with all features:\")\n",
    "df_otpw_60M_complete_parquet_with_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae85835-84bc-423e-a656-c88d54c3376f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count rows and columns in the final DataFrame\n",
    "row_count = df_otpw_60M_complete_parquet_with_features.count()\n",
    "col_count = len(df_otpw_60M_complete_parquet_with_features.columns)\n",
    "print(f\"Final DataFrame contains {row_count:,} rows and {col_count} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e9e799-df9b-48ac-aad3-53fcbda18e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save the final DataFrame for future use\n",
    "# df_otpw_60M_complete_parquet_with_features.write.parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_with_features_custom_join.parquet\")\n",
    "df_otpw_60M_complete_parquet_with_features.write.parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_with_features_custom_join_2015_2021.parquet\")\n",
    "# df_otpw_60M_complete_parquet_with_features.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_with_features_custom_join.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5041da94-8b13-49e1-99b5-a8f91cb7e177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/student-groups/Group_04_04/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d2596c2-8211-4df3-bf24-e2534a77a67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_otpw_60M_complete_parquet_with_additional_features = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_60M_complete_parquet_with_features_custom_join_2015_2021.parquet/\")\n",
    "df_otpw_60M_complete_parquet_with_additional_features = df_otpw_60M_complete_parquet_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c70da0-dd0e-4780-bd4c-b5fd682e9f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_otpw_60M_complete_parquet_with_additional_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3374cec7-90df-4a60-967e-e6a00e7f3e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill nulls in specific anomaly columns\n",
    "df_otpw_60M_complete_parquet_with_additional_features = (\n",
    "    df_otpw_60M_complete_parquet_with_additional_features\n",
    "    .withColumn(\"temp_anomaly_z\", F.coalesce(F.col(\"temp_anomaly_z\"), F.lit(0.0)))\n",
    "    # .withColumn(\"precip_anomaly_z\", F.coalesce(F.col(\"precip_anomaly_z\"), F.lit(0.0)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1af1fe-3d07-4bdc-ac39-8585d258e041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update DEP_DEL15 column: change 2 (cancelled) to 1 (delayed)\n",
    "df_otpw_60M_complete_parquet_with_additional_features_modified = df_otpw_60M_complete_parquet_with_additional_features.withColumn(\n",
    "    \"DEP_DEL15\",\n",
    "    F.when(F.col(\"DEP_DEL15\") == 2, 1).otherwise(F.col(\"DEP_DEL15\"))\n",
    ")\n",
    "\n",
    "# Verify the change\n",
    "cancelled_count = df_otpw_60M_complete_parquet_with_additional_features_modified.filter(F.col(\"DEP_DEL15\") == 2).count()\n",
    "print(f\"Number of flights still marked as cancelled (should be 0): {cancelled_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca86ab5-28a7-4287-b7e8-a6ae032c8deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_otpw_60M_complete_parquet_with_additional_features_modified.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa571d3-9a5d-48b0-aa8a-cf445ca688da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter for Southwest Airlines flights only (carrier code \"WN\")\n",
    "df_southwest = df_otpw_60M_complete_parquet_with_additional_features_modified.filter(\n",
    "    df_otpw_60M_complete_parquet_with_additional_features[\"OP_CARRIER\"] == \"WN\"\n",
    ")\n",
    "\n",
    "# Check the number of rows after filtering\n",
    "southwest_count = df_southwest.count()\n",
    "print(f\"Number of Southwest Airlines flights: {southwest_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4e4e58-d2af-4ed4-b3ac-717aef84008d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze missing values in cleaned features\n",
    "null_percentage_df = calculate_null_percentage(df_southwest)\n",
    "display(null_percentage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06d73a5-ba8e-434a-95ff-6171f5726866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the current DataFrame schema and column count before dropping\n",
    "print(\"Original DataFrame column count:\", len(df_southwest.columns))\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    \"FL_DATE\", \"OP_UNIQUE_CARRIER\", \"OP_CARRIER_AIRLINE_ID\", \"OP_CARRIER\", \"TAIL_NUM\", \"OP_CARRIER_FL_NUM\",\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\", \"ORIGIN_CITY_MARKET_ID\", \"ORIGIN\", \"ORIGIN_CITY_NAME\",\n",
    "    \"ORIGIN_STATE_FIPS\", \"ORIGIN_STATE_NM\", \"ORIGIN_WAC\",\n",
    "    \"DEST_AIRPORT_SEQ_ID\", \"DEST_CITY_MARKET_ID\", \"DEST\", \"DEST_CITY_NAME\",\n",
    "    \"DEST_STATE_FIPS\", \"DEST_STATE_NM\", \"DEST_WAC\", \n",
    "    \"DEP_TIME_BLK\", \"CRS_ARR_TIME\", \"ARR_TIME_BLK\", \"CANCELLED\", \"DIVERTED\",\n",
    "    \"FLIGHTS\", \"DIV_AIRPORT_LANDINGS\",\n",
    "    \"sched_depart_date_time\", \"sched_depart_date_time_UTC\", \"depart_date_time\", \"depart_date_time_UTC\",\n",
    "    \"arrival_date_time_UTC\", \"four_hours_prior_depart_UTC\", \"two_hours_prior_depart_UTC\",\n",
    "    \"origin_airport_name\", \"origin_station_id\", \"origin_station_name\", \"origin_dis\",\n",
    "    \"dest_airport_name\", \"dest_station_id\", \"dest_station_name\", \"dest_dis\",\n",
    "    \"PREV_ORIGIN\", \"PREV_arrival_date_time_UTC\", \"PREV_ARR_DELAY\", \"PREV_ARR_DELAY_NEW\",\n",
    "    \"STATION\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"REM\",\n",
    "    \"WindEquipmentChangeDate\", \"DATE\"\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "df_southwest_reduced = df_southwest.drop(*columns_to_drop)\n",
    "\n",
    "# Display the remaining column count\n",
    "print(\"Reduced DataFrame column count:\", len(df_southwest_reduced.columns))\n",
    "\n",
    "# Display the remaining columns for reference\n",
    "print(\"\\nRemaining columns:\")\n",
    "for col in df_southwest_reduced.columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4025d0-d4f0-4fb2-8895-99784f8583c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(df_southwest_reduced.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56494412-e646-4ee4-b574-149724691c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, isnan\n",
    "\n",
    "# Convert the 'time_bucket' column to a numeric type\n",
    "df_southwest_reduced = df_southwest_reduced.withColumn(\"time_bucket\", \n",
    "                                                     df_southwest_reduced[\"time_bucket\"].cast(\"double\"))\n",
    "\n",
    "# List of numeric columns\n",
    "numeric_cols = [\n",
    "    \"QUARTER\", \"MONTH\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\", \"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\",\n",
    "    \"DEP_DEL15\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"DISTANCE_GROUP\", \"YEAR\",\n",
    "    \"origin_airport_lat\", \"origin_airport_lon\", \"dest_airport_lat\", \"dest_airport_lon\",\n",
    "    \"PREV_CANCELLED\", \"MINUTES_BETWEEN_FLIGHTS\", \"PREV_ARR_DEL15\",\n",
    "    \"HourlyAltimeterSetting\", \"HourlyDewPointTemperature\", \"HourlyDryBulbTemperature\",\n",
    "    \"HourlyPrecipitation\", \"HourlyRelativeHumidity\", \"HourlySeaLevelPressure\",\n",
    "    \"HourlyStationPressure\", \"HourlyVisibility\", \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyWindDirection\", \"HourlyWindSpeed\", \"daily_operations\", \"rolling_30day_volume\",\n",
    "    \"origin_1yr_delay_rate\", \"route_1yr_volume\", \"sw_market_share\", \"sw_30d_delay\",\n",
    "    \"sw_route_delay\", \"sw_rel_perf\", \"dep_hour\", \"is_weekend\",\n",
    "    \"holiday_season\", \"prior_day_delay_rate\", \"prior_flights_today\", \"prior_delays_today\",\n",
    "    \"same_day_prior_delay_percentage\", \"avg_flights_per_dow_hour\",\n",
    "    \"time_based_congestion_ratio\", \"extreme_precipitation\", \"extreme_wind\",\n",
    "    \"extreme_temperature\", \"low_visibility\", \"extreme_weather_score\", \"heat_index\",\n",
    "    \"rapid_weather_change\", \"temp_anomaly_z\",\n",
    "    \"sw_time_of_day_delay_rate\", \"sw_day_of_week_delay_rate\", \"sw_aircraft_delay_rate\",\n",
    "    \"sw_origin_hub\", \"sw_schedule_buffer_ratio\", \"sw_origin_time_perf\", \"sw_route_importance\"\n",
    "]\n",
    "\n",
    "# First, handle null values in all numeric columns\n",
    "for col_name in numeric_cols:\n",
    "    df_southwest_reduced = df_southwest_reduced.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull() | isnan(col(col_name)), 0).otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Now create the vector assembler with handleInvalid=\"skip\"\n",
    "vector_col = \"correlation_features\"\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=vector_col, handleInvalid=\"skip\")\n",
    "df_vector = assembler.transform(df_southwest_reduced).select(vector_col)\n",
    "\n",
    "# Compute correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col).collect()[0][0]\n",
    "corr_matrix = matrix.toArray().tolist()\n",
    "\n",
    "# Convert to pandas DataFrame for visualization\n",
    "corr_df = pd.DataFrame(corr_matrix, columns=numeric_cols, index=numeric_cols)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(corr_df, annot=False, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.tight_layout()\n",
    "display(plt.gcf())  # Display the figure in Databricks\n",
    "\n",
    "# Save the figure if needed\n",
    "plt.savefig(\"/dbfs/FileStore/pearson_correlation_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Identify highly correlated features\n",
    "threshold = 0.8\n",
    "high_corr = []\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        if abs(corr_matrix[i][j]) > threshold:\n",
    "            high_corr.append((numeric_cols[i], numeric_cols[j], corr_matrix[i][j]))\n",
    "\n",
    "print(\"\\nHighly correlated features (correlation > 0.8):\")\n",
    "for feat1, feat2, corr in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"{feat1} - {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ce3f9c-de7b-4186-86b9-4b845cd63f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create year-quarter and quarter-month combined columns\n",
    "df_reduced = df_southwest_reduced.withColumn(\n",
    "    \"year_quarter\", \n",
    "    concat(col(\"YEAR\").cast(\"string\"), lit(\"-Q\"), col(\"QUARTER\").cast(\"string\"))\n",
    ").withColumn(\n",
    "    \"quarter_month\", \n",
    "    concat(col(\"QUARTER\").cast(\"string\"), lit(\"-\"), col(\"MONTH\").cast(\"string\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb4a33a-7781-41cd-b765-cc36d6e7ac99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    \"DISTANCE\", \n",
    "    \"low_visibility\", \n",
    "    \"HourlyDryBulbTemperature\", \n",
    "    \"DISTANCE_GROUP\", \n",
    "    \"CRS_ELAPSED_TIME\", \n",
    "    \"QUARTER\", \n",
    "    \"HourlyAltimeterSetting\", \n",
    "    \"avg_flights_per_dow_hour\", \n",
    "    \"HourlyWetBulbTemperature\", \n",
    "    \"HourlyDewPointTemperature\",\n",
    "    \"HourlyDryBulbTemperature\" \n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df_final = df_reduced.drop(*columns_to_drop)\n",
    "\n",
    "# Check the number of remaining columns\n",
    "remaining_columns = len(df_final.columns)\n",
    "print(f\"Number of columns after dropping redundant features: {remaining_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3f8ca8-d1ec-45ae-af67-57f10a17c990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75a56f7-36f3-432b-891e-c923ad82d6cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save df_southwest_cleaned as a parquet file\n",
    "df_final.write.parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_final_custom_join_2015-2021.parquet\") # use to save a new version\n",
    "# df_final.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_final_custom_join.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "332db114-3323-4bf3-b8c9-529ea858d6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Start Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5c11a5-156e-46b1-b1b1-fccb55822148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_60M_complete_parquet_final_custom_join.parquet/\")\n",
    "df_final = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_60M_complete_parquet_final_custom_join_with_graph_features.parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0773964a-8eef-48dd-b63d-95969f51a352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa197eaa-81df-46f8-86e1-3e1d72b83c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# years for PageRank\n",
    "years = [row.YEAR for row in df_final.select(\"YEAR\").distinct().collect()]\n",
    "years.sort()\n",
    "print(years)\n",
    "\n",
    "# all year_quarter values for InDegree/OutDegree\n",
    "year_quarters = [\n",
    "    row[\"year_quarter\"] \n",
    "    for row in df_final.select(\"year_quarter\").distinct().collect()\n",
    "]\n",
    "year_quarters.sort()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7a8b3fd-52b9-44b4-86c7-57d5011573ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b61700bf-a6b6-4225-94a6-9ef6db4ad6b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "all_pagerank_years = []  # list for pagerank for each year\n",
    "\n",
    "for y in years:\n",
    "    print(f\"Computing PageRank for year = {y}\")\n",
    "    #Filter DataFrame to this year\n",
    "    df_year = df_final.filter(F.col(\"YEAR\") == y)\n",
    "\n",
    "    # Create Vertices (Airports)\n",
    "    # origin side\n",
    "    origin_airports = (\n",
    "        df_year.selectExpr(\n",
    "            \"ORIGIN_AIRPORT_ID as airport_id\",\n",
    "            \"ORIGIN_STATE_ABR as state_abbr\",\n",
    "            \"origin_airport_lat as lat\",\n",
    "            \"origin_airport_lon as lon\",\n",
    "            \"origin_type as airport_type\"\n",
    "        )\n",
    "        .dropDuplicates([\"airport_id\"])\n",
    "        .withColumn(\"priority\", F.lit(1))\n",
    "    )\n",
    "\n",
    "    # destination side\n",
    "    dest_airports = (\n",
    "        df_year.selectExpr(\n",
    "            \"DEST_AIRPORT_ID as airport_id\",\n",
    "            \"DEST_STATE_ABR as state_abbr\",\n",
    "            \"dest_airport_lat as lat\",\n",
    "            \"dest_airport_lon as lon\",\n",
    "            \"dest_type as airport_type\"\n",
    "        )\n",
    "        .dropDuplicates([\"airport_id\"])\n",
    "        .withColumn(\"priority\", F.lit(2))\n",
    "    )\n",
    "\n",
    "    union_airports = origin_airports.union(dest_airports)\n",
    "\n",
    "    window = Window.partitionBy(\"airport_id\").orderBy(\"priority\")\n",
    "    all_airports = (\n",
    "        union_airports\n",
    "        .withColumn(\"row_num\", F.row_number().over(window))\n",
    "        .filter(\"row_num = 1\")\n",
    "        .drop(\"row_num\", \"priority\")\n",
    "    )\n",
    "\n",
    "    # renaming 'id' for GraphFrames\n",
    "    airports_vertices = all_airports.selectExpr(\n",
    "        \"airport_id as id\",\n",
    "        \"state_abbr\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"airport_type\"\n",
    "    )\n",
    "    # creating edges dataframe\n",
    "    # Weighted by avg prior_day_delay_rate for each (origin, dest) in this year\n",
    "    flight_edges = (\n",
    "        df_year\n",
    "        .groupBy(\"ORIGIN_AIRPORT_ID\", \"DEST_AIRPORT_ID\")\n",
    "        .agg(F.avg(\"prior_day_delay_rate\").alias(\"weight\"))\n",
    "        .selectExpr(\n",
    "            \"ORIGIN_AIRPORT_ID as src\",\n",
    "            \"DEST_AIRPORT_ID as dst\",\n",
    "            \"weight\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Run PageRank for each year\n",
    "    g = GraphFrame(airports_vertices, flight_edges)\n",
    "    results = g.pageRank(\n",
    "        resetProbability=0.15,\n",
    "        tol=0.0001,\n",
    "\n",
    "    )\n",
    "    print(f\"Airports: {g.vertices.count()}\")\n",
    "    print(f\"Trips: {g.edges.count()}\")   \n",
    "\n",
    "    # results (id, pagerank)\n",
    "    pagerank_vertices = results.vertices.select(\"id\", \"pagerank\")\n",
    "\n",
    "    \n",
    "    #Store This Year's PageRank\n",
    "    pagerank_for_year = pagerank_vertices.withColumn(\"YEAR\", F.lit(y))\n",
    "    all_pagerank_years.append(pagerank_for_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f92ff9-5154-4aff-9a48-df929d582319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if all_pagerank_years:\n",
    "    pagerank_all_years = all_pagerank_years[0]\n",
    "    for df_rest in all_pagerank_years[1:]:\n",
    "        pagerank_all_years = pagerank_all_years.union(df_rest)\n",
    "else:\n",
    "    pagerank_all_years = spark.createDataFrame([], \"id string, pagerank double, YEAR int\")\n",
    "\n",
    "# lag window\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"YEAR\")\n",
    "\n",
    "pagerank_all_years_lag = pagerank_all_years.withColumn(\n",
    "    \"pagerank_lag\",\n",
    "    F.lag(\"pagerank\").over(window_spec)\n",
    ")\n",
    "\n",
    "pagerank_all_years_lag.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3f9be7-1c6f-4023-8e13-0ac3bd964eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "pagerank_origin = (\n",
    "    pagerank_all_years_lag\n",
    "    .withColumnRenamed(\"id\", \"ORIGIN_AIRPORT_ID\")\n",
    "    .withColumnRenamed(\"pagerank\", \"pagerank_origin\")\n",
    "    .withColumnRenamed(\"pagerank_lag\", \"pagerank_origin_lag\")\n",
    ")\n",
    "\n",
    "# Similarly for destination side\n",
    "pagerank_destination = (\n",
    "    pagerank_all_years_lag\n",
    "    .withColumnRenamed(\"id\", \"DEST_AIRPORT_ID\")\n",
    "    .withColumnRenamed(\"pagerank\", \"pagerank_destination\")\n",
    "    .withColumnRenamed(\"pagerank_lag\", \"pagerank_destination_lag\")\n",
    ")\n",
    "\n",
    "# Now join them onto your main flights DataFrame\n",
    "graph_df = (\n",
    "    df_final\n",
    "    .join(\n",
    "        pagerank_origin,\n",
    "        on=[\"ORIGIN_AIRPORT_ID\", \"YEAR\"],  # match both airport and year\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .join(\n",
    "        pagerank_destination,\n",
    "        on=[\"DEST_AIRPORT_ID\", \"YEAR\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e61334c9-acbc-4960-8ed3-6200ea8375fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### InDegree/OutDegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d7f7ad-bfa8-4351-996a-bba88506b79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List to store computed per-quarter degree DataFrames\n",
    "all_degree_quarters = []\n",
    "\n",
    "# Loop over all quarters to compute the degree values from that quarter’s flight data.\n",
    "for q in year_quarters:\n",
    "    print(f\"Computing in/out degrees for quarter {q}\")\n",
    "    \n",
    "    # Filter data for quarter q\n",
    "    df_q = df_final.filter(F.col(\"year_quarter\") == q)\n",
    "    \n",
    "    # Build vertices: get distinct airports from both origins and destinations\n",
    "    origin_airports = (\n",
    "        df_q.selectExpr(\"ORIGIN_AIRPORT_ID as airport_id\")\n",
    "        .dropDuplicates([\"airport_id\"])\n",
    "        .withColumn(\"priority\", F.lit(1))\n",
    "    )\n",
    "    dest_airports = (\n",
    "        df_q.selectExpr(\"DEST_AIRPORT_ID as airport_id\")\n",
    "        .dropDuplicates([\"airport_id\"])\n",
    "        .withColumn(\"priority\", F.lit(2))\n",
    "    )\n",
    "    union_airports = origin_airports.union(dest_airports)\n",
    "    \n",
    "    # Remove duplicates: keep one row per airport\n",
    "    quarter_window = Window.partitionBy(\"airport_id\").orderBy(\"priority\")\n",
    "    all_airports = (\n",
    "        union_airports\n",
    "        .withColumn(\"row_num\", F.row_number().over(quarter_window))\n",
    "        .filter(\"row_num = 1\")\n",
    "        .drop(\"row_num\", \"priority\")\n",
    "    )\n",
    "    \n",
    "    # Rename column to \"id\" to use as vertices in GraphFrame\n",
    "    airports_vertices = all_airports.selectExpr(\"airport_id as id\")\n",
    "    \n",
    "    # Build edges for the graph (using flights within quarter q)\n",
    "    flight_edges = df_q.selectExpr(\n",
    "        \"ORIGIN_AIRPORT_ID as src\",\n",
    "        \"DEST_AIRPORT_ID as dst\"\n",
    "    )\n",
    "    \n",
    "    # Create the GraphFrame\n",
    "    g = GraphFrame(airports_vertices, flight_edges)\n",
    "    \n",
    "    # Compute inDegrees and outDegrees\n",
    "    inDegDF = g.inDegrees \n",
    "    outDegDF = g.outDegrees\n",
    "    \n",
    "    # Join in/out degree DataFrames, filling missing values with 0\n",
    "    degreeDF = (\n",
    "        inDegDF.join(outDegDF, on=\"id\", how=\"full\")\n",
    "        .na.fill(0, [\"inDegree\", \"outDegree\"])\n",
    "    )\n",
    "    \n",
    "    # Attach quarter label q to the computed degree values\n",
    "    degree_quarter = degreeDF.withColumn(\"year_quarter\", F.lit(q))\n",
    "    all_degree_quarters.append(degree_quarter)\n",
    "\n",
    "# Union all quarter DataFrames.\n",
    "deg_all = all_degree_quarters[0]\n",
    "for df in all_degree_quarters[1:]:\n",
    "    deg_all = deg_all.union(df)\n",
    "\n",
    "windowSpec = Window.partitionBy(\"id\").orderBy(\"year_quarter\")\n",
    "\n",
    "# Create lag columns for inDegree and outDegree; for the earliest quarter (e.g. Q1)\n",
    "lagged = deg_all.withColumn(\"lag_inDegree\", F.lag(\"inDegree\", 1).over(windowSpec)) \\\n",
    "                .withColumn(\"lag_outDegree\", F.lag(\"outDegree\", 1).over(windowSpec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90f80501-6779-4cf3-b126-a661beae4228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "origin_degrees = (\n",
    "    lagged\n",
    "    .withColumnRenamed(\"id\", \"ORIGIN_AIRPORT_ID\")\n",
    "    .withColumnRenamed(\"inDegree\", \"origin_indegree\")\n",
    "    .withColumnRenamed(\"outDegree\", \"origin_outdegree\")\n",
    "    .withColumnRenamed(\"lag_inDegree\", \"origin_indegree_lag\")\n",
    "    .withColumnRenamed(\"lag_outDegree\", \"origin_outdegree_lag\")\n",
    ")\n",
    "\n",
    "dest_degrees = (\n",
    "    lagged\n",
    "    .withColumnRenamed(\"id\", \"DEST_AIRPORT_ID\")\n",
    "    .withColumnRenamed(\"inDegree\", \"dest_indegree\")\n",
    "    .withColumnRenamed(\"outDegree\", \"dest_outdegree\")\n",
    "    .withColumnRenamed(\"lag_inDegree\", \"dest_indegree_lag\")\n",
    "    .withColumnRenamed(\"lag_outDegree\", \"dest_outdegree_lag\")\n",
    ")\n",
    "\n",
    "graph_df = (\n",
    "    graph_df\n",
    "    .join(origin_degrees, on=[\"ORIGIN_AIRPORT_ID\", \"year_quarter\"], how=\"left\")\n",
    "    .join(dest_degrees, on=[\"DEST_AIRPORT_ID\", \"year_quarter\"], how=\"left\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2261de1b-c68d-41a0-a7e8-7d03128b51a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a48518d-24d9-44f8-9ff6-44efbd195d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill null values in pagerank lag columns\n",
    "graph_df = graph_df.withColumn(\n",
    "    \"pagerank_origin_lag\",\n",
    "    when(col(\"pagerank_origin_lag\").isNull(), col(\"pagerank_origin\"))\n",
    "    .otherwise(col(\"pagerank_origin_lag\"))\n",
    ")\n",
    "\n",
    "graph_df = graph_df.withColumn(\n",
    "    \"pagerank_destination_lag\",\n",
    "    when(col(\"pagerank_destination_lag\").isNull(), col(\"pagerank_destination\"))\n",
    "    .otherwise(col(\"pagerank_destination_lag\"))\n",
    ")\n",
    "\n",
    "# Fill null values in degree lag columns (in_degree and out_degree)\n",
    "graph_df = graph_df.withColumn(\n",
    "    \"origin_indegree_lag\",\n",
    "    when(col(\"origin_indegree_lag\").isNull(), col(\"origin_indegree\"))\n",
    "    .otherwise(col(\"origin_indegree_lag\"))\n",
    ")\n",
    "\n",
    "graph_df = graph_df.withColumn(\n",
    "    \"origin_outdegree_lag\",\n",
    "    when(col(\"origin_outdegree_lag\").isNull(), col(\"origin_outdegree\"))\n",
    "    .otherwise(col(\"origin_outdegree_lag\"))\n",
    ")\n",
    "\n",
    "graph_df = graph_df.withColumn(\n",
    "    \"dest_indegree_lag\",\n",
    "    when(col(\"dest_indegree_lag\").isNull(), col(\"dest_indegree\"))\n",
    "    .otherwise(col(\"dest_indegree_lag\"))\n",
    ")\n",
    "\n",
    "graph_df = graph_df.withColumn(\n",
    "    \"dest_outdegree_lag\",\n",
    "    when(col(\"dest_outdegree_lag\").isNull(), col(\"dest_outdegree\"))\n",
    "    .otherwise(col(\"dest_outdegree_lag\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35ff5e6-7672-4c86-af1a-02e728627851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# columns_to_check = [\n",
    "#     \"pagerank_origin_lag\",\n",
    "#     \"pagerank_destination_lag\",\n",
    "#     \"origin_indegree_lag\",\n",
    "#     \"origin_outdegree_lag\",\n",
    "#     \"dest_indegree_lag\",\n",
    "#     \"dest_outdegree_lag\"\n",
    "# ]\n",
    "\n",
    "# null_counts = graph_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in columns_to_check])\n",
    "# display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf3f81a-c250-49cc-8155-026b649ed5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9058fc-4ec1-4731-b3e8-11b4a1d7873b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save df_southwest_cleaned as a parquet file\n",
    "df_final.write.parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_final_custom_join_2015_2024.parquet\") # use to save a new version\n",
    "# df_final.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_complete_parquet_final_custom_join.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73e1c917-1cc8-46ba-922c-793ce1adee5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis on the New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b699f2-a311-4e20-86c8-261c02a03ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_60M_complete_parquet_final_custom_join_2015_2024.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fbeaf0e-04f7-4032-b0b9-4f44c0d5250b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas for plotting\n",
    "df_pd = df_final.select(\n",
    "    \"daily_operations\", \"sw_rel_perf\", \"origin_type\", \"prior_day_delay_rate\", \n",
    "    \"same_day_prior_delay_percentage\", \"time_of_day_category\", \"DEP_DEL15\", \n",
    "    \"time_based_congestion_ratio\", \"pagerank_origin\", \"origin_outdegree\",\n",
    "    \"sw_origin_time_perf\", \"sw_market_share\"\n",
    ").sample(fraction=0.1, seed=42).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3818a944-825d-4d9b-93a3-565872b2280c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up Matplotlib aesthetics\n",
    "sns.set(style='whitegrid', palette='pastel')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# SW Relative Performance (log-scale, remove outliers)\n",
    "filtered_df = df_pd[df_pd['sw_rel_perf'] < df_pd['sw_rel_perf'].quantile(0.99)]\n",
    "sns.scatterplot(data=filtered_df, x='daily_operations', y='sw_rel_perf', hue='origin_type', ax=axes[0])\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_title('SW Relative Performance (Log scale) vs. Daily Operations (2015-2024)')\n",
    "\n",
    "# SW Market Share (violin plot)\n",
    "sns.violinplot(x=df_pd['origin_type'], y=df_pd['sw_market_share'], ax=axes[1])\n",
    "axes[1].set_title('SW Market Share Distribution by Airport Type (2015-2024)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('airport_profile.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896774c6-6e03-4064-b743-9374f2b29102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Time-based Features EDA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.regplot(\n",
    "    data=df_pd, \n",
    "    x='prior_day_delay_rate', \n",
    "    y='same_day_prior_delay_percentage',\n",
    "    scatter_kws={'alpha':0.3},\n",
    "    line_kws={'color':'red'},\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Prior Day Delay Rate vs. Same Day Prior Delay Percentage (2015-2024)')\n",
    "axes[0].set_xlabel('Prior Day Delay Rate')\n",
    "axes[0].set_ylabel('Same Day Prior Delay %')\n",
    "\n",
    "# Delay Rate by Time of Day Category\n",
    "sns.barplot(data=df_pd, x='time_of_day_category', y='DEP_DEL15', ax=axes[1])\n",
    "axes[1].set_title('Delay Rate by Time of Day Category (2015-2024)')\n",
    "axes[1].set_xlabel('Time of Day')\n",
    "axes[1].set_ylabel('Average Delay Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_based_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b7256b7-13c7-4100-9daa-0290a906dd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.violinplot(x='origin_type', y='sw_origin_time_perf', data=df_pd)\n",
    "plt.title(\"Southwest Airlines Performance by Airport Type (2015-2024)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sw_specific_features.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf6cf17-96e3-4bee-93bb-cda48789a838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bin PageRank and OutDegree\n",
    "df_pd['pagerank_bin'] = pd.qcut(df_pd['pagerank_origin'], q=10, duplicates='drop')\n",
    "df_pd['outdegree_bin'] = pd.qcut(df_pd['origin_outdegree'], q=10, duplicates='drop')\n",
    "\n",
    "# Group by bins\n",
    "pagerank_grp = df_pd.groupby('pagerank_bin')['DEP_DEL15'].mean().reset_index()\n",
    "outdegree_grp = df_pd.groupby('outdegree_bin')['DEP_DEL15'].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.barplot(x='pagerank_bin', y='DEP_DEL15', data=pagerank_grp, ax=axes[0])\n",
    "axes[0].set_title('Avg Delay Rate by PageRank Bin (2015-2024)')\n",
    "axes[0].set_xlabel('PageRank (Binned)')\n",
    "axes[0].set_ylabel('Avg Delay Rate')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.barplot(x='outdegree_bin', y='DEP_DEL15', data=outdegree_grp, ax=axes[1])\n",
    "axes[1].set_title('Avg Delay Rate by OutDegree Bin (2015-2024)')\n",
    "axes[1].set_xlabel('OutDegree (Binned)')\n",
    "axes[1].set_ylabel('Avg Delay Rate')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graph_based_features.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6081aae-0891-4f27-946a-61903464b7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spearman correlation for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a0d90e-8228-4fae-8969-5b1172e20c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define categorical features\n",
    "cat_features = [\n",
    "    'ORIGIN_AIRPORT_ID', 'ORIGIN_STATE_ABR', 'DEST_AIRPORT_ID', 'DEST_STATE_ABR',\n",
    "    'origin_type', 'dest_type', 'TRIPLET', 'route', 'time_bucket',\n",
    "    'time_of_day_category', 'year_quarter', 'quarter_month'\n",
    "]\n",
    "\n",
    "# 2. Create pipeline stages for encoding\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\", handleInvalid=\"keep\") \n",
    "           for col in cat_features]\n",
    "    \n",
    "# 3. Assemble vectors\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_idx\" for col in cat_features],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# 4. Build and run pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "indexed_data = pipeline.fit(df_final).transform(df_final)\n",
    "\n",
    "# 5. Compute Spearman correlation matrix\n",
    "correlation_matrix = Correlation.corr(indexed_data, \"features\", method=\"spearman\").head()[0]\n",
    "\n",
    "# 6. Convert to pandas/numpy for visualization\n",
    "matrix_array = correlation_matrix.toArray()\n",
    "correlation_df = pd.DataFrame(matrix_array, index=cat_features, columns=cat_features)\n",
    "\n",
    "# 7. Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_df, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Spearman Correlation Matrix for Categorical Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11dc9285-7d38-4b3f-9e92-8557df8ee968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Final features for modelling - picked based on EDA + Pearson Correlation + Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06260f99-1b26-4b32-b373-a62631ebbc7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selected_features_for_training = ['DAY_OF_MONTH',\n",
    " 'DAY_OF_WEEK',\n",
    " 'DEP_DEL15',\n",
    " 'origin_type',\n",
    " 'dest_type',\n",
    " 'PREV_CANCELLED',\n",
    " 'MINUTES_BETWEEN_FLIGHTS',\n",
    " 'PREV_ARR_DEL15',\n",
    " 'TRIPLET',\n",
    " 'daily_operations',\n",
    " 'rolling_30day_volume',\n",
    " 'origin_1yr_delay_rate',\n",
    " 'route_1yr_volume',\n",
    " 'sw_market_share',\n",
    " 'sw_30d_delay',\n",
    " 'sw_route_delay',\n",
    " 'sw_rel_perf',\n",
    " 'route',\n",
    " 'time_bucket',\n",
    " 'time_of_day_category',\n",
    " 'is_weekend',\n",
    " 'holiday_season',\n",
    " 'prior_day_delay_rate',\n",
    " 'prior_flights_today',\n",
    " 'prior_delays_today',\n",
    " 'same_day_prior_delay_percentage',\n",
    " 'time_based_congestion_ratio',\n",
    " 'sw_time_of_day_delay_rate',\n",
    " 'sw_day_of_week_delay_rate',\n",
    " 'sw_aircraft_delay_rate',\n",
    " 'sw_origin_hub',\n",
    " 'sw_schedule_buffer_ratio',\n",
    " 'sw_origin_time_perf',\n",
    " 'sw_route_importance',\n",
    " 'year_quarter',\n",
    " 'quarter_month',\n",
    " 'YEAR',\n",
    " 'MONTH',\n",
    " 'pagerank_origin_lag',\n",
    " 'pagerank_destination_lag',\n",
    " 'origin_indegree_lag',\n",
    " 'origin_outdegree_lag',\n",
    " 'dest_indegree_lag',\n",
    " 'dest_outdegree_lag'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cf4bbe-648b-4fc0-9aa4-f1ffbbe417be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the schema to verify the columns\n",
    "print(f\"Number of columns after filtering: {len(selected_features_for_training)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b613c4-bf56-4847-901a-0a893813a266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset FIRST to avoid leakage\n",
    "test_data = df_final.filter(F.col(\"YEAR\") == 2024)\n",
    "validation_data = df_final.filter((F.col(\"YEAR\") >= 2022) & (F.col(\"YEAR\") <= 2023))\n",
    "train_data = df_final.filter((F.col(\"YEAR\") >= 2015) & (F.col(\"YEAR\") <= 2021))\n",
    "\n",
    "# Get number of cores\n",
    "num_cores = spark.sparkContext.defaultParallelism\n",
    "print(f\"Number of current cores: {num_cores}\")\n",
    "\n",
    "# Repartition each set individually\n",
    "train_data = train_data.repartition(num_cores * 2)#.persist()\n",
    "validation_data = validation_data.repartition(num_cores)#.persist()\n",
    "test_data = test_data.repartition(num_cores)#.persist()\n",
    "\n",
    "# Force evaluation to materialize partitions and cache\n",
    "# print(f\"Train data count: {train_data.count()}\")\n",
    "# print(f\"Validation data count: {validation_data.count()}\")\n",
    "# print(f\"Test data count: {test_data.count()}\")\n",
    "\n",
    "# Split data using year_quarter column\n",
    "# Train: 2015, Month = 1\n",
    "# train_data = df_final.filter((col(\"YEAR\") == \"2015\") & (col(\"MONTH\") == 1))\n",
    "\n",
    "# # Validation: 2015, Month = 2\n",
    "# validation_data = df_final.filter((col(\"YEAR\") == \"2015\") & (col(\"MONTH\") == 2))\n",
    "\n",
    "# # Test: 2015, Month = 3\n",
    "# test_data = df_final.filter((col(\"YEAR\") == \"2015\") & (col(\"MONTH\") == 3))\n",
    "\n",
    "# # Cache the datasets to improve performance\n",
    "# train_data = train_data.cache()\n",
    "# validation_data = validation_data.cache()\n",
    "# test_data = test_data.cache()\n",
    "\n",
    "# # Count records in each dataset\n",
    "# print(f\"Train data count: {train_data.count()}\")\n",
    "# print(f\"Validation data count: {validation_data.count()}\")\n",
    "# print(f\"Test data count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdbb2b4-cd2a-4709-8b9c-c0b94065b382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Train data count: {train_data.count()}\")\n",
    "print(f\"Validation data count: {validation_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f7affa-bf8c-4c89-bce0-869ff8e637fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store counts in variables\n",
    "train_count = 9865375 \n",
    "validation_count = 2735873\n",
    "test_count = 1418940\n",
    "\n",
    "# Use these variables instead of calling count() again\n",
    "print(f\"Train data count: {train_count}\")\n",
    "print(f\"Validation data count: {validation_count}\")\n",
    "print(f\"Test data count: {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77230977-e1d3-4f6d-a011-77830d9a69ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save train/validate/test split as a parquet files\n",
    "# train_data.write.parquet(f\"{folder_path}/df_otpw_60M_train_data.parquet\") # use to save a new version\n",
    "# validation_data.write.parquet(f\"{folder_path}/df_otpw_60M_validation_data.parquet\") # use to save a new version\n",
    "# test_data.write.parquet(f\"{folder_path}/df_otpw_60M_test_data.parquet\") # use to save a new version\n",
    "\n",
    "\n",
    "train_data.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_2015_2024_train_data.parquet\") # use if you want to overwrite exisiting file\n",
    "validation_data.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_2015_2024_validation_data.parquet\") # use if you want to overwrite exisiting file\n",
    "test_data.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_2015_2024_test_data.parquet\") # use if you want to overwrite exisiting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24fb853f-639d-4dfa-921a-ecfb85d3b321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_2015_2024_train_data.parquet/\")\n",
    "validation_data = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_2015_2024_validation_data.parquet/\")\n",
    "test_data = spark.read.parquet(f\"dbfs:/student-groups/Group_04_04/df_otpw_2015_2024_test_data.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e989f282-1c62-47ef-96f7-0d313c909984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cache and trigger caching using .first()\n",
    "train_data.cache().first()\n",
    "validation_data.cache().first()\n",
    "test_data.cache().first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8ad544-3c17-42d9-941b-9629a1e59dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize delayed vs on-time flights for each dataset\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Function to plot delayed vs on-time flights\n",
    "def plot_delay_distribution(data, position, title, total_count=None):\n",
    "    delay_counts = data.groupBy(\"DEP_DEL15\").count().toPandas()\n",
    "    delay_counts = delay_counts.sort_values(\"DEP_DEL15\")\n",
    "    \n",
    "    # Use provided total count if available\n",
    "    if total_count is None:\n",
    "        total = delay_counts[\"count\"].sum()\n",
    "    else:\n",
    "        total = total_count\n",
    "    \n",
    "    delay_counts[\"percentage\"] = delay_counts[\"count\"] / total * 100\n",
    "    \n",
    "    # Map 0/1 to meaningful labels\n",
    "    delay_counts[\"status\"] = delay_counts[\"DEP_DEL15\"].map({0: \"On-time\", 1: \"Delayed\"})\n",
    "    \n",
    "    # Create subplot\n",
    "    plt.subplot(1, 3, position)\n",
    "    bars = plt.bar(delay_counts[\"status\"], delay_counts[\"count\"], color=[\"green\", \"red\"])\n",
    "    \n",
    "    # Add count and percentage labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
    "                f'{height:,.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Add percentage as a second line of text\n",
    "    for i, p in enumerate(delay_counts[\"percentage\"]):\n",
    "        plt.text(bars[i].get_x() + bars[i].get_width()/2., bars[i].get_height()/2,\n",
    "                f'{p:.1f}%', ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Number of Flights\")\n",
    "    plt.ylim(0, delay_counts[\"count\"].max() * 1.1)\n",
    "\n",
    "# Plot for each dataset\n",
    "plot_delay_distribution(train_data, 1, \"Train Data\", train_count)\n",
    "plot_delay_distribution(validation_data, 2, \"Validation Data\", validation_count)\n",
    "plot_delay_distribution(test_data, 3, \"Test Data\", test_count)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e01041-91ce-4c70-acdb-6902a339349b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use known delay rate (25%) instead of counting\n",
    "known_delay_rate = 0.25  # 25% of flights are delayed\n",
    "total_count = train_count  # Use your stored variable\n",
    "\n",
    "# Calculate counts based on rate\n",
    "delayed_count = int(total_count * known_delay_rate)\n",
    "on_time_count = total_count - delayed_count\n",
    "\n",
    "# Calculate weights\n",
    "delayed_weight = total_count / (2 * delayed_count)\n",
    "on_time_weight = total_count / (2 * on_time_count)\n",
    "\n",
    "# Add weight column to training data\n",
    "train_data = train_data.withColumn(\n",
    "    \"sample_weight\",\n",
    "    when(col(\"DEP_DEL15\") == 1, delayed_weight).otherwise(on_time_weight)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c8f48b-23f9-4cb3-98e2-2c1fd362d3cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLLib pipeline starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b25b80-4fa2-4855-8589-a24a49dacba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Please note that for conveniece, we are evaluating test datasets for each model such that if one model is chosen, we don't need to run the costly model training and evaluation to see its test model, better efficiency and coordination. However, similar as what we did in cross validation, when evaluating the model, we only consider validation evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcad69d2-4c6b-4b57-878a-9b823a10a299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define categorical features with few unique values (for one-hot encoding)\n",
    "categorical_features = [\n",
    "    # Airport identifiers\n",
    "    'origin_type', 'dest_type',\n",
    "    \n",
    "    # Time-related categorical features\n",
    "    'time_of_day_category', 'year_quarter', 'quarter_month',\n",
    "    \n",
    "    # Binary indicators\n",
    "    'is_weekend', 'holiday_season', 'sw_origin_hub'\n",
    "]\n",
    "\n",
    "# Define high-cardinality categorical features (for numeric encoding only)\n",
    "high_cardinality_features = [\n",
    "    'route',         \n",
    "    'time_bucket',  \n",
    "    'TRIPLET'         \n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    # Time-related numeric features\n",
    "    'DAY_OF_MONTH', 'DAY_OF_WEEK', \n",
    "    \n",
    "    # Flight history features\n",
    "    'PREV_CANCELLED', 'MINUTES_BETWEEN_FLIGHTS', 'PREV_ARR_DEL15',\n",
    "    \n",
    "    # Airport and route metrics\n",
    "    'daily_operations', 'rolling_30day_volume', 'origin_1yr_delay_rate', 'route_1yr_volume',\n",
    "    \n",
    "    # Southwest Airlines specific metrics\n",
    "    'sw_market_share', 'sw_30d_delay', 'sw_route_delay', 'sw_rel_perf',\n",
    "    'sw_time_of_day_delay_rate', 'sw_day_of_week_delay_rate', 'sw_aircraft_delay_rate',\n",
    "    'sw_schedule_buffer_ratio', 'sw_origin_time_perf', 'sw_route_importance',\n",
    "    \n",
    "    # Delay metrics\n",
    "    'prior_day_delay_rate', 'prior_flights_today', 'prior_delays_today', \n",
    "    'same_day_prior_delay_percentage', 'time_based_congestion_ratio',\n",
    "\n",
    "    # Graph based - PageRank\n",
    "        'pagerank_origin_lag',\n",
    "        'pagerank_destination_lag',\n",
    "        'origin_indegree_lag',\n",
    "        'origin_outdegree_lag',\n",
    "        'dest_indegree_lag',\n",
    "        'dest_outdegree_lag'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9752239f-2473-4254-ba83-d629d031fb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Add Repartitioning for Large Data\n",
    "# num_partitions = 200  # Adjust based on cluster size (cores * 2-3)\n",
    "# train_data = train_data.repartition(num_partitions)\n",
    "# validation_data = validation_data.repartition(num_partitions//4)\n",
    "# test_data = test_data.repartition(num_partitions//4)\n",
    "\n",
    "# 2. Modified Pipeline with Debugging Checks\n",
    "stages = []\n",
    "\n",
    "# Process categorical features\n",
    "for feature in categorical_features:\n",
    "    indexer = StringIndexer(inputCol=feature, outputCol=f\"{feature}_indexed\", handleInvalid=\"keep\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"{feature}_indexed\", outputCol=f\"{feature}_encoded\")\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# Process high-cardinality features\n",
    "for feature in high_cardinality_features:\n",
    "    indexer = StringIndexer(inputCol=feature, outputCol=f\"{feature}_numeric\", handleInvalid=\"keep\")\n",
    "    stages.append(indexer)\n",
    "\n",
    "# Add null check before assembling\n",
    "stages.append(Imputer(inputCols=numerical_features, \n",
    "                    outputCols=numerical_features,\n",
    "                    strategy=\"median\"))\n",
    "\n",
    "# Assemble features with debug\n",
    "transformed_features = [f\"{feature}_encoded\" for feature in categorical_features] + \\\n",
    "                      [f\"{feature}_numeric\" for feature in high_cardinality_features] + \\\n",
    "                      numerical_features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=transformed_features, \n",
    "                          outputCol=\"features_unscaled\", \n",
    "                          handleInvalid=\"keep\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# Add StandardScaler with mean centering disabled\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", \n",
    "                      outputCol=\"features\",\n",
    "                      withStd=True, \n",
    "                      withMean=False)  # Critical for large datasets\n",
    "stages.append(scaler)\n",
    "\n",
    "# 3. Create Pipeline with Progress Checks\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# 4. Fit Pipeline with Intermediate Validation\n",
    "try:\n",
    "    # Fit on sample data first\n",
    "    sample_data = train_data.limit(1000)\n",
    "    sample_model = pipeline.fit(sample_data)\n",
    "    sample_transformed = sample_model.transform(sample_data)\n",
    "    \n",
    "    print(\"\\n=== Sample Data Schema ===\")\n",
    "    sample_transformed.printSchema()\n",
    "    \n",
    "    print(\"\\n=== Sample Features ===\")\n",
    "    # sample_transformed.select(\"features\").show(5, truncate=False)\n",
    "    \n",
    "    # Now fit on full data\n",
    "    pipeline_model = pipeline.fit(train_data)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Pipeline failed during fitting: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 5. Transform Data with Validation\n",
    "def safe_transform(model, data, name):\n",
    "    try:\n",
    "        transformed = model.transform(data).cache()\n",
    "        # Force execution and check features\n",
    "        count = transformed.count()\n",
    "        print(f\"\\n✅ Successfully transformed {name} dataset ({count} rows)\")\n",
    "        \n",
    "        print(f\"\\n=== {name} Features Sample ===\")\n",
    "        # transformed.select(\"features\").show(5, truncate=False)\n",
    "        \n",
    "        return transformed\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Transformation failed for {name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "train_data_transformed = safe_transform(pipeline_model, train_data, \"Training\")\n",
    "validation_data_transformed = safe_transform(pipeline_model, validation_data, \"Validation\")\n",
    "test_data_transformed = safe_transform(pipeline_model, test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da257374-549e-4f3d-872e-f622550fbc20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Credit to 261 classmate Eric Mowat\n",
    "from pyspark.ml.feature import OneHotEncoderModel\n",
    "def get_one_hot_encoded_lengths(pipeline_model):\n",
    "    \"\"\"\n",
    "    Get lengths of one-hot encoded columns from OneHotEncoderModel stages in the pipeline.\n",
    "\n",
    "    Args:\n",
    "        pipeline_model (PipelineModel): Trained pipeline model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping one-hot encoded column names to their lengths.\n",
    "    \"\"\"\n",
    "    one_hot_lengths = {}\n",
    "    for stage in pipeline_model.stages:\n",
    "        if isinstance(stage, OneHotEncoderModel):\n",
    "            input_col = stage.getInputCol()\n",
    "            output_col = stage.getOutputCol()\n",
    "            num_categories = stage.categorySizes[0]  # Number of categories for this column\n",
    "            # If dropLast=True, subtract 1 from the number of categories\n",
    "            adjusted_length = num_categories - (1 if stage.getDropLast() else 0)\n",
    "            one_hot_lengths[output_col] = adjusted_length\n",
    "           # print(f\"OneHotEncoder detected. Input column: {input_col}, Output column: {output_col}, Categories: {adjusted_length}\")\n",
    "    return one_hot_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3106bc-8a2d-4909-809a-47d3702406b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print One Hot Encoder feature sizes\n",
    "print(\"One Hot Encoded Feature sizes\")\n",
    "print(get_one_hot_encoded_lengths(pipeline_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faeab97-438b-46e4-98e7-cc07171116bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train_data_transformed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a25c0a-0a92-4722-a403-153ca71f298c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c315b94-8a61-482c-bd18-f4c83899d42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "train_data_ml = train_data_transformed.select(\"DEP_DEL15\", \"features_unscaled\", \"sample_weight\")\n",
    "validation_data_ml = validation_data_transformed.select(\"DEP_DEL15\",  \"features_unscaled\")\n",
    "test_data_ml = test_data_transformed.select(\"DEP_DEL15\", \"features_unscaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75030922-61e6-44e6-8705-ac8e576cc22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_BASE_DIR = \"dbfs:/student-groups/Group_04_04/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4a432e-00ea-46d1-9ce6-5fbcccf07e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed encoded features\n",
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save train/validate/test split as a parquet files\n",
    "# train_data_ml.write.parquet(f\"{folder_path}/df_otpw_60M_train_data_encoded.parquet\") # use to save a new version\n",
    "# validation_data_ml.write.parquet(f\"{folder_path}/df_otpw_60M_validation_data_encoded.parquet\") # use to save a new version\n",
    "# test_data_ml.write.parquet(f\"{folder_path}/df_otpw_60M_test_data_encoded.parquet\") # use to save a new version\n",
    "\n",
    "\n",
    "train_data.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_train_data_encoded.parquet\") # use if you want to overwrite exisiting file\n",
    "validation_data.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_validation_data_encoded.parquet\") # use if you want to overwrite exisiting file\n",
    "test_data.write.mode('overwrite').parquet(f\"{folder_path}/df_otpw_60M_test_data.parquet_encoded\") # use if you want to overwrite exisiting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0d5b5c-e2fa-4025-86b3-8deafb694923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluators for precision, recall, and F1 score\n",
    "\n",
    "def evaluate_predictions(predictions, target_feature, dataset_name, f2_only=False):\n",
    "    # Calculate confusion matrix components\n",
    "    tp = predictions.filter((col(target_feature) == 1) & (col('prediction') == 1)).count()\n",
    "    tn = predictions.filter((col(target_feature) == 0) & (col('prediction') == 0)).count()\n",
    "    fp = predictions.filter((col(target_feature) == 0) & (col('prediction') == 1)).count()\n",
    "    fn = predictions.filter((col(target_feature) == 1) & (col('prediction') == 0)).count()\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    f2 = 5 * (precision * recall) / (4*precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "    if (f2_only):\n",
    "        return f2\n",
    "\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f2': f2,\n",
    "        'confusion_matrix': [[tn, fp], [fn, tp]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed0ae1de-c44f-4c31-b2bb-dc2176b07612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def show_predictions_eval(prediction_datasets, target_feature):\n",
    "    results = []\n",
    "    for k,v in prediction_datasets.items():\n",
    "        results.append(evaluate_predictions(v, target_feature, k))\n",
    "    # Create metrics comparison table\n",
    "    metrics_df = pd.DataFrame(results).set_index('dataset')\n",
    "    print(\"\\nMetrics Comparison:\")\n",
    "    print(metrics_df[['precision', 'recall', 'f2']].to_markdown(floatfmt=\".3f\"))\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    for idx, result in enumerate(results):\n",
    "        sns.heatmap(\n",
    "            result['confusion_matrix'], \n",
    "            annot=True, fmt=\"d\", \n",
    "            cmap=\"Blues\",\n",
    "            ax=axes[idx],\n",
    "            cbar=False\n",
    "        )\n",
    "        axes[idx].set_title(f\"{result['dataset']} Set\\nConfusion Matrix\")\n",
    "        axes[idx].set_xlabel(\"Predicted\")\n",
    "        axes[idx].set_ylabel(\"Actual\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32b4fe33-4344-485c-b141-562ac618ee88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Baseline model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "22fbe59a-40bf-430b-9b1a-d52d2a69c486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model for classification\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"DEP_DEL15\",\n",
    "    weightCol=\"sample_weight\",\n",
    "    maxIter=10,\n",
    "    regParam=0\n",
    ")\n",
    "lr_model = lr.fit(train_data_ml)\n",
    "\n",
    "# Get predictions for all datasets\n",
    "train_pred = lr_model.transform(train_data_ml)\n",
    "val_pred = lr_model.transform(validation_data_ml)\n",
    "test_pred = lr_model.transform(test_data_ml)\n",
    "model_datasets = {'Training': train_pred, 'Validation': val_pred, 'Test': test_pred}\n",
    "show_predictions_eval(model_datasets, \"DEP_DEL15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ec8830-9def-4ccc-85cc-3114b4f1bcdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the coefficients (weights)\n",
    "coefficients = lr_model.coefficients\n",
    "\n",
    "for coeff in coefficients:\n",
    "    print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d632223d-9980-4495-90eb-4d2897bbde2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6790eb3-d8b6-4d6a-8ebb-305fb811c705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### HyperTuning and Cross Validation For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78ebdb03-9959-400a-9aef-6e2bdef903ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data_cv_total = train_data_transformed.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy([\"YEAR\", \"MONTH\", \"DAY_OF_MONTH\", \"CRS_DEP_TIME\"])))\n",
    "train_data_cv_total.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe4eb6b-ba08-48f1-8e73-31bdcb086dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# % of train_data_cv_total used as training\n",
    "training_fold_percent_threshold = [0.2, 0.4, 0.6, 0.8]\n",
    "regParams = [0.00001, 0.0002, 0.001, 0.01]\n",
    "precision_cv = np.zeros(len(regParams))\n",
    "recall_cv = np.zeros(len(regParams))\n",
    "f1_cv = np.zeros(len(regParams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d8e2a644-ddce-4f81-af62-ac66ef1408f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Didn't loop over tfpt for modularity in case of failure\n",
    "tfpt=training_fold_percent_threshold[0]\n",
    "train_data_cv = train_data_cv_total.filter(train_data_cv_total.rank <= tfpt)\n",
    "val_data_cv = train_data_cv_total.filter(train_data_cv_total.rank > tfpt)\n",
    "train_data_cv = train_data_cv.repartition(200) #Had a oom issue and was recommended to use repartition, but issue still remains. I ultimately restarted the cluster runtime and it works. So perhaps this is not needed\n",
    "val_data_cv = val_data_cv.repartition(200)\n",
    "for i, regParam in enumerate(regParams):\n",
    "  lr_lasso = LogisticRegression(featuresCol=\"features\", labelCol=\"DEP_DEL15\",  weightCol=\"sample_weight\", maxIter=10, regParam=regParam, elasticNetParam=1.0)\n",
    "  lr_model_lasso = lr_lasso.fit(train_data_cv)\n",
    "  lr_predictions = lr_model_lasso.transform(val_data_cv)\n",
    "\n",
    "  eval_metrics = evaluate_predictions(lr_predictions, \"DEP_DEL15\", \"Cross Validation\")\n",
    "  lr_precision_cv = eval_metrics[\"precision\"]\n",
    "  lr_recall_cv = eval_metrics[\"recall\"]\n",
    "  lr_f1_score_cv = eval_metrics[\"f1\"]\n",
    "  print(lr_precision_cv)\n",
    "  print(lr_recall_cv)\n",
    "  print(lr_f1_score_cv)\n",
    "  precision_cv[i] += lr_precision_cv\n",
    "  recall_cv[i] += lr_recall_cv\n",
    "  f1_cv[i] += lr_f1_score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "75fc7a8c-7a62-4ba1-a746-fe234b36827e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Didn't loop over tfpt for modularity in case of failure\n",
    "tfpt=training_fold_percent_threshold[1]\n",
    "train_data_cv = train_data_cv_total.filter(train_data_cv_total.rank <= tfpt)\n",
    "val_data_cv = train_data_cv_total.filter(train_data_cv_total.rank > tfpt)\n",
    "train_data_cv = train_data_cv.repartition(200)\n",
    "val_data_cv = val_data_cv.repartition(200)\n",
    "for i, regParam in enumerate(regParams):\n",
    "  lr_lasso = LogisticRegression(featuresCol=\"features\", labelCol=\"DEP_DEL15\",  weightCol=\"sample_weight\", maxIter=10, regParam=regParam, elasticNetParam=1.0)\n",
    "  lr_model_lasso = lr_lasso.fit(train_data_cv)\n",
    "  lr_predictions = lr_model_lasso.transform(val_data_cv)\n",
    "\n",
    "  eval_metrics = evaluate_predictions(lr_predictions, \"DEP_DEL15\", \"Cross Validation\")\n",
    "  lr_precision_cv = eval_metrics[\"precision\"]\n",
    "  lr_recall_cv = eval_metrics[\"recall\"]\n",
    "  lr_f1_score_cv = eval_metrics[\"f1\"]\n",
    "  print(lr_precision_cv)\n",
    "  print(lr_recall_cv)\n",
    "  print(lr_f1_score_cv)\n",
    "  precision_cv[i] += lr_precision_cv\n",
    "  recall_cv[i] += lr_recall_cv\n",
    "  f1_cv[i] += lr_f1_score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb20e408-cb4d-461d-ba08-110dc11a2c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Didn't loop over tfpt for modularity in case of failure\n",
    "tfpt=training_fold_percent_threshold[2]\n",
    "train_data_cv = train_data_cv_total.filter(train_data_cv_total.rank <= tfpt)\n",
    "val_data_cv = train_data_cv_total.filter(train_data_cv_total.rank > tfpt)\n",
    "train_data_cv = train_data_cv.repartition(200)\n",
    "val_data_cv = val_data_cv.repartition(200)\n",
    "for i, regParam in enumerate(regParams):\n",
    "  lr_lasso = LogisticRegression(featuresCol=\"features\", labelCol=\"DEP_DEL15\",  weightCol=\"sample_weight\", maxIter=10, regParam=regParam, elasticNetParam=1.0)\n",
    "  lr_model_lasso = lr_lasso.fit(train_data_cv)\n",
    "  lr_predictions = lr_model_lasso.transform(val_data_cv)\n",
    "\n",
    "  eval_metrics = evaluate_predictions(lr_predictions, \"DEP_DEL15\", \"Cross Validation\")\n",
    "  lr_precision_cv = eval_metrics[\"precision\"]\n",
    "  lr_recall_cv = eval_metrics[\"recall\"]\n",
    "  lr_f1_score_cv = eval_metrics[\"f1\"]\n",
    "  print(lr_precision_cv)\n",
    "  print(lr_recall_cv)\n",
    "  print(lr_f1_score_cv)\n",
    "  precision_cv[i] += lr_precision_cv\n",
    "  recall_cv[i] += lr_recall_cv\n",
    "  f1_cv[i] += lr_f1_score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "078411e3-df14-4b6d-add8-dc1b96e08290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Didn't loop over tfpt for modularity in case of failure\n",
    "tfpt=training_fold_percent_threshold[3]\n",
    "train_data_cv = train_data_cv_total.filter(train_data_cv_total.rank <= tfpt)\n",
    "val_data_cv = train_data_cv_total.filter(train_data_cv_total.rank > tfpt)\n",
    "train_data_cv = train_data_cv.repartition(200)\n",
    "val_data_cv = val_data_cv.repartition(200)\n",
    "for i, regParam in enumerate(regParams):\n",
    "  lr_lasso = LogisticRegression(featuresCol=\"features\", labelCol=\"DEP_DEL15\",  weightCol=\"sample_weight\", maxIter=10, regParam=regParam, elasticNetParam=1.0)\n",
    "  lr_model_lasso = lr_lasso.fit(train_data_cv)\n",
    "  lr_predictions = lr_model_lasso.transform(val_data_cv)\n",
    "\n",
    "  eval_metrics = evaluate_predictions(lr_predictions, \"DEP_DEL15\", \"Cross Validation\")\n",
    "  lr_precision_cv = eval_metrics[\"precision\"]\n",
    "  lr_recall_cv = eval_metrics[\"recall\"]\n",
    "  lr_f1_score_cv = eval_metrics[\"f1\"]\n",
    "  print(lr_precision_cv)\n",
    "  print(lr_recall_cv)\n",
    "  print(lr_f1_score_cv)\n",
    "  precision_cv[i] += lr_precision_cv\n",
    "  recall_cv[i] += lr_recall_cv\n",
    "  f1_cv[i] += lr_f1_score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521bb218-0922-4a53-93ad-7b253fde46fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "precision_cv = precision_cv / len(training_fold_percent_threshold)\n",
    "recall_cv = recall_cv / len(training_fold_percent_threshold)\n",
    "f1_cv = f1_cv / len(training_fold_percent_threshold)\n",
    "print(precision_cv)\n",
    "print(\"Best Precision under Cross Validation is \" + str(np.max(precision_cv)) + \"at regParam = \" + str(regParams[np.argmax(precision_cv)]))\n",
    "print(\"Best Recall under Cross Validation is \" + str(np.max(recall_cv)) + \"at regParam = \" + str(regParams[np.argmax(recall_cv)]))\n",
    "print(\"Best F1 under Cross Validation is \" + str(np.max(f1_cv)) + \"at regParam = \" + str(regParams[np.argmax(f1_cv)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d82e269-8261-440f-9856-80284c099c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(recall_cv)\n",
    "print(f1_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8cb59a3-6d42-4b0e-a2a5-c342f01b77a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee9bb97-0ff2-47f4-ad1a-b8dedd040720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a xgboost pyspark regressor estimator and set device=\"cuda\"\n",
    "xgboost_classifier = SparkXGBClassifier (\n",
    "  features_col=\"features\",\n",
    "  label_col=\"DEP_DEL15\",\n",
    "  #device=\"cuda\",\n",
    ")\n",
    "\n",
    "# train and return the model\n",
    "xgboost_model = xgboost_classifier.fit(train_data_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d878b1e4-2d19-4fd5-894e-b87bb9e3a241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgboost_train_predict_df = xgboost_model.transform(train_data_ml)\n",
    "print(evaluate_predictions(xgboost_train_predict_df, \"DEP_DEL15\", \"train\"))\n",
    "xgboost_val_predict_df = xgboost_model.transform(validation_data_ml)\n",
    "print(evaluate_predictions(xgboost_val_predict_df, \"DEP_DEL15\", \"validation\"))\n",
    "xgboost_test_predict_df = xgboost_model.transform(test_data_ml)\n",
    "print(evaluate_predictions(xgboost_test_predict_df, \"DEP_DEL15\", \"validation\"))\n",
    "model_datasets = {'Training': xgboost_train_predict_df, 'Validation': xgboost_val_predict_df, 'Test': xgboost_test_predict_df}\n",
    "show_predictions_eval(model_datasets, \"DEP_DEL15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "096e807d-72ff-4277-bcdb-86a4e239eb54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "booster = xgboost_model.get_booster()\n",
    "booster.get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2bf7a1-b5ab-4a18-8716-a7dc4b05bcf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the raw data for temporal ranking, but transform it with the pipeline\n",
    "train_data_cv_total = pipeline_model.transform(\n",
    "    train_data.withColumn(\n",
    "        \"rank\", \n",
    "        percent_rank().over(Window.partitionBy().orderBy([\"YEAR\", \"MONTH\", \"DAY_OF_MONTH\"]))\n",
    "    )\n",
    ").repartition(20).cache()  # Now contains both raw columns and encoded features\n",
    "training_fold_percent_threshold = [0.2, 0.4, 0.6, 0.8]\n",
    "rolling_time_series_cv_train = {}\n",
    "rolling_time_series_cv_val = {}\n",
    "for tfpt in training_fold_percent_threshold:\n",
    "    rolling_time_series_cv_train[tfpt] = train_data_cv_total.filter(train_data_cv_total.rank <= tfpt).cache()\n",
    "    rolling_time_series_cv_val[tfpt] = train_data_cv_total.filter(train_data_cv_total.rank > tfpt).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5301cc81-cbbc-4b05-8a77-6ac84d4752b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.2, 1, log=True),\n",
    "    }\n",
    "    \n",
    "    avg_score = 0\n",
    "    for tfpt in training_fold_percent_threshold:\n",
    "        train_data_cv = rolling_time_series_cv_train[tfpt]\n",
    "        val_data_cv = rolling_time_series_cv_val[tfpt]\n",
    "\n",
    "        xgb = SparkXGBClassifier(\n",
    "            features_col=\"features_unscaled\",\n",
    "            label_col=\"DEP_DEL15\",\n",
    "            #device=\"cuda\",\n",
    "            **params\n",
    "        )\n",
    "        model = xgb.fit(train_data_cv)\n",
    "        predictions = model.transform(val_data_cv)\n",
    "        score = evaluate_predictions(predictions, \"DEP_DEL15\", \"cv\", f2_only=True)\n",
    "        avg_score += score\n",
    "\n",
    "    return avg_score / len(training_fold_percent_threshold)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, timeout=3600)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b74e00-17b3-479a-a4f2-539ef0bf2337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data_ml = train_data_ml.repartition(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7f41b1-5c76-4c47-b557-d6adc91a60e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# With Hyperparameter Tuning\n",
    "xgboost_classifier = SparkXGBClassifier (\n",
    "  features_col=\"features\",\n",
    "  label_col=\"DEP_DEL15\",\n",
    "  n_estimators=115,\n",
    "  max_depth=9,\n",
    "  learning_rate=0.27122395210228795,\n",
    "  #device=\"cuda\",\n",
    ")\n",
    "\n",
    "# train and return the model\n",
    "xgboost_model = xgboost_classifier.fit(train_data_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96d447f-c6c1-40e7-92be-23eb0cc58f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgboost_train_predict_df = xgboost_model.transform(train_data_ml)\n",
    "print(evaluate_predictions(xgboost_train_predict_df, \"DEP_DEL15\", \"train\"))\n",
    "xgboost_val_predict_df = xgboost_model.transform(validation_data_ml)\n",
    "print(evaluate_predictions(xgboost_val_predict_df, \"DEP_DEL15\", \"validation\"))\n",
    "xgboost_test_predict_df = xgboost_model.transform(test_data_ml)\n",
    "print(evaluate_predictions(xgboost_test_predict_df, \"DEP_DEL15\", \"validation\"))\n",
    "model_datasets = {'Training': xgboost_train_predict_df, 'Validation': xgboost_val_predict_df, 'Test': xgboost_test_predict_df}\n",
    "show_predictions_eval(model_datasets, \"DEP_DEL15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcfe7593-e706-4b50-9dc2-33ea960dccf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Neural Networks with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08acb68d-ee79-4b10-b42a-73feb97bbc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add sample_weight and label BEFORE flattening\n",
    "train_data_trans = train_data_transformed \\\n",
    "    .withColumn(\"label\", col(\"DEP_DEL15\")) \\\n",
    "    .withColumn(\"sample_weight\", col(\"sample_weight\"))\n",
    "\n",
    "# Use the raw data for temporal ranking\n",
    "train_data_cv_total = train_data_trans.withColumn(\n",
    "    \"rank\", \n",
    "    percent_rank().over(Window.partitionBy().orderBy([\"YEAR\", \"MONTH\", \"DAY_OF_MONTH\"]))\n",
    ")\n",
    "\n",
    "# Now convert features to array\n",
    "train_flat = train_data_cv_total.withColumn(\"features_array\", vector_to_array(\"features_unscaled\"))\n",
    "feature_length = len(train_flat.select(\"features_array\").first()[0])\n",
    "feature_cols = [f\"f_{i}\" for i in range(feature_length)]\n",
    "\n",
    "# Select feature columns + label + weight + rank\n",
    "train_flat = train_flat.select(\n",
    "    *[col(\"features_array\")[i].alias(feature_cols[i]) for i in range(feature_length)],\n",
    "    \"label\",\n",
    "    \"sample_weight\",\n",
    "    \"rank\"\n",
    ").cache()\n",
    "\n",
    "# Same for val/test (without time series ranking)\n",
    "val_data_trans = validation_data_transformed.withColumn(\"label\", col(\"DEP_DEL15\"))\n",
    "val_flat = val_data_trans.withColumn(\"features_array\", vector_to_array(\"features_unscaled\"))\n",
    "val_flat = val_flat.select(\n",
    "    *[col(\"features_array\")[i].alias(feature_cols[i]) for i in range(feature_length)],\n",
    "    \"label\"\n",
    ").cache()\n",
    "\n",
    "test_data_trans = test_data_transformed.withColumn(\"label\", col(\"DEP_DEL15\"))\n",
    "test_flat = test_data_trans.withColumn(\"features_array\", vector_to_array(\"features_unscaled\"))\n",
    "test_flat = test_flat.select(\n",
    "    *[col(\"features_array\")[i].alias(feature_cols[i]) for i in range(feature_length)],\n",
    "    \"label\"\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104f2e67-5303-4b2a-a323-bfc9d7f6589c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Save train/validate/test split as a parquet files\n",
    "# train_flat.write.parquet(f\"{folder_path}/df_otpw_60M_train_flat_petastorm.parquet\") # use to save a new version\n",
    "# val_flat.write.parquet(f\"{folder_path}/df_otpw_60M_validation_flat_petastorm.parquet\") # use to save a new version\n",
    "# test_flat.write.parquet(f\"{folder_path}/df_otpw_60M_test_flat_petastorm.parquet\") # use to save a new version\n",
    "\n",
    "\n",
    "# Save train/validate/test split as parquet files\n",
    "train_flat.write.mode(\"overwrite\").parquet(f\"{folder_path}/df_otpw_2015_2024_train_flat.parquet\")\n",
    "val_flat.write.mode(\"overwrite\").parquet(f\"{folder_path}/df_otpw_2015_2024_validation_flat.parquet\")\n",
    "test_flat.write.mode(\"overwrite\").parquet(f\"{folder_path}/df_otpw_2015_2024_test_flat.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a79639-4890-4d0c-af80-6a941e4329b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# folder_path = f\"dbfs:/student-groups/Group_04_04\"\n",
    "\n",
    "# Enable Arrow-based conversion\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Load data\n",
    "train_df = spark.read.parquet(\"dbfs:/student-groups/Group_04_04/df_otpw_2015_2024_train_flat.parquet\").toPandas()\n",
    "val_df = spark.read.parquet(\"dbfs:/student-groups/Group_04_04/df_otpw_2015_2024_validation_flat.parquet\").toPandas()\n",
    "test_df = spark.read.parquet(\"dbfs:/student-groups/Group_04_04/df_otpw_2015_2024_test_flat.parquet\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81238c91-dc56-4a22-b2f0-79fab52a852f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "feature_cols = [c for c in train_df.columns if c.startswith(\"f_\")]\n",
    "\n",
    "# Prepare arrays for validation and test sets\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "# Create time series CV splits\n",
    "training_fold_percent_threshold = [0.2, 0.4, 0.6, 0.8]\n",
    "cv_train_dfs = {}\n",
    "cv_val_dfs = {}\n",
    "\n",
    "for tfpt in training_fold_percent_threshold:\n",
    "    cv_train_dfs[tfpt] = train_df[train_df['rank'] <= tfpt].copy()\n",
    "    cv_val_dfs[tfpt] = train_df[train_df['rank'] > tfpt].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b79787-c199-46d5-bf7f-2fe9bb6bc879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "def create_model(hidden_layers=[128], dropout_rate=0.3, l2_reg=0.001, use_batch_norm=False):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(len(feature_cols),)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg)\n",
    "        ))\n",
    "        if use_batch_norm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a0c2c6-f7e3-4661-9c9e-2081f546285a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training Function with Your Exact Setup\n",
    "def train_model(model, X_train_data, y_train_data, X_val_data, y_val_data, \n",
    "                sample_weight_data=None, batch_size=256, epochs=100, lr=0.0005, verbose=0):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_data, y_train_data,\n",
    "        sample_weight=sample_weight_data,\n",
    "        validation_data=(X_val_data, y_val_data),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', \n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            lr_scheduler\n",
    "        ],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6abf10-94d5-40f8-86b3-5dcedecd0e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "def evaluate_model(model, X, y):\n",
    "    pred_proba = model.predict(X, verbose=0)\n",
    "    pred_labels = (pred_proba >= 0.5).astype(int).flatten()\n",
    "    results_df = pd.DataFrame({\n",
    "        \"label\": y,\n",
    "        \"prediction\": pred_labels,\n",
    "        \"probability\": pred_proba.flatten()\n",
    "    })\n",
    "    spark_df = spark.createDataFrame(results_df)\n",
    "    return evaluate_predictions(spark_df, \"label\", \"dataset_name\")\n",
    "\n",
    "def plot_confusion_matrix_grid(grid_results, target_col):\n",
    "    \"\"\"Plot confusion matrices for all configurations\"\"\"\n",
    "    n_models = len(grid_results)\n",
    "    \n",
    "    # Check if there are any results to plot\n",
    "    if n_models == 0:\n",
    "        print(\"No models to visualize.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(max(1, n_models), 3, figsize=(18, 5*max(1, n_models)))\n",
    "    \n",
    "    # Handle the case where there's only one model\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, result in enumerate(grid_results):\n",
    "        params = result['params']\n",
    "        title_suffix = f\"\\nLayers: {params.get('hidden_layers', [])} \" \\\n",
    "                      f\"| BS: {params.get('batch_size', '')} \" \\\n",
    "                      f\"| LR: {params.get('learning_rate', '')} \" \\\n",
    "                      f\"| DO: {params.get('dropout_rate', '')}\"\n",
    "        \n",
    "        for j, dataset in enumerate(['train', 'val', 'test']):\n",
    "            cm = np.array(result['metrics'][dataset]['confusion_matrix'])\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                      ax=axes[i][j],\n",
    "                      cbar=False)\n",
    "            axes[i][j].set_title(f\"Config {i+1} {dataset.capitalize()}{title_suffix}\")\n",
    "            axes[i][j].set_xlabel(\"Predicted\")\n",
    "            axes[i][j].set_ylabel(\"Actual\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_results(grid_results):\n",
    "    \"\"\"Enhanced Comparison Table\"\"\"\n",
    "    # Check if there are any results to compare\n",
    "    if not grid_results:\n",
    "        print(\"No models to compare.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    comparison_data = []\n",
    "    \n",
    "    for result in grid_results:\n",
    "        params = result['params']\n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Layers': str(params.get('hidden_layers', [128])),\n",
    "            'Batch Size': params.get('batch_size', 256),\n",
    "            'LR': params.get('learning_rate', 0.0005),\n",
    "            'Dropout': params.get('dropout_rate', 0.3),\n",
    "            'L2': params.get('l2_reg', 0.001),\n",
    "            'BatchNorm': params.get('use_batch_norm', False),\n",
    "            'CV F2': metrics['cv']['f2'] if 'cv' in metrics else metrics['train']['f2'],\n",
    "            'CV Recall': metrics['cv']['recall'] if 'cv' in metrics else metrics['train']['recall'],\n",
    "            'Val F2': metrics['val']['f2'],\n",
    "            'Val Recall': metrics['val']['recall'],\n",
    "            'Test F2': metrics['test']['f2'],\n",
    "            'Test Recall': metrics['test']['recall']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data).sort_values('CV F2', ascending=False)\n",
    "    \n",
    "    print(\"\\nFinal Comparison:\")\n",
    "    print(df.to_markdown(\n",
    "        index=False, \n",
    "        floatfmt=\".4f\",\n",
    "        headers=['Layers', 'Batch', 'LR', 'DO', 'L2', 'BN',\n",
    "                'CV-F2', 'CV-Rcl', 'ValF2', 'ValRcl',\n",
    "                'TstF2', 'TstRcl']\n",
    "    ))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7964ff5-5810-4dc5-b4bc-f3e8f1cf5134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_optuna_trial_and_get_best_params(n_trials=1):\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10, interval_steps=1)\n",
    "    )\n",
    "\n",
    "    def objective(trial):\n",
    "        n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "        hidden_layers = [trial.suggest_int(f'units_l{i}', 32, 512, step=32) for i in range(n_layers)]\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [128, 256, 512, 1024])\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "\n",
    "        cv_f2_scores = []\n",
    "        for tfpt in training_fold_percent_threshold:\n",
    "            train_fold_df = cv_train_dfs[tfpt]\n",
    "            val_fold_df = cv_val_dfs[tfpt]\n",
    "            X_train_fold = train_fold_df[feature_cols].values\n",
    "            y_train_fold = train_fold_df[\"label\"].values\n",
    "            sample_weight_fold = train_fold_df[\"sample_weight\"].values if \"sample_weight\" in train_fold_df.columns else None\n",
    "            X_val_fold = val_fold_df[feature_cols].values\n",
    "            y_val_fold = val_fold_df[\"label\"].values\n",
    "\n",
    "            model = create_model(hidden_layers, dropout_rate, l2_reg, use_batch_norm)\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                sample_weight=sample_weight_fold,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                epochs=20,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            pred_proba = model.predict(X_val_fold, verbose=0)\n",
    "            pred_labels = (pred_proba >= 0.5).astype(int).flatten()\n",
    "            results_df = pd.DataFrame({\n",
    "                \"label\": y_val_fold,\n",
    "                \"prediction\": pred_labels,\n",
    "                \"probability\": pred_proba.flatten()\n",
    "            })\n",
    "            spark_df = spark.createDataFrame(results_df)\n",
    "            fold_metrics = evaluate_predictions(spark_df, \"label\", f\"cv_{tfpt}\", f2_only=True)\n",
    "            cv_f2_scores.append(fold_metrics)\n",
    "\n",
    "        return np.mean(cv_f2_scores)\n",
    "\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        return study.best_params, study.best_trial\n",
    "    except Exception as e:\n",
    "        print(f\"Optuna error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b8019c-de5b-4f5d-a54c-2c21f0519663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_final_model(best_params):\n",
    "    # Extract best hyperparameters\n",
    "    hidden_layers = [best_params[f'units_l{i}'] for i in range(best_params['n_layers'])]\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    l2_reg = best_params['l2_reg']\n",
    "    batch_size = best_params['batch_size']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    use_batch_norm = best_params['use_batch_norm']\n",
    "\n",
    "    # Prepare training data\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df[\"label\"].values\n",
    "    sample_weight = train_df[\"sample_weight\"].values if \"sample_weight\" in train_df.columns else None\n",
    "\n",
    "    # Build final model\n",
    "    model = create_model(hidden_layers, dropout_rate, l2_reg, use_batch_norm)\n",
    "    history = train_model(\n",
    "        model, X_train, y_train, X_val, y_val,\n",
    "        sample_weight, batch_size, 100, learning_rate, verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate on train, val, test\n",
    "    metrics = {\n",
    "        'train': evaluate_model(model, X_train, y_train),\n",
    "        'val': evaluate_model(model, X_val, y_val),\n",
    "        'test': evaluate_model(model, X_test, y_test)\n",
    "    }\n",
    "\n",
    "    # model.save(\"best_flight_delay_model.keras\")\n",
    "    return model, history, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3170c92-5197-47e1-b8a6-966de3853ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experiment 1 - Provide Manual Grid search for Data between 2015-2021 ( Train : 2015-2019; Validation: 2020, Test: 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e8ccf4-c6d9-43b6-8a6e-7b7eb5c4e086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'hidden_layers': [128], 'batch_size': 256, \n",
    "     'learning_rate': 0.0005, 'dropout_rate': 0.2},\n",
    "    {'hidden_layers': [128, 64], 'batch_size': 512,\n",
    "     'learning_rate': 0.0003, 'dropout_rate': 0.3},\n",
    "    {'hidden_layers': [256, 128, 64], 'batch_size': 1024,\n",
    "     'learning_rate': 0.0001, 'dropout_rate': 0.5},\n",
    "    {'hidden_layers': [128], 'batch_size': 256,\n",
    "     'learning_rate': 0.001, 'dropout_rate': 0.4}\n",
    "]\n",
    "    \n",
    "# Run grid search\n",
    "grid_results = run_grid_search(param_grid)\n",
    "\n",
    "# Show comparison\n",
    "compare_results(grid_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94501dfc-a3e2-4b4c-a976-a9996b6fda3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experiment 2 - Find Best Hyperparameters using Optuna and Cross Validation for Data between 2015-2024 ( Train : 2015-2021; Validation: 2022-2023, Test: 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7ba117-ef49-479e-89ac-4410d782e478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔍 Running one Optuna trial to get best parameters...\")\n",
    "best_params, best_trial = run_optuna_trial_and_get_best_params(n_trials=1)\n",
    "\n",
    "print(\"\\n Best hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84ecd66-479a-432f-bbaf-85cb4c8db0af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These parameters have been extracted from the best trial run conducted above, due to the randomness of Optuna, the console log above might now show different values\n",
    "best_params = {\n",
    "    'n_layers': 1,\n",
    "    'units_l0': 448,\n",
    "    'dropout_rate': 0.27565986839411283,\n",
    "    'l2_reg': 0.0001554892275405439,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1.1258097381851819e-05,\n",
    "    'use_batch_norm': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321868a6-6beb-43c2-b1f8-37568b3dd7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df[feature_cols] = train_df[feature_cols].astype(np.float32)\n",
    "val_df[feature_cols] = val_df[feature_cols].astype(np.float32)\n",
    "test_df[feature_cols] = test_df[feature_cols].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07f7d890-6cfa-46ef-af67-89f5f7f6ac9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nBest hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "final_model, final_history, final_metrics = train_and_evaluate_final_model(best_params)\n",
    "\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(f\"{split.upper()} F2: {final_metrics[split]['f2']:.4f} | Recall: {final_metrics[split]['recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4164a3-1a88-45d5-aec7-2f7c558c50c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    m = final_metrics[split]\n",
    "    print(f\"{split.upper()} F2: {m['f2']:.4f} | Recall: {m['recall']:.4f} | Precision: {m['precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a40aaa-f076-4a9c-a030-8fec4c000ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, split in enumerate(['train', 'val', 'test']):\n",
    "    cm = np.array(final_metrics[split]['confusion_matrix'])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[idx])\n",
    "    axes[idx].set_title(f\"{split.capitalize()} Confusion Matrix\")\n",
    "    axes[idx].set_xlabel(\"Predicted\")\n",
    "    axes[idx].set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd23c11-5eca-43d3-b455-63fb38501a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BiLSTM on the hyperparameters found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a36940-df60-4ff4-a77c-1fddc59fd84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_bilstm_model(units=128, dropout_rate=0.3, l2_reg=0.001):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1, len(feature_cols))),\n",
    "        tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(units, return_sequences=False,\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe71b4e-2931-4e78-8c1b-9c5111873a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_bilstm_model(best_params):\n",
    "    # Extract best hyperparameters\n",
    "    units = best_params['units_l0']  # Only one layer for LSTM\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    l2_reg = best_params['l2_reg']\n",
    "    batch_size = best_params['batch_size']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    # Prepare training data with sequence dimension\n",
    "    X_train = train_df[feature_cols].values.astype(np.float32).reshape((-1, 1, len(feature_cols)))\n",
    "    y_train = train_df[\"label\"].values\n",
    "    sample_weight = train_df[\"sample_weight\"].values if \"sample_weight\" in train_df.columns else None\n",
    "\n",
    "    X_val_seq = X_val.reshape((-1, 1, X_val.shape[1]))\n",
    "    X_test_seq = X_test.reshape((-1, 1, X_test.shape[1]))\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = create_bilstm_model(units, dropout_rate, l2_reg)\n",
    "\n",
    "    history = train_model(\n",
    "        model, X_train, y_train, X_val_seq, y_val,\n",
    "        sample_weight, batch_size, 100, learning_rate, verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    metrics = {\n",
    "        'train': evaluate_model(model, X_train, y_train),\n",
    "        'val': evaluate_model(model, X_val_seq, y_val),\n",
    "        'test': evaluate_model(model, X_test_seq, y_test)\n",
    "    }\n",
    "\n",
    "    return model, history, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9cadb4-b48c-4033-974b-0514bca39a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bilstm_model, bilstm_history, bilstm_metrics = train_and_evaluate_bilstm_model(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f418e896-2413-49a6-afdd-7847636f9e86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for split in ['train', 'val', 'test']:\n",
    "    m = bilstm_metrics[split]\n",
    "    print(f\"{split.upper()} F2: {m['f2']:.4f} | Recall: {m['recall']:.4f} | Precision: {m['precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f39032-8130-45f4-b782-10f8205b3fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, split in enumerate(['train', 'val', 'test']):\n",
    "    cm = np.array(bilstm_metrics[split]['confusion_matrix'])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[idx])\n",
    "    axes[idx].set_title(f\"{split.capitalize()} Confusion Matrix\")\n",
    "    axes[idx].set_xlabel(\"Predicted\")\n",
    "    axes[idx].set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "848a23e1-4a38-4b97-b84c-de53113ea481",
     "origId": 2207202308819690,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6352054888935144,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "w261_flight_prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}