{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install isodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from isodate import parse_duration\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_key = \"/Users/sacayo/documents/Youtube_api_key\"\n",
    "with open(path_to_key, \"r\") as f:\n",
    "    api_keys = f.read().split('\\n')\n",
    "api_key1 = api_keys[0]\n",
    "api_key2 = api_keys[1]\n",
    "api_key3 = api_keys[2]\n",
    "api_key4 = api_keys[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for API query\n",
    "COMMENT_MAX_RESULTS = 200\n",
    "VIDEO_MAX_RESULTS = 50\n",
    "EXAMPLES_PER_GROUP = 300\n",
    "SAVE_FREQUENCY = 50\n",
    "SEARCH_TERMS = [\"news\", \"business\", \"markets\", \"economy\"]\n",
    "CACHE_FILE = 'video_cache.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotaManager:\n",
    "    \"\"\" Request limiter for YouTube Data API\"\"\"\n",
    "    def __init__(self, daily_limit):\n",
    "        self.daily_limit = daily_limit\n",
    "        self.usage = 0\n",
    "\n",
    "    def can_make_request(self, cost=1):\n",
    "        return self.usage + cost <= self.daily_limit\n",
    "\n",
    "    def log_request(self, cost=1):\n",
    "        self.usage += cost\n",
    "\n",
    "def create_youtube_client(api_key):\n",
    "    return build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def load_cache(cache_file):\n",
    "    try:\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def save_cache(cache, cache_file):\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def execute_with_backoff(request, quota_manager):\n",
    "    for n in range(6):  # Maximum of 5 retries\n",
    "        if not quota_manager.can_make_request():\n",
    "            raise Exception(\"Daily quota limit reached\")\n",
    "        try:\n",
    "            response = request.execute()\n",
    "            quota_manager.log_request()\n",
    "            return response\n",
    "        except HttpError as e:\n",
    "            if e.resp.status in [403, 429, 500, 503]:  # Retry on these status codes\n",
    "                delay = 2 ** n  # Exponential backoff\n",
    "                print(f\"Quota exceeded or server error. Retrying in {delay} seconds.\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Failed after multiple retries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_comments(youtube, video_id, quota_manager, max_results=COMMENT_MAX_RESULTS):\n",
    "    \"\"\"Fetch comments for a video.\"\"\"\n",
    "    next_page_token = None\n",
    "    while True:\n",
    "        try:\n",
    "            comments_request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_results,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            comments_response = execute_with_backoff(comments_request, quota_manager)\n",
    "            \n",
    "            # Process comments here if needed\n",
    "            \n",
    "            if 'nextPageToken' not in comments_response:\n",
    "                break\n",
    "            next_page_token = comments_response['nextPageToken']\n",
    "        except HttpError as e:\n",
    "            if e.resp.status == 403 and 'commentsDisabled' in str(e.content):\n",
    "                print(f\"Comments disabled for video {video_id}\")\n",
    "            else:\n",
    "                print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_videos_details(youtube, video_ids, quota_manager):\n",
    "    \"\"\"Fetch details for multiple videos in a single request.\"\"\"\n",
    "    try:\n",
    "        video_request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics,status\",\n",
    "            id=','.join(video_ids),\n",
    "            fields=\"items(id,snippet(title,channelTitle,categoryId,publishedAt),contentDetails(duration),statistics(viewCount,likeCount,dislikeCount,commentCount),status(madeForKids))\"\n",
    "        )\n",
    "        video_response = execute_with_backoff(video_request, quota_manager)\n",
    "        return {item['id']: item for item in video_response.get('items', [])}\n",
    "    except HttpError as e:\n",
    "        print(f\"Error fetching details for videos: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_data(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        # File exists, append without writing the header\n",
    "        df.to_csv(filename, mode='a', header=False, index=False)\n",
    "        print(f\"Data appended to existing file {filename}\")\n",
    "    else:\n",
    "        # File doesn't exist, create a new file with header\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"New file created: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\" Data Collection of YouTube API\"\"\"\n",
    "    f_name =  f'youtube_extract1_tv_{datetime.now().strftime(\"%d%b%Y\")}.csv'\n",
    "    youtube = create_youtube_client(api_key2)\n",
    "    quota_manager = QuotaManager(DAILY_QUOTA_LIMIT)\n",
    "    video_cache = load_cache(CACHE_FILE)\n",
    "    \n",
    "    video_data = []\n",
    "    video_ids_seen = set()\n",
    "    videos_collected = 0\n",
    "    published_after = (datetime.now() - timedelta(days=7)).isoformat(\"T\") + \"Z\"\n",
    "\n",
    "    while videos_collected < EXAMPLES_PER_GROUP:\n",
    "        search_query = random.choice(SEARCH_TERMS)\n",
    "        print(f\"Searching for: {search_query}\")\n",
    "        \n",
    "        try:\n",
    "            search_request = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                q=search_query,\n",
    "                type=\"video\",\n",
    "                publishedAfter=published_after,\n",
    "                maxResults=VIDEO_MAX_RESULTS,\n",
    "                relevanceLanguage='en'\n",
    "            )\n",
    "            search_response = execute_with_backoff(search_request, quota_manager)\n",
    "\n",
    "            video_ids_batch = []\n",
    "            for item in search_response.get('items', []):\n",
    "                video_id = item['id']['videoId']\n",
    "                if video_id not in video_ids_seen:\n",
    "                    video_ids_batch.append(video_id)\n",
    "                    video_ids_seen.add(video_id)\n",
    "\n",
    "                if len(video_ids_batch) == VIDEO_MAX_RESULTS:\n",
    "                    break\n",
    "\n",
    "            videos_details = fetch_videos_details(youtube, video_ids_batch, quota_manager)\n",
    "\n",
    "            for video_id, video_details in videos_details.items():\n",
    "                if video_id in video_cache:\n",
    "                    video_details = video_cache[video_id]\n",
    "                else:\n",
    "                    video_cache[video_id] = video_details\n",
    "\n",
    "                if video_details['status']['madeForKids']:\n",
    "                    print(f\"Skipping video {video_id} as it is made for kids\")\n",
    "                    continue\n",
    "\n",
    "                duration = parse_duration(video_details['contentDetails']['duration']).total_seconds()\n",
    "                if not (60 < duration <= 600):\n",
    "                    continue\n",
    "\n",
    "                fetch_comments(youtube, video_id, quota_manager)\n",
    "\n",
    "                snippet = video_details['snippet']\n",
    "                statistics = video_details['statistics']\n",
    "                \n",
    "                video_data.append({\n",
    "                    'video_id': video_id,\n",
    "                    'video_title': snippet['title'],\n",
    "                    'channel_name': snippet['channelTitle'],\n",
    "                    'genre': youtube.videoCategories().list(part=\"snippet\", id=snippet['categoryId']).execute()['items'][0]['snippet']['title'],\n",
    "                    'views': int(statistics.get('viewCount', 0)),\n",
    "                    'likes': int(statistics.get('likeCount', 0)),\n",
    "                    'dislikes': statistics.get('dislikeCount', 0),\n",
    "                    'comment_count': int(statistics.get('commentCount', 0)),\n",
    "                    'video_length': duration,\n",
    "                    'video_posting_date': datetime.strptime(snippet['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d\"),\n",
    "                    'made_for_kids': video_details['status']['madeForKids']\n",
    "                })\n",
    "\n",
    "                videos_collected += 1\n",
    "\n",
    "                if videos_collected % SAVE_FREQUENCY == 0:\n",
    "                    save_data(video_data, f'youtube_extract1_tv_{datetime.now().strftime(\"%d%b%Y\")}_partial_{videos_collected}.csv')\n",
    "                    save_cache(video_cache, CACHE_FILE)\n",
    "\n",
    "                if videos_collected >= EXAMPLES_PER_GROUP:\n",
    "                    break\n",
    "\n",
    "            time.sleep(1)  # Basic rate limiting\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            if e.resp.status in [403, 429]:\n",
    "                print(\"Quota exceeded or rate limit hit. Waiting before retrying...\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    save_data(video_data, f_name)\n",
    "    save_cache(video_cache, CACHE_FILE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # for loop to automate rerun of main\n",
    "    for i in range(50):\n",
    "        print(\"Attempt: \",i)\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run after getting all the other videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_file =  f'youtube_extract_tv_{datetime.now().strftime(\"%d%b%Y\")}_combined.csv'\n",
    "\n",
    "# List of specific CSV files to combine\n",
    "csv_files = [\n",
    "    \"sacayo_youtube_extract_tv_15Jul2024_combined_deduplicated.csv\",\n",
    "    \"sacayo_youtube_extract_tv_16Jul2024_combined_deduplicated.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "# List to hold each DataFrame\n",
    "dfs = []\n",
    "\n",
    "# Loop through all specified CSV files and read them into DataFrames\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {csv_file} not found. Skipping.\")\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(combined_file, index=False)\n",
    "\n",
    "print(f\"Combined data saved to {combined_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_deduped_file =  f'youtube_extract_tv_{datetime.now().strftime(\"%d%b%Y\")}_combined_deduplicated.csv'\n",
    "combined_file =  f'youtube_extract_tv_{datetime.now().strftime(\"%d%b%Y\")}_combined.csv'\n",
    "f_name =  f'youtube_extract1_tv_{datetime.now().strftime(\"%d%b%Y\")}.csv'\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(combined_file)\n",
    "\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Save cleaned data to a new CSV file\n",
    "df_cleaned.to_csv(combined_deduped_file, index=False)\n",
    "\n",
    "print(\"Duplicates removed and cleaned CSV saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the deduplicated CSV file\n",
    "df = pd.read_csv(combined_deduped_file)\n",
    "\n",
    "# Find titles not containing 'bitcoin' or 'cryptocurrency'\n",
    "non_bitcoin_titles = df[~df['video_title'].str.contains('bitcoin', case=False) & ~df['video_title'].str.contains('cryptocurrency', case=False)]\n",
    "\n",
    "# Count the number of videos\n",
    "num_videos = non_bitcoin_titles.shape[0]\n",
    "\n",
    "# Print the count and titles\n",
    "print(f\"Number of videos without 'bitcoin' or 'cryptocurrency': {num_videos}\\n\")\n",
    "for idx, title in enumerate(non_bitcoin_titles['video_title'], 1):\n",
    "    print(f\"{idx}. {title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_tokenize(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Read the deduplicated CSV file\n",
    "df = pd.read_csv(combined_deduped_file)\n",
    "\n",
    "# Find titles not containing 'bitcoin' or 'cryptocurrency'\n",
    "non_bitcoin_titles = df[~df['video_title'].str.contains('bitcoin', case=False) & ~df['video_title'].str.contains('cryptocurrency', case=False)\n",
    "                        & ~df['video_title'].str.contains('crypto', case=False) & ~df['video_title'].str.contains('blockchain', case=False)]\n",
    "\n",
    "# Extract all titles as a single text\n",
    "all_titles = ' '.join(non_bitcoin_titles['video_title'])\n",
    "\n",
    "# Clean and tokenize the text\n",
    "tokens = clean_tokenize(all_titles)\n",
    "\n",
    "# Count frequency of each word\n",
    "word_freq = Counter(tokens)\n",
    "\n",
    "# Get the most common words (adjust the number as needed)\n",
    "common_words = word_freq.most_common(30)\n",
    "\n",
    "# Print common words\n",
    "print(\"Common words in video titles without 'bitcoin' 'crypto' 'blockchain' or 'cryptocurrency':\")\n",
    "for word, freq in common_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['made_for_kids'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import csv\n",
    "\n",
    "# Load the video IDs from the combined deduplicated CSV file\n",
    "df = pd.read_csv(combined_deduped_file)\n",
    "video_ids = df['video_id'].tolist()\n",
    "\n",
    "# Load the existing comments CSV file and extract the video IDs\n",
    "try:\n",
    "    existing_comments_df = pd.read_csv('youtube_comments_07jul2024.csv')\n",
    "    existing_video_ids = set(existing_comments_df['video_id'].tolist())\n",
    "except FileNotFoundError:\n",
    "    existing_video_ids = set()\n",
    "\n",
    "# Function to extract comments for a given video ID\n",
    "def extract_comments(youtube, video_id, max_results=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            comments_request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_results,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            comments_response = comments_request.execute()\n",
    "\n",
    "            for item in comments_response.get('items', []):\n",
    "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comment = {\n",
    "                    'video_id': video_id,\n",
    "                    'comment_id': item['id'],\n",
    "                    'author': top_comment['authorDisplayName'],\n",
    "                    'text': top_comment['textDisplay'],\n",
    "                    'published_at': top_comment['publishedAt'],\n",
    "                    'like_count': top_comment['likeCount']\n",
    "                }\n",
    "                comments.append(comment)\n",
    "\n",
    "                # Get replies to the top-level comment\n",
    "                total_reply_count = item['snippet']['totalReplyCount']\n",
    "                if total_reply_count > 0:\n",
    "                    comment_id = item['id']\n",
    "                    next_reply_page_token = None\n",
    "\n",
    "                    while True:\n",
    "                        replies_request = youtube.comments().list(\n",
    "                            part=\"snippet\",\n",
    "                            parentId=comment_id,\n",
    "                            maxResults=max_results,\n",
    "                            pageToken=next_reply_page_token,\n",
    "                            textFormat=\"plainText\"\n",
    "                        )\n",
    "                        replies_response = replies_request.execute()\n",
    "\n",
    "                        for reply in replies_response.get('items', []):\n",
    "                            reply_snippet = reply['snippet']\n",
    "                            reply_comment = {\n",
    "                                'video_id': video_id,\n",
    "                                'comment_id': reply['id'],\n",
    "                                'author': reply_snippet['authorDisplayName'],\n",
    "                                'text': reply_snippet['textDisplay'],\n",
    "                                'published_at': reply_snippet['publishedAt'],\n",
    "                                'like_count': reply_snippet['likeCount']\n",
    "                            }\n",
    "                            comments.append(reply_comment)\n",
    "\n",
    "                        next_reply_page_token = replies_response.get('nextPageToken')\n",
    "                        if not next_reply_page_token:\n",
    "                            break\n",
    "\n",
    "            next_page_token = comments_response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        except HttpError as e:\n",
    "            print(f\"An HTTP error {e.resp.status} occurred:\\n{e.content}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Function to save comments to a CSV file\n",
    "def save_comments_to_csv(comments, filename='youtube_comments_07jul2024.csv'):\n",
    "    # Define the header\n",
    "    header = ['video_id', 'comment_id', 'author', 'text', 'published_at', 'like_count']\n",
    "\n",
    "    # Check if the file already exists\n",
    "    try:\n",
    "        with open(filename, 'x', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    # Append comments to the file\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        for comment in comments:\n",
    "            writer.writerow(comment)\n",
    "\n",
    "# Collect all comments for all video IDs and save them incrementally\n",
    "for video_id in video_ids:\n",
    "    if video_id in existing_video_ids:\n",
    "        print(f\"Skipping video ID {video_id} as it's already processed.\")\n",
    "        continue\n",
    "    print(f\"Extracting comments for video ID: {video_id}\")\n",
    "    comments = extract_comments(youtube, video_id)\n",
    "    save_comments_to_csv(comments)\n",
    "    print(f\"Comments for video ID {video_id} saved.\")\n",
    "\n",
    "print(\"All comments have been saved incrementally to youtube_comments_07jul2024.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to get comments and replies\n",
    "\n",
    "def get_all_comments_and_replies(youtube, video_id, quota_manager):\n",
    "    comments_data = []\n",
    "\n",
    "    # Function to get replies for a given comment\n",
    "    def get_replies(parent_id):\n",
    "        replies = []\n",
    "        request = youtube.comments().list(\n",
    "            part='snippet',\n",
    "            parentId=parent_id,\n",
    "            maxResults=100\n",
    "        )\n",
    "        while request:\n",
    "            response = execute_with_backoff(request, quota_manager)\n",
    "            for item in response['items']:\n",
    "                replies.append(item['snippet']['textDisplay'])\n",
    "            request = youtube.comments().list_next(request, response)\n",
    "        return replies\n",
    "\n",
    "    # Function to get top-level comments and their replies\n",
    "    def get_comments(video_id):\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=100\n",
    "            )\n",
    "            while request:\n",
    "                response = execute_with_backoff(request, quota_manager)\n",
    "                for item in response['items']:\n",
    "                    video_id = item['snippet']['videoId']\n",
    "                    top_level_comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                    comment_replies = get_replies(item['id']) if item['snippet']['totalReplyCount'] > 0 else []\n",
    "                    comments_data.append({'video_id': video_id, 'comment': top_level_comment, 'replies': comment_replies})\n",
    "                request = youtube.commentThreads().list_next(request, response)\n",
    "        except HttpError as e:\n",
    "            if e.resp.status == 403:\n",
    "                print(f\"Comments are disabled for video ID: {video_id}\")\n",
    "            else:\n",
    "                print(f\"An HTTP error {e.resp.status} occurred: {e.content}\")\n",
    "\n",
    "    get_comments(video_id)\n",
    "    return comments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_stats(video_id, quota_manager):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key3)\n",
    "\n",
    "    try:\n",
    "        request = youtube.videos().list(\n",
    "            part='statistics',\n",
    "            id=video_id\n",
    "        )\n",
    "        video_stats = execute_with_backoff(request, quota_manager)\n",
    "        if 'items' in video_stats and video_stats['items']:\n",
    "            return video_stats['items'][0]['statistics']\n",
    "        else:\n",
    "            print(f\"No statistics available for video ID: {video_id}\")\n",
    "            return {}\n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error {e.resp.status} occurred: {e.content}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while fetching stats for video {video_id}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to get video statistics\n",
    "\n",
    "def get_views(video_stats):\n",
    "    return int(video_stats.get('viewCount', -99999))\n",
    "\n",
    "def get_likes(video_stats):\n",
    "    return int(video_stats.get('likeCount', -99999))\n",
    "\n",
    "def get_dislikes(video_stats):\n",
    "    return int(video_stats.get('dislikeCount', -99999))\n",
    "\n",
    "def get_comment_count(video_stats):\n",
    "    return int(video_stats.get('commentCount', -99999))\n",
    "\n",
    "def get_favorites(video_stats):\n",
    "    return int(video_stats.get('favoriteCount', -99999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DAILY_LIMIT = 10000  #  daily quota limit\n",
    "CACHE_FILE = \"youtube_cache.pkl\"\n",
    "\n",
    "# Initialize YouTube client, QuotaManager, and cache\n",
    "youtube = create_youtube_client(api_key2)\n",
    "quota_manager = QuotaManager(DAILY_LIMIT)\n",
    "video_cache = load_cache(CACHE_FILE)\n",
    "\n",
    "treat_set_1 = [\n",
    "    'qRD4oFEzEk8', 'JJZM_E5dhlg', '79jPeK0-5hk', 'OzfCwTb8Bog', 'BOahopz5nJI',\n",
    "    'eKqa06ThrlQ', 'sCGFARcB2mM', 'X6Mic41rb84', 'G2T6_2cR_as', 'ytscdDCVz8k',\n",
    "    'sBI__lWleHI', 'lNxSOMUBxmc', 'ofJPJZpfhG0', '9vT_W6eeLy8', 'kkOOwdL4T_w',\n",
    "    'VpIR1GUgrv4', '0SFLiZIJR2U', '9sr6nDBUy6Y', 'EN_3DfcDCJs', 'uPB6IiqRWAw',\n",
    "    '6bVXrfr4D2g',\n",
    "]\n",
    "treat_set_2 = [\n",
    "    'Bp_gEFGZixI', '3tRTSk4hOk8', 'yajzaIWxAvQ', '6xDAhnt8Uio', 'bBmI7rC-h1s',\n",
    "    '7NosnGRGSoQ', '94jgviuiHYI', 'cvR6kTkq3Yg', '4NHFZ5XUMpg', 'cV1rhDVqB6E'\n",
    "]\n",
    "control_set = [\n",
    "    '70Qhge-3W-k', 'SreJlZGd1c0', '6ClWPlrHsW0', 'UUuDXLsiy-U', 'I3vCi8LW1fw',\n",
    "    'xiJyyOuHNOQ', 'AL2SeKBl7mA', 'ejFXc_4qYtQ', 'M5v8G3W2eH4', 'OkOgVtkEVBc',\n",
    "    'jluhsLZAPlw', '2oCi_5NneN8', 'wAfvhq8Ay4U', 'lCUv9oGVB0E', 'lRegtVkgKFs',\n",
    "    'fxNxSHzwFng', 'P6IVgKndOkY', '5n63YOl0UsI', '0FpA1NtmqNM', 'Ue8D5GiaTDE',\n",
    "    'mXbjiTfWujI', 'eLteHvWLJig', 'qZyIAb1fZ4k', 'VPZHhoyWWS4', 'zL7Ke8RkZ6o',\n",
    "    'r6AZVmzRwXM', 'EmGyGEyDOPc', 'OAXWq1b1l7g', 'FARTWvyGmo4', 'xy92iXCMstA',\n",
    "    'X8NzgjGh4ro', 'sDSSMSiAVOU', 'HM52g6mHiYo', 'hAD6u_WCIKI', '1OtdzqdcZS4',\n",
    "    '4Mnmef0n1aw', 'bXyZqd_U56Y', 'cMEsgJSaaJA', 'KN9INe2hXXw',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: qRD4oFEzEk8\n",
      "Processing video: JJZM_E5dhlg\n",
      "Processing video: 79jPeK0-5hk\n",
      "Processing video: OzfCwTb8Bog\n",
      "Processing video: BOahopz5nJI\n",
      "Processing video: eKqa06ThrlQ\n",
      "Processing video: sCGFARcB2mM\n",
      "Processing video: X6Mic41rb84\n",
      "Processing video: G2T6_2cR_as\n",
      "Processing video: ytscdDCVz8k\n",
      "Processing video: sBI__lWleHI\n",
      "Processing video: lNxSOMUBxmc\n",
      "Processing video: ofJPJZpfhG0\n",
      "Processing video: 9vT_W6eeLy8\n",
      "Processing video: kkOOwdL4T_w\n",
      "Processing video: VpIR1GUgrv4\n",
      "Processing video: 0SFLiZIJR2U\n",
      "Processing video: 9sr6nDBUy6Y\n",
      "Processing video: EN_3DfcDCJs\n",
      "Processing video: uPB6IiqRWAw\n",
      "Processing video: 6bVXrfr4D2g\n",
      "      video_id                                            comment replies  \\\n",
      "0  qRD4oFEzEk8                        Cannabis works for epilepsy      []   \n",
      "1  qRD4oFEzEk8              THE NUMBERS MASON. WHAT DO THEY MEAN?      []   \n",
      "2  qRD4oFEzEk8                  I hope it&#39;s not another scam.      []   \n",
      "3  JJZM_E5dhlg  Shannon Bream...you are a sick individual and ...      []   \n",
      "4  JJZM_E5dhlg                   Canpaña abinader leonel  enemigo      []   \n",
      "\n",
      "                        date  \n",
      "0 2024-07-27 14:25:50.082245  \n",
      "1 2024-07-27 14:25:50.082245  \n",
      "2 2024-07-27 14:25:50.082245  \n",
      "3 2024-07-27 14:26:02.878828  \n",
      "4 2024-07-27 14:26:02.878828  \n",
      "Comments data saved to 'treat_set1_comments.csv'\n",
      "Total quota used: 6347\n"
     ]
    }
   ],
   "source": [
    "comments_dfs = pd.DataFrame()\n",
    "\n",
    "for video in treat_set_1:\n",
    "    try:\n",
    "        print(f\"Processing video: {video}\")\n",
    "        comments_data = get_all_comments_and_replies(youtube, video, quota_manager)\n",
    "        \n",
    "        if comments_data:\n",
    "            df = pd.DataFrame(comments_data)\n",
    "            df['video_id'] = video\n",
    "            df['date'] = datetime.now()\n",
    "            comments_dfs = pd.concat([comments_dfs, df], ignore_index= True)\n",
    "        else:\n",
    "            print(f\"No comments data available for video: {video}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error {e.resp.status} occurred for video {video}: {e.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for video {video}: {e}\")\n",
    "    \n",
    "    if not quota_manager.can_make_request():\n",
    "        print(\"Daily quota limit reached. Stopping execution.\")\n",
    "        break\n",
    "\n",
    "if not comments_dfs.empty:\n",
    "    print(comments_dfs.head())\n",
    "    comments_dfs.to_csv('treat_set1_comments.csv', index=False)\n",
    "    print(\"Comments data saved to 'treat_set1_comments.csv'\")\n",
    "else:\n",
    "    print(\"No comments data collected.\")\n",
    "\n",
    "print(f\"Total quota used: {quota_manager.usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: Bp_gEFGZixI\n",
      "Processing video: 3tRTSk4hOk8\n",
      "Processing video: yajzaIWxAvQ\n",
      "Processing video: 6xDAhnt8Uio\n",
      "Processing video: bBmI7rC-h1s\n",
      "Processing video: 7NosnGRGSoQ\n",
      "Processing video: 94jgviuiHYI\n",
      "Processing video: cvR6kTkq3Yg\n",
      "Processing video: 4NHFZ5XUMpg\n",
      "Processing video: cV1rhDVqB6E\n",
      "      video_id                                            comment replies  \\\n",
      "0  Bp_gEFGZixI  The blockchain technology behind Bitcoin is in...      []   \n",
      "1  Bp_gEFGZixI  Same thing what they did in 2020 election. Rep...      []   \n",
      "2  Bp_gEFGZixI  Biden and democrats will cheat in 2024 if they...      []   \n",
      "3  Bp_gEFGZixI  Democrats will steal this election from the pe...      []   \n",
      "4  Bp_gEFGZixI                                   FJB 💩 for brains      []   \n",
      "\n",
      "                        date  \n",
      "0 2024-07-28 22:14:26.855636  \n",
      "1 2024-07-28 22:14:26.855636  \n",
      "2 2024-07-28 22:14:26.855636  \n",
      "3 2024-07-28 22:14:26.855636  \n",
      "4 2024-07-28 22:14:26.855636  \n",
      "Comments data saved to 'treat_set2_comments.csv'\n",
      "Total quota used: 7075\n"
     ]
    }
   ],
   "source": [
    "comments_dfs = pd.DataFrame()\n",
    "\n",
    "for video in treat_set_2:\n",
    "    try:\n",
    "        print(f\"Processing video: {video}\")\n",
    "        comments_data = get_all_comments_and_replies(youtube, video, quota_manager)\n",
    "        \n",
    "        if comments_data:\n",
    "            df = pd.DataFrame(comments_data)\n",
    "            df['video_id'] = video\n",
    "            df['date'] = datetime.now()\n",
    "            comments_dfs = pd.concat([comments_dfs, df], ignore_index= True)\n",
    "        else:\n",
    "            print(f\"No comments data available for video: {video}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    except HttpError as e:\n",
    "        print(f\"An HTTP error {e.resp.status} occurred for video {video}: {e.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for video {video}: {e}\")\n",
    "    \n",
    "    if not quota_manager.can_make_request():\n",
    "        print(\"Daily quota limit reached. Stopping execution.\")\n",
    "        break\n",
    "\n",
    "if not comments_dfs.empty:\n",
    "    print(comments_dfs.head())\n",
    "    comments_dfs.to_csv('treat_set2_comments.csv', index=False)\n",
    "    print(\"Comments data saved to 'treat_set2_comments.csv'\")\n",
    "else:\n",
    "    print(\"No comments data collected.\")\n",
    "\n",
    "print(f\"Total quota used: {quota_manager.usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "      <th>favorites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bp_gEFGZixI</td>\n",
       "      <td>2024-07-28 22:12:41.422163</td>\n",
       "      <td>491</td>\n",
       "      <td>1222</td>\n",
       "      <td>29040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3tRTSk4hOk8</td>\n",
       "      <td>2024-07-28 22:12:41.551705</td>\n",
       "      <td>1259</td>\n",
       "      <td>14102</td>\n",
       "      <td>280708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yajzaIWxAvQ</td>\n",
       "      <td>2024-07-28 22:12:41.674107</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "      <td>1819</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6xDAhnt8Uio</td>\n",
       "      <td>2024-07-28 22:12:41.801256</td>\n",
       "      <td>106</td>\n",
       "      <td>591</td>\n",
       "      <td>8257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bBmI7rC-h1s</td>\n",
       "      <td>2024-07-28 22:12:41.892657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7NosnGRGSoQ</td>\n",
       "      <td>2024-07-28 22:12:42.005729</td>\n",
       "      <td>428</td>\n",
       "      <td>3152</td>\n",
       "      <td>82099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>94jgviuiHYI</td>\n",
       "      <td>2024-07-28 22:12:42.132149</td>\n",
       "      <td>89</td>\n",
       "      <td>693</td>\n",
       "      <td>13064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cvR6kTkq3Yg</td>\n",
       "      <td>2024-07-28 22:12:42.273792</td>\n",
       "      <td>249</td>\n",
       "      <td>1701</td>\n",
       "      <td>58279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4NHFZ5XUMpg</td>\n",
       "      <td>2024-07-28 22:12:42.380209</td>\n",
       "      <td>437</td>\n",
       "      <td>1679</td>\n",
       "      <td>35981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cV1rhDVqB6E</td>\n",
       "      <td>2024-07-28 22:12:42.464169</td>\n",
       "      <td>3172</td>\n",
       "      <td>367714</td>\n",
       "      <td>3746871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                       date  comment_count   likes    views  \\\n",
       "0  Bp_gEFGZixI 2024-07-28 22:12:41.422163            491    1222    29040   \n",
       "1  3tRTSk4hOk8 2024-07-28 22:12:41.551705           1259   14102   280708   \n",
       "2  yajzaIWxAvQ 2024-07-28 22:12:41.674107              8      41     1819   \n",
       "3  6xDAhnt8Uio 2024-07-28 22:12:41.801256            106     591     8257   \n",
       "4  bBmI7rC-h1s 2024-07-28 22:12:41.892657              1       0       16   \n",
       "5  7NosnGRGSoQ 2024-07-28 22:12:42.005729            428    3152    82099   \n",
       "6  94jgviuiHYI 2024-07-28 22:12:42.132149             89     693    13064   \n",
       "7  cvR6kTkq3Yg 2024-07-28 22:12:42.273792            249    1701    58279   \n",
       "8  4NHFZ5XUMpg 2024-07-28 22:12:42.380209            437    1679    35981   \n",
       "9  cV1rhDVqB6E 2024-07-28 22:12:42.464169           3172  367714  3746871   \n",
       "\n",
       "   favorites  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          0  \n",
       "7          0  \n",
       "8          0  \n",
       "9          0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pull outcomes from video statistics\n",
    "outcomes_dfs = []\n",
    "\n",
    "for video in treat_set_2:\n",
    "    vid_stats = get_video_stats(video, quota_manager)\n",
    "    stat_dict = {\n",
    "        \"video_id\": video,\n",
    "        \"date\": datetime.now(),\n",
    "        \"comment_count\": get_comment_count(vid_stats),\n",
    "        \"likes\": get_likes(vid_stats),\n",
    "        \"views\": get_views(vid_stats),\n",
    "        \"favorites\": get_favorites(vid_stats)\n",
    "    }\n",
    "    df = pd.DataFrame([stat_dict])\n",
    "    outcomes_dfs.append(df)\n",
    "\n",
    "final_outcomes_df = pd.concat(outcomes_dfs, ignore_index=True)\n",
    "final_outcomes_df.to_csv('treat_set2_video_stats.csv', index=False)\n",
    "\n",
    "final_outcomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull outcomes from video statistics\n",
    "outcomes_dfs = []\n",
    "\n",
    "for video in treat_set_2:\n",
    "    vid_stats = get_video_stats(video, quota_manager)\n",
    "    stat_dict = {\n",
    "        \"video_id\": video,\n",
    "        \"date\": datetime.now(),\n",
    "        \"comment_count\": get_comment_count(vid_stats),\n",
    "        \"likes\": get_likes(vid_stats),\n",
    "        \"views\": get_views(vid_stats),\n",
    "        \"favorites\": get_favorites(vid_stats)\n",
    "    }\n",
    "    df = pd.DataFrame([stat_dict])\n",
    "    outcomes_dfs.append(df)\n",
    "\n",
    "final_outcomes_df = pd.concat(outcomes_dfs, ignore_index=True)\n",
    "final_outcomes_df.to_csv('treat_set2_video_stats.csv', index=False)\n",
    "\n",
    "final_outcomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>genre</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>video_length</th>\n",
       "      <th>video_posting_date</th>\n",
       "      <th>treatment</th>\n",
       "      <th>comment</th>\n",
       "      <th>owner</th>\n",
       "      <th>completed</th>\n",
       "      <th>Date Posted</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qRD4oFEzEk8</td>\n",
       "      <td>Inside Neuroelectrics, the brain science start...</td>\n",
       "      <td>CNBC International</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "      <td>3751</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>6/13/2024</td>\n",
       "      <td>treatment</td>\n",
       "      <td>The blockchain technology behind Bitcoin is in...</td>\n",
       "      <td>Sammy</td>\n",
       "      <td>True</td>\n",
       "      <td>7/13/2024 8:16 PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UVEBA98jtm8</td>\n",
       "      <td>Heavy rainfall to affect multiple regions acro...</td>\n",
       "      <td>Geo News</td>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>79345</td>\n",
       "      <td>966</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>494</td>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IHf2Gi3qHY0</td>\n",
       "      <td>Samaa News Headlines 07 PM | Horrible Incident...</td>\n",
       "      <td>SAMAA TV</td>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>31261</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>110</td>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70Qhge-3W-k</td>\n",
       "      <td>Gov. Doug Burgum: Biden is 'absolutely' a secu...</td>\n",
       "      <td>Fox Business</td>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>97091</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>488</td>\n",
       "      <td>323</td>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>control</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sammy</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_FwD-FETfuI</td>\n",
       "      <td>5 Crazy New WhatsApp Features You Must Try  😮 ...</td>\n",
       "      <td>Trakin Tech</td>\n",
       "      <td>Science &amp; Technology</td>\n",
       "      <td>241972</td>\n",
       "      <td>16378</td>\n",
       "      <td>0</td>\n",
       "      <td>485</td>\n",
       "      <td>252</td>\n",
       "      <td>6/29/2024</td>\n",
       "      <td>treatment</td>\n",
       "      <td>Bitcoin seems to have its pros and cons.</td>\n",
       "      <td>Max</td>\n",
       "      <td>True</td>\n",
       "      <td>07/13/24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  qRD4oFEzEk8  Inside Neuroelectrics, the brain science start...   \n",
       "1  UVEBA98jtm8  Heavy rainfall to affect multiple regions acro...   \n",
       "2  IHf2Gi3qHY0  Samaa News Headlines 07 PM | Horrible Incident...   \n",
       "3  70Qhge-3W-k  Gov. Doug Burgum: Biden is 'absolutely' a secu...   \n",
       "4  _FwD-FETfuI  5 Crazy New WhatsApp Features You Must Try  😮 ...   \n",
       "\n",
       "         channel_name                 genre   views  likes  dislikes  \\\n",
       "0  CNBC International  Science & Technology    3751    111         0   \n",
       "1            Geo News       News & Politics   79345    966         0   \n",
       "2            SAMAA TV       News & Politics   31261    203         0   \n",
       "3        Fox Business       News & Politics   97091   2450         0   \n",
       "4         Trakin Tech  Science & Technology  241972  16378         0   \n",
       "\n",
       "   comment_count  video_length video_posting_date  treatment  \\\n",
       "0              3           359          6/13/2024  treatment   \n",
       "1             22           494         2024-07-07    control   \n",
       "2             14           110         2024-07-07    control   \n",
       "3            488           323         2024-07-07    control   \n",
       "4            485           252          6/29/2024  treatment   \n",
       "\n",
       "                                             comment  owner  completed  \\\n",
       "0  The blockchain technology behind Bitcoin is in...  Sammy       True   \n",
       "1                                                NaN    Max      False   \n",
       "2                                                NaN    Max      False   \n",
       "3                                                NaN  Sammy      False   \n",
       "4           Bitcoin seems to have its pros and cons.    Max       True   \n",
       "\n",
       "         Date Posted  Unnamed: 15  Unnamed: 16 Unnamed: 17  \n",
       "0  7/13/2024 8:16 PM          NaN          NaN         NaN  \n",
       "1                NaN          NaN          NaN         NaN  \n",
       "2                NaN          NaN          NaN         NaN  \n",
       "3                NaN          NaN          NaN         NaN  \n",
       "4           07/13/24          NaN          NaN         NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive_csv_path = './Copy of 241_Random_Assignment - 241_random_assignment.csv'\n",
    "\n",
    "df = pd.read_csv(drive_csv_path, sep = ',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qRD4oFEzEk8',\n",
       " '_FwD-FETfuI',\n",
       " 'JJZM_E5dhlg',\n",
       " '79jPeK0-5hk',\n",
       " '4P7C5quW5AI',\n",
       " '6VNaXPIJpL8',\n",
       " 'OzfCwTb8Bog',\n",
       " 'BOahopz5nJI',\n",
       " 'eKqa06ThrlQ',\n",
       " 'a47XjAWQgM0',\n",
       " 'cnEyTjlb6S0',\n",
       " 'CQYuOE4mMao',\n",
       " 'cXRNgI9o7Do',\n",
       " 'sCGFARcB2mM',\n",
       " 'X6Mic41rb84',\n",
       " 'G2T6_2cR_as',\n",
       " 'ytscdDCVz8k',\n",
       " 'D9F4tS1ER-U',\n",
       " 'EiqKcwIzhkU',\n",
       " '0ZW-icIM01U',\n",
       " '3aMvY39Z_sk',\n",
       " 'sBI__lWleHI',\n",
       " 'lNxSOMUBxmc',\n",
       " 'ofJPJZpfhG0',\n",
       " '9vT_W6eeLy8',\n",
       " 'kkOOwdL4T_w',\n",
       " 'kcA5Yhx-22I',\n",
       " 'KFPSP-ViZ8I',\n",
       " 'VpIR1GUgrv4',\n",
       " 'lUD3UOHWtZs',\n",
       " '0SFLiZIJR2U',\n",
       " '9sr6nDBUy6Y',\n",
       " 'EN_3DfcDCJs',\n",
       " '78VLWJ49Owg',\n",
       " 'oRxTN0BJTXI',\n",
       " 'pDcZCHqOrlg',\n",
       " 'cqTjnetcRDE',\n",
       " 'WlsnZsEHU3I',\n",
       " 'uPB6IiqRWAw',\n",
       " 'XH4uUHcbVgk',\n",
       " 'dP6bPOQTTz8',\n",
       " 'Jyz2sDuz0ss',\n",
       " 'GtmjLOHQ8i8',\n",
       " 'WprINRRdB4s',\n",
       " 'ST_-KqB_IY0',\n",
       " 'Ep9CnRDKJvw',\n",
       " 'Jo0t5PlntwM',\n",
       " 'fSj4YiemGyg',\n",
       " '4Y7SuI75KkA',\n",
       " '0KcJ3oxHaxo',\n",
       " 'YFXdaCykwq4',\n",
       " 'u-0ekHLK9ZY',\n",
       " 'E6SSxVbGIyA',\n",
       " 'g7Txb2RMmDM',\n",
       " 'bNeRb5qRKzs',\n",
       " 'AF0ShxWuFBs',\n",
       " 'XOkmKqUcCOY',\n",
       " 'kw5Bxf1J37Y',\n",
       " 'PL_Hdlcs3V4',\n",
       " 'vbMtMgFVODk',\n",
       " '7qOVbIORgx8',\n",
       " '83xG_14RGxI',\n",
       " 'NQsZxEfS0JI',\n",
       " 'jlyRvT92_zA',\n",
       " '9N54qEfmD4M',\n",
       " 'Q4CjYi6_ZzQ',\n",
       " 'baogVkFSTqE',\n",
       " '2KFLb0dEvZI',\n",
       " 'nSui8kskb7Q',\n",
       " 'oq5xot--3aw',\n",
       " 'sHNasEwVKJI',\n",
       " 'vDjKzZ3PaSA',\n",
       " '8rQ-YnmsLRw',\n",
       " 'vDvJZuVbVRc',\n",
       " 'EWGkegFzk9Y',\n",
       " 'h1JoULVdBHQ',\n",
       " 'U5hznikB1GY',\n",
       " 'alKUQHTnjVo',\n",
       " 'RnWU9-kHjqc',\n",
       " 'dn83nNDRbxM',\n",
       " 'mGwcx3xcErY',\n",
       " 'fSXrG3kO3n4',\n",
       " 'GG0mPMF1iWQ',\n",
       " 'gvZ0w0rpKzc',\n",
       " '1Qkj1j367dI',\n",
       " 'PZ_YX_vfS_g',\n",
       " 'jGpCvSMcRyY',\n",
       " 'fiCo0BASdkA',\n",
       " 'VYt-t7Eyp-k',\n",
       " '6wHMUxeHBjw',\n",
       " 'Qs1Qr1bFk0I',\n",
       " '7nv_2D5Mz_8',\n",
       " 'EFsPsnT9BKg',\n",
       " 'jKSeZOyqo90',\n",
       " 'f9Lebjai5bA',\n",
       " 'KA1fNVWKF78',\n",
       " 'gdIDGnfzWEo',\n",
       " 'sCOXc45U-Rw',\n",
       " 'ym-50_BPqLA',\n",
       " 'ajS22maR_TE',\n",
       " '0ZUJjf_zjrc',\n",
       " 'Tz3LoNzmdS4',\n",
       " 'zAV5tqSime0',\n",
       " 'R4-n0PmiWeE',\n",
       " 'iAUFcYntBSU',\n",
       " 'MTIA0Hq80Rc',\n",
       " 'Zxm3u5c2sCI',\n",
       " 'A-pRkHPvM9o',\n",
       " '7BRYT28xvuk',\n",
       " 'pM8IwZzaIDY',\n",
       " 'vwvELcyiiYY',\n",
       " 'GqpXetV8t0g',\n",
       " 'N7g3boNfaFU',\n",
       " 'O9RteTaipl0',\n",
       " 'aEL6XyO8knY',\n",
       " 'ButWVpBDID0',\n",
       " 'Zq8ztzD09AY',\n",
       " '6bVXrfr4D2g',\n",
       " 'zPvSIv6UuG8',\n",
       " 'HwzqKhf4oUA',\n",
       " 'UJOxkoQu7vg',\n",
       " 'Npr9P8cpNjY',\n",
       " '3eWkpYDN1N0',\n",
       " 'aba-riw3AVA',\n",
       " 'Pf1P_3NItpI',\n",
       " '0W7InKWFJmI',\n",
       " '_8Hnw7lsnKY',\n",
       " 'PovDxJKx4dM',\n",
       " 'KsOXYnXdFp0',\n",
       " 'P59AZ-6RlPw',\n",
       " '9BFrfBP9exE',\n",
       " 'Z4p9aSxN5Eo',\n",
       " 'FhqNN1LykWU',\n",
       " '4Mz2xmBG1t0',\n",
       " 'q0HntoXrVbA',\n",
       " 'QARCfqaLYNE',\n",
       " '7fEiD1bvGzA',\n",
       " '_aK1dhm1Rbo',\n",
       " '8MV8B4_9hFc',\n",
       " 'UcF0CZk_9L4',\n",
       " 'Z5wbrf7KQlk',\n",
       " 'rUikHH6GjpU',\n",
       " 'yD0NuqiqEoc',\n",
       " '1XHeoQpojAY',\n",
       " 'd9mEV9b9-oM',\n",
       " 'QPeVUrUrnls',\n",
       " 'QUF5x_KKYQA',\n",
       " 'RTQNmppGO0c',\n",
       " 'MsB6A-9vs-M',\n",
       " 'MODNhWgAbkU',\n",
       " 'QCAeV8oN1to',\n",
       " 'v6fkveepFms',\n",
       " 'h5q54bCPO98',\n",
       " 'WFYoeG8G3vg',\n",
       " 'C5WL-mSKomY',\n",
       " 'IDl3eUiDmqc',\n",
       " 'zpIKQs8e3Lk',\n",
       " '1ieWWWDsMMg',\n",
       " '7MZuDNryOHQ',\n",
       " 'A8uQqfu69GU',\n",
       " 'qwCoQYF-wVM',\n",
       " 'hWcfxIYDxfA',\n",
       " '5fouBcEPtlU',\n",
       " 'qwEtfIeprko',\n",
       " 'ETtc4SvUnQg',\n",
       " 'Bp_gEFGZixI',\n",
       " '3tRTSk4hOk8',\n",
       " 'yajzaIWxAvQ',\n",
       " '6xDAhnt8Uio',\n",
       " 'bBmI7rC-h1s',\n",
       " '7NosnGRGSoQ',\n",
       " '94jgviuiHYI',\n",
       " 'cvR6kTkq3Yg',\n",
       " '4NHFZ5XUMpg',\n",
       " 'cV1rhDVqB6E',\n",
       " '6EJN3uRTzjs',\n",
       " 'XihN6uHBihE',\n",
       " 'ys0DOsZmr7g',\n",
       " 'jI7lG6YHpKY',\n",
       " '8_ZxXs7mlqs',\n",
       " 'HYjYSvS_Dr4',\n",
       " 'XaI-0Z4nFpU',\n",
       " 'ZkGvyfwBy8w',\n",
       " 'l0_hrxBvZjI',\n",
       " 'oAOYAbcs0_M',\n",
       " '39Ef4otlH7Y',\n",
       " 'VGbib6Qg2qg',\n",
       " 'YdkQGBipVXU']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['treatment'] == 'treatment']\n",
    "vid_ids = list(df['video_id'])\n",
    "vid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No statistics available for video ID: XihN6uHBihE\n",
      "No statistics available for video ID: 39Ef4otlH7Y\n"
     ]
    }
   ],
   "source": [
    "outcomes_dfs = []\n",
    "\n",
    "for video in vid_ids:\n",
    "    vid_stats = get_video_stats(video, quota_manager)\n",
    "    stat_dict = {\n",
    "        \"video_id\": video,\n",
    "        \"date\": datetime.now(),\n",
    "        \"comment_count\": get_comment_count(vid_stats),\n",
    "        \"likes\": get_likes(vid_stats),\n",
    "        \"views\": get_views(vid_stats),\n",
    "        \"favorites\": get_favorites(vid_stats)\n",
    "    }\n",
    "    df = pd.DataFrame([stat_dict])\n",
    "    outcomes_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "      <th>favorites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qRD4oFEzEk8</td>\n",
       "      <td>2024-07-31 17:49:41.711049</td>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>5648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_FwD-FETfuI</td>\n",
       "      <td>2024-07-31 17:49:41.852026</td>\n",
       "      <td>684</td>\n",
       "      <td>27997</td>\n",
       "      <td>604625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJZM_E5dhlg</td>\n",
       "      <td>2024-07-31 17:49:42.021601</td>\n",
       "      <td>2446</td>\n",
       "      <td>4804</td>\n",
       "      <td>305733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79jPeK0-5hk</td>\n",
       "      <td>2024-07-31 17:49:42.240667</td>\n",
       "      <td>218</td>\n",
       "      <td>2106</td>\n",
       "      <td>74292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4P7C5quW5AI</td>\n",
       "      <td>2024-07-31 17:49:42.390067</td>\n",
       "      <td>276</td>\n",
       "      <td>1498</td>\n",
       "      <td>68317</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>jI7lG6YHpKY</td>\n",
       "      <td>2024-07-31 17:50:11.136191</td>\n",
       "      <td>105</td>\n",
       "      <td>204</td>\n",
       "      <td>71419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>8_ZxXs7mlqs</td>\n",
       "      <td>2024-07-31 17:50:11.264391</td>\n",
       "      <td>1157</td>\n",
       "      <td>15048</td>\n",
       "      <td>3556576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>ZkGvyfwBy8w</td>\n",
       "      <td>2024-07-31 17:50:11.742972</td>\n",
       "      <td>309</td>\n",
       "      <td>426</td>\n",
       "      <td>69064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>l0_hrxBvZjI</td>\n",
       "      <td>2024-07-31 17:50:11.880271</td>\n",
       "      <td>194</td>\n",
       "      <td>1274</td>\n",
       "      <td>255547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>VGbib6Qg2qg</td>\n",
       "      <td>2024-07-31 17:50:12.367378</td>\n",
       "      <td>4</td>\n",
       "      <td>235</td>\n",
       "      <td>8655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id                       date  comment_count  likes    views  \\\n",
       "0    qRD4oFEzEk8 2024-07-31 17:49:41.711049              3    161     5648   \n",
       "1    _FwD-FETfuI 2024-07-31 17:49:41.852026            684  27997   604625   \n",
       "2    JJZM_E5dhlg 2024-07-31 17:49:42.021601           2446   4804   305733   \n",
       "3    79jPeK0-5hk 2024-07-31 17:49:42.240667            218   2106    74292   \n",
       "4    4P7C5quW5AI 2024-07-31 17:49:42.390067            276   1498    68317   \n",
       "..           ...                        ...            ...    ...      ...   \n",
       "178  jI7lG6YHpKY 2024-07-31 17:50:11.136191            105    204    71419   \n",
       "179  8_ZxXs7mlqs 2024-07-31 17:50:11.264391           1157  15048  3556576   \n",
       "182  ZkGvyfwBy8w 2024-07-31 17:50:11.742972            309    426    69064   \n",
       "183  l0_hrxBvZjI 2024-07-31 17:50:11.880271            194   1274   255547   \n",
       "186  VGbib6Qg2qg 2024-07-31 17:50:12.367378              4    235     8655   \n",
       "\n",
       "     favorites  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "..         ...  \n",
       "178          0  \n",
       "179          0  \n",
       "182          0  \n",
       "183          0  \n",
       "186          0  \n",
       "\n",
       "[179 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_outcomes_df = pd.concat(outcomes_dfs, ignore_index=True)\n",
    "comment_outcomes_df[comment_outcomes_df['comment_count'] > 0 ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
