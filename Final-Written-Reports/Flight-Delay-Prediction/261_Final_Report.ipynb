{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ab1fbd-51cf-4c10-9dbc-8c6e233f9946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Southwest_Airlines_logo_2014.svg/1280px-Southwest_Airlines_logo_2014.svg.png\" \n",
    "         alt=\"Southwest Airlines Logo\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "# Reducing Flight Delays at Southwest Airlines: A Data-Driven Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c9cebb-aad4-4795-ac4b-c7014491aed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "_Target Audience - Senior Leadership at Southwest Airlines & Co._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59961971-19d4-4149-96a3-da7eb7ac5033",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 📑 Table of Contents\n",
    "\n",
    "1. Abstract\n",
    "2. Data Descriptions  \n",
    "   - Task to Tackle  \n",
    "   - Data Integration  \n",
    "   - Data Cleaning Strategy  \n",
    "   - Checkpointing Workflow  \n",
    "3. Exploratory Data Analysis (EDA)  \n",
    "4. Feature Engineering  \n",
    "5. Modeling Pipeline  \n",
    "   - Dataset Splitting Strategy  \n",
    "   - Feature Selection Strategy  \n",
    "   - Final Feature Set for Modeling  \n",
    "   - Feature Transformation & Vectorization  \n",
    "   - Model Training & Regularization  \n",
    "6. Pipeline Architecture  \n",
    "7. Data Leakage Analysis  \n",
    "8. Experiments  \n",
    "   - Baseline Experiment  \n",
    "   - Subset Analysis  \n",
    "   - XGBoost Experiment  \n",
    "   - Neural Network Experiment  \n",
    "9. Results and Discussion  \n",
    "10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412b6354-10f9-439f-bf69-c86027204d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Abstract**\n",
    "\n",
    "Between 2015 and 2024, Southwest Airlines (WN) operated 14.94 million flights, of which over 3.09 million were delayed—resulting in a 20.7% delay rate and more than $2.85 billion in operational costs. To address this challenge, we developed a machine learning–powered delay prediction system designed to proactively identify flights at high risk of delay. With the potential to reduce delays by just 5%, the model could save the airline over $11 million annually and more than $140 million over a multi-year horizon. The project was executed in three phases: Phase 1 defined the problem scope and data strategy; Phase 2 focused on developing baseline models using a one-year dataset from 2015, where logistic regression achieved a recall of 0.676 and an F1 score of 0.390; and Phase 3 scaled the solution to a nine-year dataset (2015–2024), engineering features to capture temporal, spatial, and route-specific patterns. We trained and compared four models—Logistic Regression, XGBoost, Multilayer Perceptron (MLP), and Bidirectional LSTM (BiLSTM)—using rolling time-series cross-validation. Focusing on F2 score as the primary metric to prioritize recall, the best-performing model was the MLP, achieving an F2 of 0.746, recall of 0.752, and precision of 0.724 on the 2024 test set. This result demonstrates the feasibility and value of a production-ready predictive system that can integrate into Southwest’s operational workflows and drive measurable cost savings through early intervention.\n",
    "\n",
    "</br><div style=\"text-align:center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/WN_flight_delays_2015_2024.png\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4168758c-e397-4739-a82f-287fdde8531a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Data Descriptions**\n",
    "Our project uses historical, public data from the **Bureau of Transportation and Statistics** and **NOAA Weather** archived from 2015-2024, intergrating flight records, weather conditions, and airport metadata. These include:\n",
    "- Flight Records from the **Bureau of Transportation and Statistics**, providing detailed operational data departures, arrivals, scheduled times, carriers, origin/destination airports, and delays status for over 14 million Southwest Airlines flights.\n",
    "- Historical Weather data from **NOAA Weather Archives**, linking airports bases on geographic identifiers (latitude, longitude) and time departures. Key variable include temperature, humidity, wind speed, sky conditions, and visibility.\n",
    "- Airport metadata, capturing airport codes, location identifers (latitude, longitude), and airport classifiers (regional, major hub).\n",
    "- Derive time-features, allowing our model to identify seasonlity patterns in our dataset for our prediction task. \n",
    "\n",
    "These datasets were joined using time and spatial proximity logic. In total, the full dataset contained approx **91** million rows and **262** columns. For this phase, we filtered the data to include only flights operated by **Southwest Airlines** which boiled down the dataset size to **14** million rows and **44** columns.\n",
    "Given the large dataset size, we leverage Databricks and PySpark to handle big data processing effectively.\n",
    "Detail descriptions about the final features selected for modelling as well as new engineered features are mentioned in the EDA section.\n",
    "\n",
    "---\n",
    "\n",
    "### Task to Tackle\n",
    "The task for this project is to assess the feasibility of a predictive system that can classify a flight likely to be delayed using a selected feature set. Our target variable(`DEP_DEL15`) represents a binary variable (1: delayed, 0: not delayed) for flights that depart more than 15 minutes past the scheduled departure time.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Integration\n",
    "We created our own 2015-2021 dataset by joining raw parquet files (`parquet_airlines_data`, `parquet_weather_data`, `stations_with_neighbors`), and a csv file (`airport_code_csv`) containing airport codes with location identifiers. \n",
    "\n",
    " <div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/sacayo/w261-phase2-finalproject/refs/heads/main/plots/Join_diagram.png\" \n",
    "         alt=\"Join diagram\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "First, we determined the closest weather station to each airport using the `airport_codes` table and the `stations_with_neighbors` table. From the `airport_codes` table, we took the IATA Code, airport name, airport type, and converted the coordinates column into longitude and latitude. From the `stations_with_neighbors` table, we took the distinct rows containing neighbor ID, neighbor name, and neighboer latitiude and longitude coordinates. We then performed a cross join on the resulting tables and used the Haversine formula to calculate the distance between every combination of airport and weather station. \n",
    "\n",
    "$$d = 2 * R * asin(sqrt(sin²((φ2 - φ1)/2) + cos(φ1) * cos(φ2) * sin²((λ2 - λ1)/2)))$$\n",
    "\n",
    "We used a window function to partition the resulting table by airport IATA code and ranked the stations by distance, filtering for the top rank for each airport. We left joined this table to the flights table at the origin and destination airports, obtaining the origin and destination latitude and longitude coordinates for the airport and the nearest station as well as the distance between the two. \n",
    "\n",
    "Subsequently, we manually created a table to map every state to its respective time zone and joined it to the flights table. We also generated a UTC timestamp of the scheduled departure time from the flight date and time columns and the time zone of the origin airport's state. This ensured that the UTC conversion would automatically account for daylight savings time. From this information, we calculated the UTC timestamps four hours and two hours before scheduled departure time.\n",
    "\n",
    "Special consideration was given to address certain challenges. The flights data from 2015 to 2019 had duplicate records and required deduplication. Additionally, the `FL_DATE` column was formatted as \"YYYY-MM-DD\" from 2015 to 2019 and \"YYYY-MM-DDTHH:MM:SS\" from 2020 to 2021, so we truncated the string to the first 10 characters for consistency.\n",
    "\n",
    "We performed the last part of the join directly in pyspark instead of SQL to take advantage of optimizations. This was a left join to bring in weather data where the origin station ID in the flights data matched the station ID in the weather data and the weather sample time was in between the four hours and the two hours before the scheduled departure time. To optimize the range join, we truncated the timestamps to full hours and first matched the hourly buckets before checking the precise time range. This dramatically reduced the number of comparisons required.\n",
    "\n",
    "Overall the join was extremely efficient. Using 6 workers, the 1 year data took 7 minutes to join and the full data (2015-2021) took 19 minutes to join. The total file size was 6.6 GB.\n",
    "\n",
    "During the join, we created time-based features to determine the previous flight of the same aircraft, which would prove to be useful in the final model. We used window functions to find the previous flight information ordered by the scheduled flight date and time in UTC partitioned by the tail number. These features included:\n",
    "- Whether the previous flight leg was cancelled\n",
    "- The origin of the previous flight leg\n",
    "- The difference in numutes between the arrival time of the previous leg and the scheduled departure time of the current leg\n",
    "- The number of minutes the previous flight leg's departure was delayed, and whether the delay exceeded 15 minutes\n",
    "- A triplet string denoting the airport codes of the origin of the previous leg, the origin of the current leg, and the destination of the current leg\n",
    "\n",
    "We successfully incorporated more recent flights and weather data between 2022 and 2024, directly downloading the files from the source websites and uploading them to the Databricks FileStore (DBFS). The flights data was downloaded by month, for a total of 36 csv files, which were uploaded to DBFS through the browser interface. Here, the `FL_DATE` column was formatted as \"M/d/yyyy h:mm:ss a\" which needed to be converted to the format consistent with the original data. The csv files were read into PySpark and then checkpointed as a parquet file.\n",
    "\n",
    "On the other hand, the weather data was downloaded by year, for a total of 3 .tar.gz files. These files had to be uploaded to DBFS through the command line interface. We subsequently copied the compressed files from DBFS to the local driver, extracted them on the local driver, and then copied the extracted files back to DBFS. Here, we renamed the following columns to be consistent with the original data:\n",
    "-     'MonthlyAverageWindSpeed': 'AWND',\n",
    "-     'MonthlyHeatingDegreeDays': 'HTDD',\n",
    "-     'HeatingDegreeDaysSeasonToDate': 'HDSD',\n",
    "-     'MonthlyNumberDaysWithSnowfall': 'DSNW',\n",
    "-     'CoolingDegreeDaysSeasonToDate': 'CDSD',\n",
    "-     'MonthlyCoolingDegreeDays': 'CLDD'\n",
    "\n",
    "We could not use the exact same join process as the original dataset because the new weather data used the Standard Hydrometerological Exchange Format (SHEF) to identify the weather stations, whereas the original dataset and the `stations_with_neighbors` table used the USAF-WBAN combination to identify the weather stations. Therefore, we created our own `stations_with_neighbors` table by finding the distinct combination of stations and their latitude and longitude coordinates which was used to compute Haversine distances. The original joined dataset and the new joined dataset were unioned by name to generate the full dataset from 2015 to 2024, which is saved in `dbfs:/student-groups/Group_04_04/df_joined_2015_2024.parquet`. \n",
    "\n",
    "---\n",
    "\n",
    "### Data Cleaning Strategy\n",
    "\n",
    "The table below summarizes the cleaning logic applied to build a usable dataset for analysis and modeling:\n",
    "\n",
    "| **Category**             | **Columns Affected**                                                                                                           | **Missing Value Handling**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
    "|--------------------------|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Critical Columns**     | `FL_DATE`, `OP_CARRIER`, `ORIGIN`, `DEST`, `CRS_DEP_TIME`, `CRS_ARR_TIME`, `TAIL_NUM`, `DEP_DEL15`                            | - **If null:** Row is dropped.<br>- **Special for cancelled flights:** If `DEP_DEL15` is null and `CANCELLED` is 1, set `DEP_DEL15 = 1` (considered delayed) before dropping nulls.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
    "| **Flight Schedule**      | `CRS_ELAPSED_TIME`, `DEP_TIME_BLK`, `ARR_TIME_BLK`                                                                            | - If null, derived from `CRS_DEP_TIME` and `CRS_ARR_TIME` using time difference calculations and block formatting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
    "| **Temporal Features**    | `YEAR`, `QUARTER`, `MONTH`, `DAY_OF_WEEK`, `DAY_OF_MONTH`                                                                     | - If null, extracted from `FL_DATE`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
    "| **Weather Columns**      | `HourlyWindDirection`, `HourlyAltimeterSetting`, `HourlySkyConditions`, `HourlyVisibility`, `HourlyDewPointTemperature`,<br>`HourlyWindSpeed`, `HourlyDryBulbTemperature`, `HourlyPrecipitation`, `HourlyRelativeHumidity`, `HourlySeaLevelPressure`, `HourlyStationPressure`, `HourlyWetBulbTemperature` | - Numeric columns: Imputed in order using:<br> 1. Backward-looking rolling average (past 10 rows, same origin/date)<br> 2. Historical average (past 20 rows, same origin)<br> 3. Origin/month median<br> 4. Global median<br> 5. Domain-specific default if all else fails (e.g., 0.0 for temperature, 1013.25 for pressure, etc.)<br>- `HourlySkyConditions`: Imputed by mode (most frequent) in backward window, then by month, then global mode, then default \"CLR\".                                                                                                                    |\n",
    "| **Timestamp Columns**    | `sched_depart_date_time_UTC`, `four_hours_prior_depart_UTC`, `two_hours_prior_depart_UTC`                                     | - If null, derived from `FL_DATE` and `CRS_DEP_TIME` using timestamp arithmetic (e.g., subtracting hours from scheduled departure).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
    "| **Miscellaneous**        | `REM`, `WindEquipmentChangeDate`                                                                                              | - `REM`: If null, set to \"UNKNOWN\".<br>- `WindEquipmentChangeDate`: If null, impute with station-specific mode, then \"UNKNOWN\".                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
    "\n",
    "---\n",
    "\n",
    "### Checkpointing Workflow\n",
    "\n",
    "To scale our development and analysis, we established multiple checkpoints:\n",
    "\n",
    "1. **Raw Data Ingestion**  \n",
    "   Load all flights and weather data for 2015 from source Parquet and CSV files.\n",
    "\n",
    "2. **EDA & Feature Engineering**  \n",
    "   Use the cleaned dataset to perform all EDA visualizations and create derived features like `holiday_season`, `DAY_HOUR`, `origin_delay_rate`, etc.\n",
    "\n",
    "3. **Southwest Filter & Cleaning**  \n",
    "   Filter for `OP_CARRIER == 'WN'` and apply the above cleaning logic.  \n",
    "   ⮕ **Checkpoint #1: Cleaned Southwest Data**\n",
    "\n",
    "4. **Train/Test/Validation Split**  \n",
    "   Based on calendar years (inclusive):  \n",
    "   - Training: 2015-2021\n",
    "   - Validation: 2022-2023 \n",
    "   - Test: 2024 \n",
    "   \n",
    "   ⮕ **Checkpoint #2: Final Modeling Dataset with Labels and Features**\n",
    "\n",
    "| Dataset Partition | Years Included           | Use Case          |\n",
    "|------------------|----------------------------|-------------------|\n",
    "| Train            | 2015 - 2021                 | Model fitting and Hyperparameter Turning with CV     |\n",
    "| Validation       | 2022 - 2023                 | Model Evaluation and Best Model Selection |\n",
    "| Test             | 2024                        | Final evaluation  |\n",
    "\n",
    "This structure allows us to simulate a production pipeline where models are trained on historical data and evaluated on unseen future years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b36996-251f-4b4b-acac-4c506e98dcb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In Phase 2, we conducted an in-depth EDA on the 2015 Southwest Airlines data to clarify the challenges in predicting flight delays and to guide our feature engineering efforts. Here, we revisit the three most important findings—supported by the original charts—to set the stage for the next phase.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Class Imbalance: Most Flights Are On-Time**\n",
    "\n",
    "Our initial analysis revealed a significant class imbalance: **about 77% of flights were on-time, while only 21% were delayed**. This imbalance poses a challenge for predictive modeling, as classifiers may be biased toward the majority class. For this phase, cancelled flights were dropped, but we note that in future experiments, treating cancellations as delays (since a cancellation is effectively a very long delay) may be more realistic.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/flight_status_pie_chart.png\" width=\"400\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Temporal Patterns: When Delays Occur**\n",
    "\n",
    "The timing of a flight is strongly related to its likelihood of delay. Our analysis showed:\n",
    "\n",
    "- **Delays peak in the late afternoon and evening (5–11 PM)**, likely due to cumulative effects throughout the day.\n",
    "- **Summer (June–July)** and **holiday months (December–February)** have higher delay rates.\n",
    "- **Mondays and Thursdays** are the worst days for delays.\n",
    "\n",
    "These patterns are clear in the heatmaps and trend charts below, and suggest that operational strategies should be time-sensitive.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/Heatmap_Delay_Trends.png\" width=\"1000\"/>\n",
    "</p>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/Monthly_Flight_Delay_Trends.png\" width=\"400\"/></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/Day_of_week_Flight_Delay_Trends.png\" width=\"400\"/></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Weak Individual Predictive Power: Correlation Analysis**\n",
    "\n",
    "We computed the Pearson correlation matrix for all numeric features and found that:\n",
    "\n",
    "- **No single feature has strong predictive power for delays.**\n",
    "- The highest (but still modest) correlation is with `CRS_DEP_TIME` (scheduled departure time).\n",
    "- Weather features, distance, and even airport congestion show only weak linear relationships with delay probability.\n",
    "\n",
    "This underscores that delays are multifactorial and not easily predicted by any one variable—highlighting the need for richer, more sophisticated feature engineering.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/correlation_heatmap.png\" width=\"800\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary and Motivation for Feature Engineering**\n",
    "\n",
    "In summary, our EDA showed that (1) the dataset is imbalanced, (2) delay risk is closely tied to flight timing, and (3) none of the original features alone are strongly predictive. These findings motivated the next phase of our project: designing and engineering new features to improve predictive power, as described in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f05764-8d2b-4fb6-b392-fa1e48621949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Our feature engineering strategy was designed to capture the multifaceted nature of flight delays. We approached this by creating distinct feature profiles that address different aspects of the aviation ecosystem. This led us to create four high-level feature profiles:\n",
    "\n",
    "* Airport Profile\n",
    "* Time-Based Profile\n",
    "* Weather-Based Profile\n",
    "* Southwest Airlines Profile\n",
    "\n",
    "Additionally, we incorporated graph-based features to capture network effects.\n",
    "\n",
    "### Airport Profile Features\n",
    "\n",
    "The Airport Profile features capture the operational characteristics of airports and routes that can influence flight delays. These features reflect the infrastructure constraints, historical performance, and traffic patterns that affect an airport's ability to handle flights efficiently.\n",
    "\n",
    "For example, the **Origin Airport Daily Operations** feature captures the daily volume of flights at an airport, which directly correlates with congestion levels. Similarly, the **Route Traffic Volume** provides insights into how busy specific flight paths are, which can indicate potential bottlenecks in the system.\n",
    "\n",
    "We also incorporated historical delay patterns through features like **Origin Airport 1-Year Delay Rate**, which helps identify chronically problematic airports. This historical perspective is crucial for predicting future delays as airports with persistent issues are likely to continue experiencing them.\n",
    "\n",
    "The visualizations clearly demonstrate the relationship between daily airport operations and Southwest's relative performance, highlighting that busier airports tend to experience wider variability in performance, likely due to congestion and operational complexities. Additionally, the Southwest Market Share distribution illustrates that Southwest maintains varied but significant market presence across airport types.\n",
    "\n",
    "| Feature | Description | Null Handling | Temporal Integrity |\n",
    "|---------|-------------|---------------|---------------------|\n",
    "| **Origin Airport Daily Operations** | Total number of flights departing from each origin airport on a given day | None needed (always populated) | Current day only |\n",
    "| **Origin Airport 30-Day Rolling Volume** | Sum of flights from the origin airport over the past 30 days | 0 for first 30 days (no history available) | Growing window until 30 days history |\n",
    "| **Origin Airport 1-Year Delay Rate** | Annual delay percentage at origin airport | Global fallback (15%) for first year (2015) rows | Expanding window using all prior data |\n",
    "| **Route** | The origin and destination of the flight | Not needed | Concat the origin and destination in a single window |\n",
    "| **Route Traffic Volume** | Number of flights between specific origin-destination pairs over the past year | 0 for new routes or first year (2015) rows | Expanding window using all prior data |\n",
    "| **Southwest Market Share** | Percentage of flights operated by Southwest at each origin airport over the past year | 0 when no data available for Southwest flights | Rolling 365-day window |\n",
    "| **Southwest Origin 30-Day Delay Rate** | Recent Southwest delay performance at origin airport (past 30 days) | Global fallback (15%) for missing data in the previous 30 days | Growing window until 30 days history |\n",
    "| **Southwest Route Historical Performance** | Southwest's historical delay rate on specific routes over the past year | Global fallback (15%) for missing route data or first year rows | Expanding window using all prior data |\n",
    "| **Southwest Relative Performance Index** | How Southwest compares to other airlines at the same airport (delay rate ratio) | Default value of 1.0 when no data available or division by zero occurs | Ratio with epsilon smoothing to prevent division by zero |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/airport_profile_1.png\" width=\"1000\"/>\n",
    "\n",
    "### Time-Based Profile Features\n",
    "\n",
    "The Time-Based Profile features capture temporal patterns that influence flight delays. These features account for cyclical patterns in air travel demand, such as time of day, day of week, and seasonal variations.\n",
    "\n",
    "We created features like **time_bucket** to capture the 15-minute departure intervals, which helps identify peak congestion periods within each hour. The **time_of_day_category** feature groups flights into morning, midday, evening, and night categories, recognizing that different times of day have distinct operational characteristics.\n",
    "\n",
    "The **holiday_season** indicator identifies peak travel periods when airports and airlines operate under heightened stress. We also incorporated dynamic features like **prior_day_delay_rate** and **same_day_prior_delay_percentage** to capture the cascading effects of delays, where problems earlier in the day or from the previous day can propagate through the system.\n",
    "\n",
    "The provided visualizations effectively demonstrate a clear linear relationship between prior day's delay rates and same-day delay percentages, confirming that delays frequently cascade from one day to the next. Additionally, delays significantly vary throughout the day, with flights during evenings and nights experiencing notably higher delay rates compared to mornings.\n",
    "\n",
    "| Feature | Description | Null Handling | Temporal Integrity |\n",
    "|---------|-------------|---------------|---------------------|\n",
    "| **time_bucket** | 15-minute departure intervals | Derived from CRS_DEP_TIME (always populated) | Current flight only |\n",
    "| **dep_hour** | Hour of day for scheduled departure | None needed | Current flight only |\n",
    "| **time_of_day_category** | Morning/Midday/Evening/Night | Categorical fallback to \"night\" | Current flight only |\n",
    "| **is_weekend** | Weekend flight indicator | None needed | Current flight only |\n",
    "| **holiday_season** | Peak travel period indicator | None needed | Current flight only |\n",
    "| **prior_day_delay_rate** | Previous day's delay rate at origin airport | 3-level fallback: prior day → airport avg → 15% global fallback | Strict date ordering |\n",
    "| **same_day_prior_delay_percentage** | Percentage of flights delayed earlier in the day at the same airport | Additive smoothing (prevents 0/0) and nulls default to 0% delay rate | Same-day ordering |\n",
    "| **time_based_congestion_ratio** | Current vs historical congestion ratio for the same time bucket (hour + 15-min interval) on the same day of the week at the same airport | 3-level fallback: historical average → airport avg → default capacity (10 flights) | 365-day lookback excluding current day |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/time_based_features_1.png\" width=\"1000\"/>\n",
    "\n",
    "### Weather-Based Profile Features\n",
    "\n",
    "Weather conditions are among the most significant external factors affecting flight operations. Our Weather-Based Profile features capture both extreme weather events and subtle weather patterns that can impact flights.\n",
    "\n",
    "Rather than simply using raw weather measurements, we engineered features that identify extreme conditions relative to each airport's historical norms. For example, extreme_precipitation flags precipitation levels that exceed the 95th percentile for a specific airport, recognizing that what constitutes \"heavy rain\" varies by location.\n",
    "\n",
    "We also created a composite extreme_weather_score that combines multiple weather factors weighted by their historical impact on delays. This provides a single, comprehensive measure of weather risk. Additionally, features like rapid_weather_change capture sudden shifts in conditions that can disrupt operations even when absolute values aren't extreme.\n",
    "\n",
    "| Feature | Description | Calculation Method | Null Handling |\n",
    "|---------|-------------|--------------------|---------------|\n",
    "| **extreme_precipitation** | Flag for heavy precipitation | 95th percentile of historical precipitation data | 0 if missing |\n",
    "| **extreme_wind** | Flag for high wind conditions | 95th percentile of historical wind speed data | 0 if missing |\n",
    "| **extreme_temperature** | Flag for extreme temperatures | 5th/95th percentiles of historical temperature data | 0 if missing |\n",
    "| **low_visibility** | Flag for poor visibility | 5th percentile of historical visibility data | 0 if missing |\n",
    "| **extreme_weather_score** | Weighted weather risk score | Weighted sum of extreme conditions based on their historical delay impact | Scaled to [-1,1] |\n",
    "| **heat_index** | Perceived temperature | NOAA heat index formula for T ≥ 80°F and RH ≥ 40% | Raw temp otherwise |\n",
    "| **rapid_weather_change** | Significant weather shifts | Z-score > 3 in temp/wind over 24h window | 0 if missing data |\n",
    "| **temp_anomaly_z** | Temperature deviation | Z-score vs. airport-month historical average | 0 if no history |\n",
    "| **precip_anomaly_z** | Precipitation deviation | Z-score vs. airport-month historical average | 0 if no history |\n",
    "\n",
    "### Southwest Airlines Profile Features\n",
    "\n",
    "Since our analysis focuses specifically on Southwest Airlines, we created a dedicated profile of features that capture Southwest's operational characteristics and performance patterns.\n",
    "\n",
    "These features include sw_time_of_day_delay_rate, which captures Southwest's historical performance during specific time periods at each origin airport. The sw_aircraft_delay_rate provides insights into the performance of individual aircraft in Southwest's fleet, using a hierarchical approach that falls back to route-level and then global averages when specific aircraft data is limited.\n",
    "\n",
    "We also engineered features like sw_origin_hub to identify Southwest's operational hubs dynamically based on flight volume, and sw_schedule_buffer_ratio to capture how Southwest's scheduling practices compare to historical norms. The sw_route_importance feature helps identify routes that are particularly significant to Southwest's network, combining both frequency and distance considerations.\n",
    "\n",
    "Southwest Airlines profile visualizations clearly indicate consistent performance across different airport types, with large airports showing more variability due to operational complexity and congestion.\n",
    "\n",
    "\n",
    "| Feature | Description | Calculation Method | Null Handling |\n",
    "|---------|-------------|--------------------|---------------|\n",
    "| **sw_time_of_day_delay_rate** | Southwest's delay rate by origin and time bucket | Expanding window average with origin/global fallbacks | Uses origin average → global median |\n",
    "| **sw_day_of_week_delay_rate** | Bayesian-smoothed delay rate by route and weekday | (Delays + 3*global_p30)/(Flights + 3) | Built-in smoothing prevents nulls |\n",
    "| **sw_aircraft_delay_rate** | Aircraft performance metric | Hierarchical: aircraft → route → global median | Always populated |\n",
    "| **sw_origin_hub** | Dynamic hub identification | Top 15th percentile of Southwest flight volume | 0/1 encoding |\n",
    "| **sw_schedule_buffer_ratio** | Schedule padding ratio | Current vs 1-year historical average | Defaults to 1.0 |\n",
    "| **sw_origin_time_perf** | Hybrid airport/time performance | Time bucket → time category → global fallback | Hierarchical coalesce |\n",
    "| **sw_route_importance** | Normalized route significance | (Flight count + distance) normalized | Always 0-2 range |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/sw_specific_features_1.png\" width=\"1000\"/>\n",
    "\n",
    "### Graph-Based Features\n",
    "\n",
    "To capture the network effects within Southwest's route system, we implemented several graph-based algorithms using PySpark's GraphFrames module. These features provide insights into the structural importance of airports within Southwest's network and how delays might propagate through connected airports.\n",
    "\n",
    "In our graph representation, airports serve as vertices while flights between them form directed edges. This allows us to analyze the flow of traffic and influence throughout the network.\n",
    "\n",
    "The PageRank algorithm identifies influential airports based on their connections to other important airports. For example, Hartsfield–Jackson Atlanta International Airport (ATL) emerged as having the highest lagged PageRank destination value for 2016, reflecting its role as a major connection hub for Southwest flights. PageRank values were calculated annually and lagged by year to capture seasonal patterns while maintaining temporal integrity.\n",
    "\n",
    "We complemented PageRank with InDegree and OutDegree metrics, which measure the volume of incoming and outgoing flights at each airport. These metrics were calculated quarterly to capture seasonal variations in traffic patterns. Airports with high InDegree values represent major destination hubs, while those with high OutDegree values serve as significant origin points in Southwest's network.\n",
    "\n",
    "Visualizations of graph-based features show that airports with higher PageRank or OutDegree tend to have higher average delay rates, highlighting the operational risks associated with central or highly connected hubs in the network.\n",
    "\n",
    "Together, these graph-based features provide a multi-dimensional view of airport connectivity and importance within Southwest's operational network, helping our model understand how delays might cascade through the system.\n",
    "\n",
    "| Graph Feature Category         | Description             | **Calculation Method** | **Lag method** |\n",
    "|--------------------------|----------------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| `PageRank`    | Measure of influence of high-traffic airports based on flight connection       | Distinct airport ID as Vertices and flight routes as Edges (src: origin airport ids, dst: destination airport ids) |  Year |\n",
    "|  `InDegree`   | Measure of high-traffic airport arrival patterns | Count of incoming connections from an airport | Quarter |\n",
    "|   `OutDegree`   | Measure of high-traffic airport departure patterns  | Count of outgoing connections from an airport | Quarter |\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/graph_based_features_1.png\" width=\"1200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd7677c-e272-4e07-ac30-e9c1eb9c7a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modeling Pipeline\n",
    "\n",
    "With the dataset cleaned, features engineered, and delays dissected through exploratory analysis, we now move to the modeling phase. The task is to build a binary classifier that predicts if a Southwest Airlines flight will be delayed at departure by 15 minutes or more (`DEP_DEL15` = 1 for delayed, 0 for on-time).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Dataset Splitting Strategy\n",
    "\n",
    "To simulate a real-world pipeline where training happens on historical data and evaluation on future quarters, we split our 2015 dataset into Train, Validation, and Test sets based on calendar years:\n",
    "\n",
    "| Dataset Partition | Years Included         | Use Case              |\n",
    "|-------------------|--------------------------|------------------------|\n",
    "| Train             | 2015 - 2021              | Model fitting         |\n",
    "| Validation        | 2022 - 2023              | Hyperparameter tuning |\n",
    "| Test              | 2024                     | Final evaluation      |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Feature Selection Strategy\n",
    "\n",
    "The dataset initially had **262 columns**. We adopted a structured feature selection pipeline involving multiple filtering and evaluation stages:\n",
    "\n",
    "1. **Null-based filtering**: Dropped columns with more than 50% missing values → 94 columns retained.  \n",
    "2. **Manual domain-driven pruning**: Removed 15 leakage-prone columns.  \n",
    "3. **Initial subset**: Resulted in 79 columns.  \n",
    "4. **Feature Engineering**: Added robust temporal, weather, airline-specific and graph features → 117 columns.  \n",
    "5. **Manual inspection**: Pruned down to 52 columns.  \n",
    "6. **Pearson correlation (numerical)**: Reduced to 49.  \n",
    "7. **Spearman correlation (categorical/ordinal)**: Final selected features → **44 columns**.\n",
    "\n",
    "Feature selection methods included **Pearson correlation for numerical variables** and **Spearman correlation for categorical or ordinal features**, balancing statistical relevance with domain knowledge. \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; gap: 20px;\">\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/perason.png\" width=\"49%\"/>\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/spearman.png\" width=\"49%\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Final Feature Set for Modeling\n",
    "\n",
    "Below are the final 44 features used for model training, grouped by category:\n",
    "\n",
    "#### 📆 Date/Time Features\n",
    "| Feature Name        | Description                                  |\n",
    "|---------------------|----------------------------------------------|\n",
    "| `DAY_OF_MONTH`      | Day of the month the flight departs          |\n",
    "| `DAY_OF_WEEK`       | Day of the week the flight departs           |\n",
    "| `YEAR`              | Flight year                                  |\n",
    "| `MONTH`             | Flight month                                 |\n",
    "| `year_quarter`      | Combined year and quarter                    |\n",
    "| `quarter_month`     | Month within the quarter                     |\n",
    "\n",
    "#### 🛫 Airport Profile Features\n",
    "| Feature Name            | Description                                              |\n",
    "|-------------------------|----------------------------------------------------------|\n",
    "| `origin_type`           | Size category of origin airport (small/medium/large)     |\n",
    "| `dest_type`             | Size category of destination airport                     |\n",
    "| `daily_operations`      | Total flights departing origin on the day                |\n",
    "| `rolling_30day_volume`  | 30-day rolling flight count at origin                    |\n",
    "| `origin_1yr_delay_rate` | 1-year average delay rate at origin                      |\n",
    "\n",
    "#### 🛬 Route Information\n",
    "| Feature Name         | Description                                                            |\n",
    "|----------------------|------------------------------------------------------------------------|\n",
    "| `TRIPLET`            | Concatenated route of previous and current legs                        |\n",
    "| `route`              | Origin-destination pair identifier                                     |\n",
    "| `route_1yr_volume`   | Total flights on route over the past year                              |\n",
    "| `sw_market_share`    | Southwest's share of total origin traffic                              |\n",
    "| `sw_30d_delay`       | Southwest 30-day delay rate at origin                                  |\n",
    "| `sw_route_delay`     | Southwest average delay on this route                                  |\n",
    "| `sw_rel_perf`        | Southwest delay rate vs airport average                                |\n",
    "| `sw_route_importance`| Normalized score of route importance to Southwest                      |\n",
    "\n",
    "#### 🕒 Time-Based Features\n",
    "| Feature Name                    | Description                                                     |\n",
    "|--------------------------------|-----------------------------------------------------------------|\n",
    "| `time_bucket`                  | 15-minute interval for scheduled departure                      |\n",
    "| `time_of_day_category`         | Categorized time of day: morning/evening/etc.                   |\n",
    "| `is_weekend`                   | Indicator for weekend flights                                   |\n",
    "| `holiday_season`              | Indicator for peak travel month                                 |\n",
    "| `prior_day_delay_rate`         | Delay rate at origin airport on previous day                    |\n",
    "| `prior_flights_today`          | Number of flights earlier in the same day                       |\n",
    "| `prior_delays_today`           | Count of delayed flights earlier today                          |\n",
    "| `same_day_prior_delay_percentage` | Fraction of delays prior to this flight                     |\n",
    "| `time_based_congestion_ratio`  | Current congestion vs historical average                        |\n",
    "\n",
    "#### ✈️ Southwest Profile Features\n",
    "| Feature Name                 | Description                                                       |\n",
    "|------------------------------|-------------------------------------------------------------------|\n",
    "| `sw_time_of_day_delay_rate` | Historical SW delay rate by time of day                           |\n",
    "| `sw_day_of_week_delay_rate` | SW delay rate by day of week on route                             |\n",
    "| `sw_aircraft_delay_rate`    | Average delay for aircraft (TAIL_NUM)                             |\n",
    "| `sw_origin_hub`             | Whether origin airport is a Southwest hub                         |\n",
    "| `sw_schedule_buffer_ratio` | Flight padding relative to historical average                     |\n",
    "| `sw_origin_time_perf`       | SW performance by airport + time bucket                           |\n",
    "\n",
    "#### 🌐 Lagged Graph-Based Features\n",
    "| Feature Name               | Description                                            |\n",
    "|----------------------------|--------------------------------------------------------|\n",
    "| `pagerank_origin_lag`      | Lagged PageRank score of origin airport               |\n",
    "| `pagerank_destination_lag`| Lagged PageRank score of destination airport          |\n",
    "| `origin_indegree_lag`      | Lagged indegree of origin airport                    |\n",
    "| `origin_outdegree_lag`     | Lagged outdegree of origin airport                   |\n",
    "| `dest_indegree_lag`        | Lagged indegree of destination airport               |\n",
    "| `dest_outdegree_lag`       | Lagged outdegree of destination airport              |\n",
    "\n",
    "#### 🧩 Other Raw Inputs\n",
    "| Feature Name             | Description                                      |\n",
    "|--------------------------|--------------------------------------------------|\n",
    "| `DEP_DEL15`              | Target variable: delayed departure (1=yes)       |\n",
    "| `PREV_CANCELLED`         | Whether previous leg was cancelled               |\n",
    "| `MINUTES_BETWEEN_FLIGHTS`| Gap in minutes from previous flight              |\n",
    "| `PREV_ARR_DEL15`         | Whether the previous leg arrived late            |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Transformation & Vectorization\n",
    "\n",
    "To prepare the data for modeling:\n",
    "\n",
    "- **Categorical Features** were one-hot encoded using `OneHotEncoder` from `pyspark.ml`\n",
    "- **Numerical Features** were standardized using `StandardScaler`\n",
    "- All transformed columns were **assembled into a single feature vector** using `VectorAssembler`, forming the final modeling input under column `features`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Model Training & Regularization\n",
    "\n",
    "We trained three models using concepts like **regularization, grid search, and cross-validation**, employing **Optuna** for hyperparameter tuning with **time-series-aware cross-validation blocks**:\n",
    "\n",
    "- **Logistic Regression** served as our baseline model.  \n",
    "- **XGBoost** provided gradient-boosted tree performance.  \n",
    "- **Neural Networks** enabled flexible non-linear modeling.\n",
    "\n",
    "Details about model-specific experiments, compute time, and performance are described in later sections.\n",
    "\n",
    "We evaluated the models using:\n",
    "- **Precision**: How many predicted delays were actually delayed.\n",
    "- **Recall**: Ability to detect actual delays (critical for minimizing false negatives).\n",
    "- **F2 Score**: Emphasizes recall more than precision — crucial in our domain.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Regularization\n",
    "\n",
    "Given the sparsity of our feature space, we attempt to apply **Lasso Regularization** for potential feature selection, a technique that penalizes large or complex coefficients in the model and encourages generalization by shrinking less useful weights toward zero.\n",
    "\n",
    "Regularization is introduced into the logistic loss function by adding a penalty term to the objective:\n",
    "\n",
    "#### Generalized Regularized Logistic Loss:\n",
    "\n",
    "$$\n",
    "\\\\mathcal{L}_{\\\\text{total}} = \\\\mathcal{L}_{\\\\text{data}} + \\\\lambda \\\\cdot \\\\mathcal{R}(w)\n",
    "$$\n",
    "\n",
    "We explored two primary forms:\n",
    "\n",
    "#### Lasso (L1) Regularization\n",
    "\n",
    "Promotes **sparse solutions** by shrinking some weights to zero. This is useful for implicit feature selection.\n",
    "\n",
    "$$\n",
    "\\\\mathcal{R}_{L1}(w) = \\\\sum_{j=1}^{p} |w_j|\n",
    "$$\n",
    "\n",
    "#### Elastic Net (L1+L2) Regularization\n",
    "\n",
    "In addition to pure Lasso Regularization, we also try Elastic Net Regularization, which is a combination of Lasso and Ridge Regularization.\n",
    "\n",
    "$$\n",
    "\\\\mathcal{R}_{L1+L2}(w) = \\\\sum_{j=1}^{p} |w_j| + \\\\sum_{j=1}^{p} w_j^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Model Selection Strategy\n",
    "\n",
    "**Recall and F2 Score** are the primary evaluation metrics.\n",
    "\n",
    "- Low recall implies high false negatives — i.e., delayed flights predicted as on-time, which is risky for airlines.  \n",
    "- F2 Score balances precision and recall but **emphasizes recall**, making it well-suited for delay prediction.\n",
    "\n",
    "We also monitored **precision** to ensure false positives (over-predicting delays) are reasonably controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec5adb3-9cf5-4c75-9a19-fadf3dd3fec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pipeline Architecture\n",
    "\n",
    "Below is the end-to-end pipeline diagram summarizing the complete flow:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/sacayo/w261-phase2-finalproject/refs/heads/main/plots/Phase3_model_Pipeline.png\" \n",
    "         alt=\"ML Pipelines diagram\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "This architecture allowed us to reuse checkpoints, track transformations efficiently, and iterate rapidly across modeling experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dfae388-4d62-4c99-ac7a-27264737f06c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Leakage Analysis\n",
    "\n",
    "### What is Data Leakage?\n",
    "\n",
    "Data leakage occurs when information from outside the training dataset is used to create a model, allowing it to make predictions with artificially high accuracy. This happens when data that would not be available at prediction time is inadvertently included during training.\n",
    "\n",
    "A hypothetical example in the context of flight delay prediction: If we included the actual departure delay time (`DEP_DELAY`) as a feature to predict whether a flight was delayed by 15+ minutes (`DEP_DEL15`), this would constitute severe leakage since the target variable is directly derived from the feature. The model would achieve near-perfect accuracy but would be useless in practice since the actual departure delay wouldn't be known when making predictions.\n",
    "\n",
    "### Leakage Analysis of Current Feature Set\n",
    "\n",
    "#### Temporal Features and Leakage Prevention\n",
    "\n",
    "Our feature engineering demonstrates awareness of temporal dependencies:\n",
    "\n",
    "- **Proper Lagging of Time-Based Features**: Features like `prior_day_delay_rate`, `sw_30d_delay`, and `origin_1yr_delay_rate` use historical data rather than future data.\n",
    "- **Graph Features with Appropriate Temporal Boundaries**: The `PageRank`, `InDegree`, and `OutDegree` features are properly lagged, ensuring that future information doesn't leak into the training process.\n",
    "- **Same-Day Features with Careful Implementation**: The `same_day_prior_delay_percentage` only considers flights that have already departed, maintaining the temporal integrity of our model.\n",
    "\n",
    "#### Potential Leakage Concerns\n",
    "\n",
    "While our feature engineering is generally sound, there are a few areas that warrant careful consideration:\n",
    "\n",
    "- **Graph Feature Imputation**: As noted, the imputation strategy for 2015 `PageRank` values and 2015-Q1 `InDegree`/`OutDegree` values involves using the same period's data due to the lack of prior data. This creates a minor risk of leakage, but our mitigation strategy is appropriate:\n",
    "  - Confining this imputation to the training set (2015-2017)\n",
    "  - Keeping validation (2018) and test (2019) sets completely separate\n",
    "  - Using this as a temporary solution rather than a predictive enhancement\n",
    "- **Flight-Specific Features**: Features like `PREV_CANCELLED`, `MINUTES_BETWEEN_FLIGHTS`, and `PREV_ARR_DEL15` need careful implementation to ensure they only use information from previous flights, not concurrent or future ones.\n",
    "- **Time-Based Aggregations**: Features like `time_based_congestion_ratio` must be calculated using only historical data at each point in time, which appears to be the case in our implementation.\n",
    "\n",
    "### Cardinal Sins of ML and Our Pipeline\n",
    "\n",
    "- **Training-Test Contamination**: Our pipeline properly separates data chronologically (2015-2017 for training, 2018 for validation, 2019 for testing), which prevents this sin.\n",
    "- **Target Leakage**: Our features appear to be constructed without using information that would not be available at prediction time. None of our features directly incorporate the target variable (`DEP_DEL15`) or its derivatives.\n",
    "- **Data Snooping**: Our feature engineering process seems to be guided by domain knowledge rather than repeatedly testing against the validation set, which helps avoid this sin.\n",
    "- **Temporal Leakage**: Our time series cross-validation approach respects the temporal nature of the data, preventing models from being trained on future data.\n",
    "- **Feature Selection Bias**: While not explicitly mentioned, our comprehensive feature set suggests a thoughtful selection process rather than automated selection that might introduce bias.\n",
    "\n",
    "### How Our Pipeline Prevents Leakage\n",
    "\n",
    "Our pipeline incorporates several best practices that effectively prevent data leakage:\n",
    "\n",
    "- **Chronological Data Splitting**: By strictly separating training (2015-2017), validation (2018), and test (2019) data chronologically, we ensure that models are evaluated on truly unseen future data.\n",
    "- **Temporal Feature Engineering**: Features like `rolling_30day_volume`, `prior_day_delay_rate`, and `sw_30d_delay` are constructed using only historical information available at prediction time.\n",
    "- **Proper Handling of High-Cardinality Features**: Our approach to high-cardinality features like `route` and `time_bucket` avoids leakage by encoding them appropriately.\n",
    "- **Cross-Validation Strategy**: Our time series cross-validation approach respects the temporal ordering of data, preventing future information from influencing the training process.\n",
    "- **Graph Feature Implementation**: Despite the necessary imputation for 2015 data, our approach minimizes leakage risk by containing it within the training set and ensuring validation and test sets remain unaffected.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our flight delay prediction pipeline demonstrates understanding of data leakage concerns and implements appropriate safeguards. The feature engineering process respects temporal boundaries, the data splitting strategy maintains chronological integrity, and the few instances where compromises were necessary (like the 2015 graph feature imputation) are handled with appropriate caution.\n",
    "\n",
    "The pipeline does not appear to violate any cardinal sins of machine learning and incorporates best practices for preventing data leakage in time series forecasting. The comprehensive feature set, spanning airport profiles, time-based metrics, and graph-based network analysis, provides a rich foundation for prediction without introducing artificial advantages through leaked information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e3677c0-161c-4d4f-917b-3011faa09d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Experiments\n",
    "All models are trained with the same set of features listed above. Each model varied in Databricks Cluster (CPUs vs GPUs) and training speed. Each model may have different hyperparameter tuning approach (Grid Search vs Optuna Bayesian Search). The cross validation streatgy is 4-fold rolling time series cross validation to avoid any leakage. For each fold, we split the time-ordered training data by threshold `[0.2, 0.4, 0.6, 0.8]` respectively into training data for CV and validation data for CV; for example, for the third fold, the first 60% of the training data, ordered from the earliest time to the latest, is used for CV training, and the rest 40% is used for validation.\n",
    "\n",
    "#### 1. Baseline Experiment: Logistic Regression\n",
    "\n",
    "As a baseline model, we trained a **Logistic Regression (LR)** classifier using Spark ML. The first version of this model was trained **without regularization**, but with **class-balanced sample weights**, computed as:\n",
    "\n",
    "$$\n",
    "\\mathcal{SW}_{\\text{delayed}} = \\frac{\\text{totalCounts}}{2 \\times \\text{totalDelay}}, \\quad \n",
    "\\mathcal{SW}_{\\text{onTime}} = \\frac{\\text{totalCounts}}{2 \\times \\text{totalOnTime}}\n",
    "$$\n",
    "\n",
    "The cluster configuration and model training time is as follows:\n",
    "- **Driver**: `m6gd.8xlarge` (128 GB RAM, 32 Cores)\n",
    "- **Workers (10)**: `m6g.2xlarge` (32 GB RAM, 8 Cores each)\n",
    "- **Runtime**: Databricks Runtime `15.4 LTS ML` with Spark 3.5.0\n",
    "- **Model Training Time (including Hyperparameters Tuning with Cross Validation)**: 1.1 hour\n",
    "\n",
    "As detailed in Phase 2, we have performed hyperparameter tuning for a logistic regression model with Lasso Regularization, under rolling time series cross validation. However, no better performance seen. Therefore, the unregularized baseline is selected as our final baseline model, serving as a strong benchmark for comparison with tree based and deep learning approaches. Predictions were generated across train, validation, and test sets using this unregularized baseline. The results below show the model's precision, recall, F2 score, and confusion matrices.\n",
    "\n",
    "<br/><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/lr_results.png\" width=\"1000\"/>\n",
    "\n",
    "#### 2. Subset analysis\n",
    "As part of our baseline experimentation process, we conducted a subset analysis to evaluate the performance of our Logistic Regression model. Specifically, we compared model results trained on our Southwest-only dataset against a broader dataset with all unique carriers for the 3 years — training set was for years 2015-2017. To ensure consistency, we applied the same datacleaning, feature engineering, train/test splitting, and modeling process to both datasets. The key difference between the two lies in the size and composition of the training data:\n",
    "\n",
    "* The full carrier dataset included approximately 18.15 million training records (more than four times larger than the Southwest-only training set, which contained 4.19 million records.\n",
    "* For both models, validation and test sets were derived using the same methodology. We filtered for Southwest-only before modeling.\n",
    "\n",
    "A full table of the size of both dataset can be found below. Despite the significant increase in training data, the model trained on all carriers underperformed on key evaluation metrics.  Across the board we found the precision, recall, and F2 score were lower when compared to the Southwest-only baseline. This suggests that carrier-specific patterns may be critical for predictive performance, and that broader datasets may introduce noise that diminishes model accuracy when applied to Southwest operations.\n",
    "\n",
    "| Datasets | Training Size | Validation Size | Test Size |\n",
    "|---------|--------------|--------------|--------------|\n",
    "| **Southwest-only** | 4,192,913 | 1,476,345 | 1,471,235 |  \n",
    "| **Full Carrier Training Set** |18,153,200  |1,476,345 | 1,471,235 |  \n",
    "\n",
    "\n",
    "\n",
    "#### 3. XGBoost Experiment\n",
    "\n",
    "A XGBoost with default hyperparameters (n_estimators=100, max_depth=6, learning_rate=0.3) is fitted, achieving a validation precision of 0.872, a recall of 0.696, a f2 of 0.725. Then, we apply hyperparameter tuning with rolling time series cross validation for a better set of n_estimators, max_depth, and learning_rate. Unlike the logistic regression model, we use Optuna, a hyperparameter optimization framework powered by Bayesian Search, for a more efficient XGBoost model hyperparameters search.\n",
    "\n",
    "The cluster configuration and model training time is as follows:\n",
    "- **Driver**: `m6gd.8xlarge` (128 GB RAM, 32 Cores)\n",
    "- **Workers (10)**: `m6g.2xlarge` (32 GB RAM, 8 Cores each)\n",
    "- **Runtime**: Databricks Runtime `15.4 LTS ML` with Spark 3.5.0\n",
    "- **Model Training Time (including Hyperparameters Tuning with Cross Validation)**: 2.47 hours\n",
    "\n",
    "Optune yields a set of hyperparameters:\n",
    "- **n_estimators**: 115\n",
    "- **max_depth**: 9\n",
    "- **learning_rate**: 0.27122395210228795\n",
    "\n",
    "The following is the evaluation metrics and confusion matrix:\n",
    "<br/><img src=\"https://raw.githubusercontent.com/fan005mids/DS261FinalProj/refs/heads/main/Phase3%20XGBoost%20Model%20Eval.png\"/>\n",
    "\n",
    "#### 4. Neural Network Experiment\n",
    "\n",
    "This section describes three key experiments conducted to train and tune a feed-forward neural network (NN) model for binary classification of flight delays. All experiments were run on a GPU-enabled Databricks cluster with the following configuration:\n",
    "\n",
    "- **Driver**: `g4dn.8xlarge` (128 GB RAM, 32 vCPUs)\n",
    "- **Workers (4)**: `g4dn.xlarge` (64 GB RAM, 16 vCPUs each)\n",
    "- **Runtime**: Databricks Runtime `15.4 LTS ML` with Spark 3.5.0\n",
    "\n",
    "---\n",
    "\n",
    "##### **Experiment 1: Manual Grid Search without Cross-Validation**\n",
    "\n",
    "In this experiment, a basic grid search was performed manually over a small set of predefined hyperparameter configurations. Each model was trained using early stopping on the validation set (patience = 5). The evaluation was done across **train**, **validation**, and **test** splits.\n",
    "\n",
    "- **Date Range**: 2015–2021  \n",
    "  - **Train**: 2015–2019  \n",
    "  - **Validation**: 2020  \n",
    "  - **Test**: 2021  \n",
    "- **Training Time**: ~27 minutes\n",
    "\n",
    "**Grid Search Space:**\n",
    "\n",
    "```python\n",
    "param_grid = [\n",
    "    {'hidden_layers': [128], 'batch_size': 256, \n",
    "     'learning_rate': 0.0005, 'dropout_rate': 0.2},\n",
    "    {'hidden_layers': [128, 64], 'batch_size': 512,\n",
    "     'learning_rate': 0.0003, 'dropout_rate': 0.3},\n",
    "    {'hidden_layers': [256, 128, 64], 'batch_size': 1024,\n",
    "     'learning_rate': 0.0001, 'dropout_rate': 0.5},\n",
    "    {'hidden_layers': [128], 'batch_size': 256,\n",
    "     'learning_rate': 0.001, 'dropout_rate': 0.4}\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "<br/><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/NN_table_comparision.png\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "<br/><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/NN_confusion_matrix.png\" width=\"1000\"/>\n",
    "\n",
    "From the comparison table and confusion matrices, the best-performing configuration was the deepest model with three hidden layers `[256, 128, 64]`, a batch size of 1024, a learning rate of 0.0001, and a relatively high dropout rate of 0.5. This configuration achieved the **highest F2 score on the validation set (0.7135)** and a **strong F2 score on the test set (0.6947)**, indicating a well-generalized model with a good balance between precision and recall for the delayed class. The confusion matrices show consistent behavior across train, validation, and test splits, with slightly more false negatives than false positives—a common trade-off when optimizing for recall-heavy metrics like F2. Overall, this setup offered the most effective performance in the manual search and served as a strong candidate to compare against the more automated tuning process in Experiment 2.\n",
    "\n",
    "         \n",
    "##### **Experiment 2: Hyperparameter Optimization with Cross-Validation (Optuna)**\n",
    "\n",
    "In this experiment, a more automated and systematic hyperparameter tuning was performed using Optuna with time-series cross-validation. Each trial trains a model across 4 time-based CV folds, optimizing for the F2 score on the validation splits.\n",
    "\n",
    "This setup allows for better generalization and temporal robustness in a time-sensitive prediction task like flight delays.\n",
    "\n",
    "- **Date Range**: 2015–2024\n",
    "  - **Train**: 2015–2021\n",
    "  - **Validation**: 2022–2023\n",
    "  - **Test**: 2024\n",
    "- **Training Time**: \n",
    "  - To find best hyperparameters: ~1 hour 30 minutes \n",
    "  - To train final model: ~45 minutes\n",
    "- **Optuna Trials**: 1 trial (extendable)\n",
    "- **Search Method**: Tree-structured Parzen Estimator (TPE)\n",
    "- **Objective**: Maximize average F2 score across folds\n",
    "\n",
    "**Search Space**:\n",
    "\n",
    "- Number of hidden layers: 1–3\n",
    "- Units per layer: 32–512\n",
    "- Dropout: 0.1–0.5\n",
    "- L2 regularization: 1e-5 – 1e-2\n",
    "- Batch size: 128, 256, 512, 1024\n",
    "- Learning rate: 1e-5 – 1e-2\n",
    "- Batch normalization: True / False\n",
    "\n",
    "**Best hyperparameters:**\n",
    "\n",
    "- **n_layers**: 1\n",
    "- **units_l0**: 448\n",
    "- **dropout_rate**: 0.27565986839411283\n",
    "- **l2_reg**: 0.0001554892275405439\n",
    "- **batch_size**: 256\n",
    "- **learning_rate**: 1.1258097381851819e-05\n",
    "- **use_batch_norm**: False\n",
    "\n",
    "After training with the best parameters, the model achieved the following performance:\n",
    "\n",
    "| Split | F2 Score | Recall | Precision |\n",
    "|-------|----------|--------|-----------|\n",
    "| Train | 0.7488   | 0.7555 | 0.7234    |\n",
    "| Val   | 0.7581   | 0.7638 | 0.7362    |\n",
    "| Test  | 0.7469   | 0.7527 | 0.7243    |\n",
    "\n",
    "<br/><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/NN_best_model.png\" width=\"1000\"/>\n",
    "\n",
    "The confusion matrices show strong alignment across train, validation, and test sets — indicating a stable and robust generalization performance.\n",
    "\n",
    "##### **Experiment 3: Bidirectional LSTM (BiLSTM)**\n",
    "\n",
    "While the first two experiments focused on **multilayer perceptrons (MLP)**, this third experiment explores a **Bidirectional Long Short-Term Memory (BiLSTM)** network to assess whether sequential modeling over feature space improves performance.\n",
    "\n",
    "Taking inspiration from recent research ([Bisandu and Moulitsas (2023)](https://dspace.lib.cranfield.ac.uk/server/api/core/bitstreams/1b1c8945-a27d-47cf-b8db-4b6481cbb536/content)), which suggests the effectiveness of LSTMs in capturing temporal and contextual dependencies in flight delay data, we implemented a BiLSTM-based architecture.\n",
    "\n",
    "The BiLSTM model processes the flattened feature vectors as 1-timestep sequences, allowing it to model directional relationships between features. Although this isn’t a traditional sequential dataset (like text or time-series per row), treating features as sequences can still allow LSTM to capture higher-order interactions.\n",
    "\n",
    "- **Date Range**: 2015–2024\n",
    "  - **Train**: 2015–2021\n",
    "  - **Validation**: 2022–2023\n",
    "  - **Test**: 2024\n",
    "- **Training Time**: ~1 hour 27 minutes\n",
    "- **Model Type**: Single-layer BiLSTM\n",
    "- **Batch Size**: 256 (Optuna-inspired)\n",
    "- **Dropout**: 0.3\n",
    "- **L2 Regularization**: 0.0001\n",
    "- **Learning Rate**: 1e-4\n",
    "\n",
    "While this experiment did not undergo exhaustive hyperparameter tuning due to compute constraints, it served as a proof of concept and demonstrated encouraging results:\n",
    "\n",
    "| Split | F2 Score | Recall | Precision |\n",
    "|-------|----------|--------|-----------|\n",
    "| Train | 0.7284   | 0.7196 | 0.7660    |\n",
    "| Val   | 0.7210   | 0.7070 | 0.7833    |\n",
    "| Test  | 0.7149   | 0.7030 | 0.7667    |\n",
    "\n",
    "<br/><img src=\"https://raw.githubusercontent.com/ayushigoel9/w_261_final_project/main/BiLSTM_results.png\" width=\"1000\"/>\n",
    "\n",
    "As shown in the confusion matrices, the BiLSTM model produced a **slightly more balanced distribution** of false positives and false negatives compared to MLP models. While precision increased on the validation and test sets, the F2 score remained in the same ballpark — indicating that BiLSTM offers **comparable performance** with a different modeling perspective.\n",
    "\n",
    "Future work could involve expanding this architecture with:\n",
    "- More LSTM layers\n",
    "- Attention mechanisms\n",
    "- Tuning sequence length or embedding temporal hierarchies\n",
    "\n",
    "This experiment highlights the **potential of sequential modeling** for even non-time-sequential tabular features in flight delay prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c37831c-b6e6-4138-8bd8-1c92e3284caf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Result and Discussions\n",
    "\n",
    "\n",
    "This section interprets and compares the performance of all models explored in our experiments — including Logistic Regression (baseline), XGBoost, Feedforward Neural Network (MLP), and BiLSTM. Each model was evaluated using consistent data splits and metrics (Precision, Recall, F2), with a particular focus on **F2-score** due to its emphasis on recall for the delayed class.\n",
    "\n",
    "All models were evaluated on the same temporal splits to ensure fair comparison:  \n",
    "- **Train**: 2015–2021  \n",
    "- **Validation**: 2022–2023  \n",
    "- **Test**: 2024  \n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance Comparison\n",
    "\n",
    "| Metric    | Logistic Regression | XGBoost        | Neural Network (MLP) | BiLSTM         |\n",
    "|-----------|---------------------|----------------|-----------------------|----------------|\n",
    "| **Train F2**     | 0.725               | **0.752**        | 0.748                 | 0.728          |\n",
    "| **Train Recall** | **0.759**           | 0.722            | 0.755                 | 0.720          |\n",
    "| **Train Precision** | 0.614           | **0.897**        | 0.723                 | 0.766          |\n",
    "| **Val F2**       | 0.754               | 0.733            | **0.758**             | 0.721          |\n",
    "| **Val Recall**   | **0.802**           | 0.707            | 0.764                 | 0.707          |\n",
    "| **Val Precision**| 0.609               | 0.863            | 0.736                 | **0.783**      |\n",
    "| **Test F2**      | 0.732               | 0.744            | **0.746**             | 0.715          |\n",
    "| **Test Recall**  | 0.777               | 0.730            | **0.753**             | 0.703          |\n",
    "| **Test Precision**| 0.595              | 0.760            | 0.724                 | **0.767**      |\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix: Best MLP Model on Test Set\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/fan005mids/DS261FinalProj/refs/heads/main/Phase%203%20NN%20Test%20Eval.png\" \n",
    "         alt=\"Test Data Confusion Matrix\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation and Model Insights\n",
    "\n",
    "- **Logistic Regression**  \n",
    "  - On the **validation set**, the model achieved the highest **recall** (0.802) among all models, but with low **precision** (0.609), resulting in many false positives.\n",
    "  - On the **test set**, its F2 dropped to 0.732, with **recall** at 0.777 but **precision** at just 0.595.\n",
    "  - While recall-heavy, its poor precision makes it less suitable for operational use, where false alarms can be costly.\n",
    "\n",
    "- **XGBoost**  \n",
    "  - On the **validation set**, XGBoost had the highest **precision** (0.863), but lower **recall** (0.707), leading to a validation F2 of 0.733.\n",
    "  - On the **test set**, it maintained balanced performance with **F2 = 0.744**, **precision = 0.760**, and **recall = 0.730**.\n",
    "  - This pattern suggests that XGBoost prioritizes precision and is slightly prone to underpredicting delays.\n",
    "\n",
    "- **Neural Network (MLP)**  \n",
    "  - On the **validation set**, the MLP achieved the **best F2 score (0.758)** with a strong balance of **recall (0.764)** and **precision (0.736)**.\n",
    "  - On the **test set**, it continued to perform consistently with **F2 = 0.746**, **recall = 0.753**, and **precision = 0.724**.\n",
    "  - This consistency across splits indicates that the model generalizes well without overfitting and aligns best with our F2-optimized objective.\n",
    "\n",
    "- **BiLSTM**  \n",
    "  - On the **validation set**, BiLSTM showed the **highest precision** (0.783), even higher than XGBoost, but recall was slightly lower at 0.707, leading to **F2 = 0.721**.\n",
    "  - On the **test set**, its performance was similar, with **F2 = 0.715**, **recall = 0.703**, and **precision = 0.767**.\n",
    "  - This indicates that BiLSTM is more conservative, prioritizing precision and correctly identifying delays it is more confident about. While its F2 was slightly lower, the precision-focused behavior could be desirable in certain use cases (e.g., limited intervention resources).\n",
    "\n",
    "---\n",
    "\n",
    "### Model Selection Reasoning\n",
    "\n",
    "We selected the **Feedforward Neural Network (MLP)** as our best model based on:\n",
    "- **Highest validation and test F2 scores**\n",
    "- Consistent performance across all data splits\n",
    "- Balanced trade-off between recall and precision\n",
    "\n",
    "While XGBoost was more precise, it underperformed in recall. BiLSTM offered better precision than MLP, but its recall was lower, making it less suitable for our F2-optimized goal. The baseline Logistic Regression also lacked the precision needed to avoid unnecessary operational escalations.\n",
    "\n",
    "---\n",
    "\n",
    "### Gap Analysis\n",
    "\n",
    "In 2024, Southwest Airlines experienced 340,115 delays. Our model, during testing, successfully identified 256,012 of these delays. Considering that our model has a recall rate of over 75%, we are confident that it can accurately identify three-quarters of the total delay.Early identification of delayed flights is crucial. If we can prepare in advance and achieve a 5% cost reduction, we could save approximately 11.7 million dollars in 2024. A 10% cost reduction would yield savings of over 23 million dollars.We also conducted a long-term analysis spanning six years from 2015 to 2021, assuming a 10% cost reduction. This analysis projected a savings of approximately 140 million dollars. Detailed calculations are available in the appendix.\n",
    "\n",
    "Our next step involves collaborating with all stakeholders to plan a soft launch and integrate the model into our daily operations. This will enable the corporation to start enjoying these cost savings as soon as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c969b54a-7f84-43fd-9d31-85ccb0c3c23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "Southwest Airlines faces a significant operational challenge in predicting flight delays, which have resulted in substantial financial losses of over **$2.84 billion** between 2015 and 2024. We hypothesized that a machine learning pipeline, equipped with meticulously crafted features, could reliably forecast flight delays before departure. This would enable proactive planning and data-driven interventions to mitigate operational disruptions.\n",
    "\n",
    "In **Phase 2**, we validated this hypothesis using a 12-month dataset from 2015. We integrated flight-level data from the Department of Transportation (DOT) and weather records from the National Oceanic and Atmospheric Administration (NOAA). We constructed an end-to-end machine learning pipeline and trained a baseline **Logistic Regression** model. Despite its simplicity, the model demonstrated promising early potential in predicting flight delays, achieving a recall of **0.676** and an F1 score of **0.390**.\n",
    "\n",
    "In **Phase 3**, we significantly expanded the scope of our analysis by scaling the dataset to encompass the entire period from 2015 to 2024. Additionally, we engineered domain-specific features that provided valuable insights into potential delay triggers, such as `origin_delay_rate`, `triplet`, `airport_congestion_level`, and `previous_leg_delayed`. We evaluated a range of advanced models, including **XGBoost**, **MLP (Multilayer Perceptron)**, and **BiLSTM**. The **best-performing model** emerged as a single-layer MLP with 448 hidden units, a dropout rate of 0.276, L2 regularization of 0.000155, a batch size of 256, and a learning rate of approximately **1e-5**. This model achieved remarkable performance, achieving an **F2 score of 0.746**, **Recall of 0.752**, and **Precision of 0.724** on the 2024 test set—strongly validating the effectiveness of this architecture and feature design.\n",
    "\n",
    "As outlined in the Results and Discussion section, accurately identifying **three-quarters of delayed flights** presents a significant opportunity for substantial operational savings. Even a modest **5% reduction in delay-related costs** amounts to over **$11 million** in annual savings—and over **$140 million** over a 10-year period. These findings validate the operational viability and financial worth of this predictive system.\n",
    "\n",
    "Moving forward, the next phase involves collaborating with stakeholders across Southwest’s operations to pilot a **soft launch** of this delay classifier. Crucially, all input features utilized by the model are accessible **at least 15 minutes prior to departure**, ensuring its real-time feasibility. While we can adjust this time buffer to a larger value, such as 30 minutes prior, to facilitate earlier identification, this may result in a slight performance compromise. Therefore, during the soft-launch phase, we aim to assess whether an at least 15-minute-early identification is sufficient. If successful, we can gradually expand its application to all Southwest flights, leading to measurable savings and enhancing the customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6da07b5b-bb89-47ff-86eb-8cd007affa8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Meet the Team - Group 4_4**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U068N7K7D71-275409804ab1-512\" width=\"100\"><br>\n",
    "      <strong>Ayushi Goel</strong><br>\n",
    "      ✉️ <a href=\"mailto:ayugoel@ischool.berkeley.edu\">Email</a>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U050HPGAQ7Q-75fe6a983dc6-512\" width=\"100\"><br>\n",
    "      <strong>Licheng Zhong</strong><br>\n",
    "      ✉️ <a href=\"mailto:lzmids@ischool.berkeley.edu\">Email</a>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U061UNNDZPH-968279cfd6bb-512\" width=\"100\"><br>\n",
    "      <strong>Louis Wu</strong><br>\n",
    "      ✉️ <a href=\"mailto:louiswu1201@ischool.berkeley.edu\">Email</a>\n",
    "    </td>\n",
    "    <td align=\"center\">\n",
    "      <img src=\"https://ca.slack-edge.com/T0WA5NWKG-U05E8ARFHAR-ad9dda100c72-512\" width=\"100\"><br>\n",
    "      <strong>Sammy Cayo</strong><br>\n",
    "      ✉️ <a href=\"mailto:sacayo@ischool.berkeley.edu\">Email</a>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84c50f4e-1fdb-48a9-80b8-82c8b0d796cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### [Code notebook](https://dbc-fae72cab-cf59.cloud.databricks.com/editor/notebooks/3336123436790193?o=4021782157704243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69390e86-8416-4728-9357-cd90545d327f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Appendix: Clarification of Financial Calculations\n",
    "\n",
    "The financial estimates provided in this project are **approximations** and should be interpreted with caution. The total economic impact of flight delays and potential savings from reducing delays are derived from **publicly available data**, **not Southwest Airlines' internal financial records**.\n",
    "\n",
    "#### Source of Economic Impact Estimate:\n",
    "The cost per delayed flight is taken from the article **[\"Flight Delays in Numbers – Not Only Painful For Passengers\"](https://www.allthingsontimeperformance.com/flight-delays-in-numbers-not-only-painful-for-passengers/)**, which states that **each delayed flight costs approximately $920**.\n",
    "\n",
    "#### Calculation Details:\n",
    "\n",
    "- **Total Delayed Flights (2015–2021):** **3.09 million**  \n",
    "- **Cost Per Delayed Flight:** **$920**  \n",
    "- **Total Economic Impact of Delays:**  \n",
    "  - **3,090,000 × 920 = $2.84 billion**  \n",
    "\n",
    "- **Potential Savings from a 5% Reduction in Delays:**\n",
    "  - **5% of Total Delayed Flights:**  \n",
    "    - **3,090,000 × 0.05 = 154,500 flights**  \n",
    "  - **Potential Savings:**  \n",
    "    - **154,500 × 920 = $142.1 million** \n",
    "\n",
    "#### Model Prediction vs Actual Delays\n",
    "- **2024 total delay: 340,115**\n",
    "- **True prediction delay: 256,012**\n",
    "- **Delay opportunity**\n",
    "  - 256,012 (True Positives) * 920  ~ $235 Million\n",
    "- **5% cost reduction**\n",
    "  - 256,012 * 5% * 920 = 11,845.6 * 920 ~ $11.7 Million \n",
    "- **10% cost reduction**\n",
    "  - 256,012 * 10% * 920 =  23,691 * 920 ~  $23 Million\n",
    "\n",
    "<a name=\"appendix\"></a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "261_Final_Report",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}